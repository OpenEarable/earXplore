ID,First Author,Year,Location,Input Body Part,Gesture,Sensing_PANEL_Sensors,Sensing_PANEL_No Additional Sensing,Interaction_PANEL_Number of Selected Gestures,Interaction_PANEL_Resolution,Interaction_PANEL_Hands-Free,Interaction_PANEL_Eyes-Free,Interaction_PANEL_Possible on One Ear,Interaction_PANEL_Adaptation of the Interaction Detection Algorithm to the Individual User,Interaction_PANEL_Discreetness of Interaction Techniques,Interaction_PANEL_Social Acceptability of Interaction Techniques,Interaction_PANEL_Accuracy of Interaction Recognition,Interaction_PANEL_Robustness of Interaction Detection,Study_PANEL_Elicitation Study,Study_PANEL_Usability Evaluations,Study_PANEL_Cognitive Ease Evaluations,Study_PANEL_Discreetness of Interactions Evaluations,Study_PANEL_Social Acceptability of Interactions Evaluations,Study_PANEL_Accuracy of Interactions Evaluations,Study_PANEL_Alternative Interaction Validity Evaluations,Study_PANEL_Evaluation of Different Conditions,Study_PANEL_Evaluation of Different Settings,Device_PANEL_Earphone Type,Device_PANEL_Development Stage,Device_PANEL_Real-Time Processing,Device_PANEL_On-Device Processing,Applications_PANEL_Intended Applications,Keywords,Abstract,Study Link
1,Weisenberger et al.,1987,Actuation,N/A,Vibration (Actuation),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,N/A,N/A,"Accessibility, Health",N/A,"A binaural earmold sound-to-tactile aid was constructed by inserting a vibrating element into a Lucite earmold. The earmold could be vibrated at either 80 Hz (when incoming acoustic signals were below 2000 I-Ir), at 300 Hz (when incoming acoustic signals were above 2000 Hz), or both (when incoming acoustic signals were broadband). Subjects were fitted with one of these bimodal vibrating earmolds in each ear. Normal-hearing and hearing-impaired subjects were tested in three tasks: sound localization, errvironmental sound identification, and syllable rhythm and stress. The device provided some benefit to performance, although the amounts of improvement varied across tasks and subjects. Possible modifications in device design, and potential combinations of auditory and tactile input via earmold systems, are discussed.",https://www.researchgate.net/publication/19575713_Development_and_preliminary_evaluation_of_an_earmold_sound-to-tactile_aid_for_the_hearing-impaired
2,Brewster et al.,2003,Head Gestures and Pointing,Head,"Pitch, Roll","Accelerometer, Gyroscope, Magnetometer",Yes,1,Coarse,Yes,Yes,No,No,Medium,High,N/A,N/A,No,Yes (N=18),No,No,No,No,No,Sitting,Lab,Custom Device,Commercial,Yes,No,Device Input,"Gestural Interaction, Wearable Computing","Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users’ gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.",https://dl.acm.org/doi/abs/10.1145/642611.642694
3,Metzger et al.,2004,"Hand Gestures and Location, Head Gestures and Pointing","Hand, Head","Hold (Mid-Air), Roll, Slide (Mid-Air)","Accelerometer, Proximity Sensor",Yes,7,"Coarse, Semantic",No,Yes,No,No,Low,Low,High (N=1),N/A,No,No,No,No,No,Yes (N=1),No,Sitting,Lab,Headphone,Research Prototype,Yes,No,"Accessibility, Device Control, Music Player, Phone Calls",N/A,"We present FreeDigiter, an interface for mobile devices which enables rapid entry of digits using finger gestures. FreeDigiter is an infrared proximity sensor with a dual axis accelerometer and requires little signal processing. Initial laboratory experiments attain accuracy rates of 99.0%; and the system is tolerant to highly varying lighting conditions. The FreeDigiter system requires little power and could be implemented in a very small form factor appropriate for controlling in–ear hearing aids, small MP3 players, and hands–free mobile phone headsets.",https://ieeexplore.ieee.org/document/1364684
4,Buil & Hollemans,2005,Ear and Earable,Hand,Press (Earable),Button,Yes,3,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,Music Player,N/A,"The touch headphones are a solution for providing playback and volume controls on in-ear type headphones.  One of the issues with placing controls on earpieces is that applied pressure is transferred to the inner ear,  which potentially creates discomfort. The experiment described in this short paper shows that conventional button switches  are not well accepted. Users preferred to operate a button on an earpiece with a force of around 85 grams.",https://ieeexplore.ieee.org/abstract/document/1550805
5,Buil et al.,2005,Ear and Earable,"Hand, Wearable State","Attach Earbud, Hold (Earable), Remove Earbud, Tap (Earable)",Capacitive Sensor,Yes,5,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Research Prototype,Yes,No,"Device Control, Music Player","Capacitive Touch Control, Headphones, MP3, Music Playback, User Interface, User System Interaction","The Touch Headphones are meant for portable music players and aim to present an improvement to the conventional remote control in the headphone wire, and a solution for controls on wireless in-ear type headphones. Two capacitive touch sensors per earpiece sense when earpieces are being tapped on, and being put in or out.",https://dl.acm.org/doi/abs/10.1145/1085777.1085877
11,Gamper et al.,2011,Head Gestures and Pointing,Head,Yaw,Microphone,Yes,1,Fine,Yes,Yes,No,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=3),Sitting,Lab,Headphone,Commercial,No,No,"AR/VR, Motion Tracking, Video Conference",N/A,"A head orientation tracking system using binaural headset microphones is proposed. Unlike previous approaches, the proposed method does not require anchor sources, but relies on speech signals of the wearers of the binaural headsets. From the binaural microphone signals, time of arrival (TOA) and time difference of arrival (TDOA) estimates are obtained. The tracking is performed using a particle filter integrated with a maximum likelihood estimation function. In a case study, the proposed method is used to track the head orientations of three conferees in a meeting scenario. With an accuracy of about 10 degrees, the proposed method is shown to outperform a reference method which achieves an accuracy of about 35 degrees.",https://hannesgamper.com/wp-content/papercite-data/pdf/gamper2011b.pdf
12,Manabe & Fukumoto,2011,Ear and Earable,Hand,Tap (Earable),Speaker,Yes,2,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=6),Medium (N=6),No,No,No,No,No,Yes (N=6),No,"Head Movement, Jumping, Sitting, Walking, Walking Stairs",Lab,"Earbud, Headphone",Research Prototype,Yes,No,"Device Control, Music Player","Headphones, Input Device, Tap, Wearable","A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.",https://doi.org/10.1145/2047196.2047236
13,Matsumura & Fukumoto,2012,Ear and Earable,Wearable State,"Share (Earable), Wear (Earable)","EMG, Proximity Sensor",Yes,2,Semantic,Yes,Yes,No,No,High,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,"Earbud, Headphone",Research Prototype,Yes,Yes,Music Player,"Earphones, Implicit Interaction, Intelligent Interface","We present universal earphones that use both a proximity sensor and a skin conductance sensor and we demonstrate several implicit interaction techniques they achieve by automatically detecting the context of use. The universal earphones have two main features. The first involves detecting the left and right sides of ears, which provides audio to either ear, and the second involves detecting the shared use of earphones and this provides mixed stereo sound to both earphones. These features not merely free users from having to check the left and right sides of earphones, but they enable them to enjoy sharing stereo audio with other people.",https://dl.acm.org/doi/abs/10.1145/2166966.2167025
15,Tessendorf and Derleth,2012,Ear and Earable,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=21),Yes (N=21),No,Yes (N=21),No,No,"Head Movement, Jumping, Walking, Walking Stairs",Lab,Custom Device,Research Prototype,Yes,Yes,"Accessibility, Data Annotation",N/A,"In this work we present a newly developed earworn sensing and annotation device to unobtrusively capture head movements in real life situations. It has been designed in the context of developing multimodal hearing instruments (HIs), but is not limited to this application domain. The ear-worn device captures triaxial acceleration, rate of turn and magnetic field and features a one-button-approach for real-time data annotation through the user. The system runtime is over 5 hours at a sampling rate of 128 Hz. In a user study with 21 participants the device was perceived as comfortable and showed a robust hold at the ear. On the example of head acceleration data we perform unsupervised clustering to demonstrate the benefit of head movements for multimodal HIs. We believe the novel technology will help to push the boundaries of HI technology.",https://ieeexplore.ieee.org/abstract/document/6346464
18,Lissermann et al.,2014,Ear and Earable,Hand,Touch (Ear),Capacitive Sensor,Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,Yes (N=27),No,No,No,No,No,No,"Music, Sitting",Lab,Custom Device,Research Prototype,Yes,No,"Device Control, Gaming, Music Player","Device Augmentation, Ear-Based Interaction, Ear-Worn, Eyes-Free, Mobile Interaction, Multi-Touch, Touch","One of the pervasive challenges in mobile interaction is  decreasing the visual demand of interfaces towards eyes-free  interaction. In this paper, we focus on the unique affordances  of the human ear to support one-handed and eyes-free mobile  interaction. We present EarPut, a novel interface concept and  hardware prototype, which unobtrusively augments a variety  of accessories that are worn behind the ear (e.g. headsets  or glasses) to instrument the human ear as an interactive  surface. The contribution of this paper is three-fold. We  contribute (i) results from a controlled experiment with 27  participants, providing empirical evidence that people are  able to target salient regions on their ear effectively and  precisely, (ii) a first, systematically derived design space  for ear-based interaction and (iii) a set of proof of concept  EarPut applications that leverage on the design space and  embrace mobile media navigation, mobile gaming and smart  home interaction.",https://dl.acm.org/doi/10.1145/2686612.2686655
20,Sahni et al.,2014,Mouth,Speech Apparatus,Silent Speech (Words),"Magnetometer, Proximity Sensor",No,11,Semantic,Yes,Yes,No,No,High,High,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"Accessibility, Device Input, Silent Speech","Mobile Interfaces, Silent Speech Recognition, Wearable Computing","We address the problem of performing silent speech recognition where vocalized audio is not available (e.g. due to a user’s medical condition) or is highly noisy (e.g. during firefighting or combat). We describe our wearable system to capture tongue and jaw movements during silent speech. The system has two components: the Tongue Magnet Interface (TMI), which utilizes the 3-axis magnetometer aboard Google Glass to measure the movement of a small magnet glued to the user’s tongue, and the Outer Ear Interface (OEI), which measures the deformation in the ear canal caused by jaw movements using proximity sensors embedded in a set of earmolds. We collected a data set of 1901 utterances of 11 distinct phrases silently mouthed by six able-bodied participants. Recognition relies on using hidden Markov modelbased techniques to select one of the 11 phrases. We present encouraging results for user dependent recognition.",https://dl.acm.org/doi/10.1145/2634317.2634322
21,Bedri et al.,2015,Mouth,Jaw,General Jaw Movement,Proximity Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,Yes (N=27),No,No,No,No,No,"Eating, Sitting, Speaking, Walking, Walking Stairs",Lab,Earbud,Research Prototype,No,No,"Activity Recognition, Silent Speech","Food Intake, Jaw Gestures, Jaw Motion, Outer Ear, Proximity Sensor, Silent Speech","The human ear seems to be a rigid anatomical part with no apparent activity, yet many facial and body activity can be measured from it. Research apparatuses and commercial products have demonstrated the capability of monitoring hart rate, tongue activities, jaw motion and eye blinking from the ear. In this paper we describe the design and the implementation of the Outer Ear Interface (OEI) which utilizes a set of infrared proximity sensors to measure the deformation in the ear canal caused by the lower jaw movement. OEI has been used in different applications that requires tracking of jaw activity which includes silent speech recognition, jaw gesture detection and food intake monitoring.",https://dl.acm.org/doi/10.1145/2800835.2807933
25,Weigel et al.,2015,Ear and Earable,Hand,Touch (Earable),"Capacitive Sensor, Resistive Sensor",Yes,1,"Coarse, Semantic",No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Custom Device,Research Prototype,Yes,No,"Device Input, Music Player","Electronic Skin, Flexible Sensor, Mobile Computing, On-Body Input, Stretchable Sensor, Touch Input, Wearable Computing","We propose iSkin, a novel class of skin-worn sensors for touch input on the body. iSkin is a very thin sensor overlay, made of biocompatible materials, and is flexible and stretchable. It can be produced in different shapes and sizes to suit various locations of the body such as the finger, forearm, or ear. Integrating capacitive and resistive touch sensing, the sensor is capable of detecting touch input with two levels of pressure, even when stretched by 30% or when bent with a radius of 0.5 cm. Furthermore, iSkin supports single or multiple touch areas of custom shape and arrangement, as well as more complex widgets, such as sliders and click wheels. Recognizing the social importance of skin, we show visual design patterns to customize functional touch sensors and allow for a visually aesthetic appearance. Taken together, these contributions enable new types of on-body devices. This includes finger-worn devices, extensions to conventional wearable devices, and touch input stickers, all fostering direct, quick, and discreet input for mobile computing.",https://dl.acm.org/doi/10.1145/2702123.2702391
27,Laput et al.,2016,Ear and Earable,Wearable State,"Attach Earbud, Remove Earbud","Microphone, Speaker",Yes,2,Semantic,No,Yes,Yes,No,Medium,High,High (N=12),High (N=12),No,No,No,No,No,Yes (N=12),No,Music,"Office, University Building",Earbud,Commercial,Yes,No,"Device Control, Music Player, Phone Calls","Acoustic Sensing, Interaction Techniques, Mobile Devices, Novel Input","Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration. However, adding extra, special purpose sensors increases size, price and build complexity. Instead, we use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos, and conclude with an evaluation that quantifies their performance and demonstrates high accuracy.",https://dl.acm.org/doi/10.1145/2856767.2856812
30,Ando et al.,2017,"Face, Head Gestures and Pointing","Head, Jaw","Facial Gesture (Mouth), Pitch, Roll, Yaw",Pressure Sensor,Yes,11,"Coarse, Semantic",Yes,Yes,Yes (Performance Loss),No,Medium,High,High (N=12),Medium (N=6),No,No,No,No,No,Yes (N=12),No,"Music, Sitting",Lab,Earbud,Research Prototype,No,No,"Device Control, Music Player","Barometer, Earphones, Eyes-Free, Facial Movement, Hands-Free, Head Movement, Jaw Movement, Mouth Movement, Outer Ear Interface, Wearable Computing","We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four Open Mouth levels.",https://dl.acm.org/doi/10.1145/3126594.3126649
33,Kikuchi et al.,2017,Ear and Earable,Hand,Pull (Ear),Proximity Sensor,Yes,9,Semantic,No,Yes,Yes,No,Medium,Medium,Low (N=8),Medium (N=8),No,No,No,No,No,Yes (N=8),No,"Sitting, Walking",Lab,Earbud,Research Prototype,Yes,No,"Device Control, Device Input, Music Player","Earphone, Photo Reflective Sensor, Skin Deformation","In this paper, we propose EarTouch, a new sensing technology for ear-based input for controlling applications by slightly pulling the ear and detecting the deformation by an enhanced earphone device. It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an “eyes-free” interface. As for the operation of EarTouch, the shape deformation of the ear is measured by optical sensors. Deformation of the skin caused by touching the ear with the fingers is recognized by attaching optical sensors to the earphone and measuring the distance from the earphone to the skin inside the ear. EarTouch supports recognition of multiple gestures by applying a support vector machine (SVM). EarTouch was validated through a set of user studies.",https://dl.acm.org/doi/10.1145/3098279.3098538
35,Matthies et al.,2017,"Face, Head Gestures and Pointing","Facial Expression, Head","Facial Gesture (Eye), Facial Gesture (Mouth), Pitch, Smile, Yaw","Capacitive Sensor, EMG, Electrical Field Sensing",Yes,15,Semantic,Yes,Visual Attention,No,No,Low,Low,Low (N=1),Medium (N=3),No,No,No,No,No,Yes (N=1),No,Sitting,Lab,"Custom Device, Earbud",Research Prototype,No,No,Device Input,"Body Potential Sensing, Electric Field Sensing, Eyes-Free, Facial Expression Control, Hands-Fee, Wearable Computing","EarFieldSensing (EarFS) is a novel input method for mobile and wearable computing using facial expressions. Facial muscle movements induce both electric field changes and physical deformations, which are detectable with electrodes placed inside the ear canal. The chosen ear-plug form factor is rather unobtrusive and allows for facial gesture recognition while utilizing the close proximity to the face. We collected 25 facial-related gestures and used them to compare the performance levels of several electric sensing technologies (EMG, CS, EFS, EarFS) with varying electrode setups. Our developed wearable fine-tuned electric field sensing employs differential amplification to effectively cancel out environmental noise while still being sensitive towards small facial-movement-related electric field changes and artifacts from ear canal deformations. By comparing a mobile with a stationary scenario, we found that EarFS continues to perform better in a mobile scenario. Quantitative results show EarFS to be capable of detecting a set of 5 facial gestures with a precision of 90% while sitting and 85.2% while walking. We provide detailed instructions to enable replication of our low-cost sensing device. Applying it to different positions of our body will also allow to sense a variety of other gestures and activities.",https://dl.acm.org/doi/10.1145/3025453.3025692
38,Carioli et al.,2018,Mouth,Jaw,General Jaw Movement,Piezoelectric Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=4),Sitting,Lab,Earbud,Research Prototype,No,No,Motion Tracking,"Bending of Curved Surfaces, Earcanal Deformation, Piezoelectric Sensor","The earcanal shape is unique for each human being and temporarily changes when the jaw moves due to eating, chewing, or speaking. The earcanal deformation can be studied by the geometrical analysis of a distorted earpiece custom-fitted inside the earcanal, but the distortion of the earpiece is complex in nature and complicated to analyze. An earcanal bending sensor consisting of a thin piezoelectric strip attached to a customfitted earpiece is presented in this paper. An analytical approach based on computing the geometrical parameters of distorted and undistorted earpieces is developed to: 1) estimate the average bending moment and the resulting stress applied to the customfitted earpiece while opening the jaw and 2) calculate the sensitivity of the piezoelectric earcanal bending sensor. The theoretical model is experimentally validated. The proposed approach can be applied to measure the bending of any curved body in general, and custom-fitted earpieces in particular. It, therefore, enables the designing of versatile in-ear sensors capable of tracking jaw activity and evaluating the energy capacity of earcanal deformation for in-ear energy harvesting purposes.",https://ieeexplore.ieee.org/document/8194832
41,Lee et al.,2018,Ear and Earable,Hand,"Drag (Ear), Dwell (Ear), Joystick (Ear), Lift (Ear), Tap (Ear), Toggle (Ear)",Camera,Yes,6,"Coarse, Semantic",No,Yes,No,No,Medium,Medium,N/A,N/A,Yes (N=20),No,No,Yes (N=32),Yes (N=32),No,No,Sitting,"Café, Lab",Custom Device,Research Prototype,Yes,No,"AR/VR, Device Control, Device Input","Augmented Reality, Hand-To-Face Input, Head Mounted Display, Social Acceptability, User Elicitation","Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable handto-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",https://dl.acm.org/doi/10.1145/3242587.3242642
42,Min et al. ,2018,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,No,No,Medium,Medium,Low(N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Activity Recognition, Communication, Data Annotation, Emotion Recognition, Health","Audio Sensing, Earable, Earbud, Kinetic Sensing","In this paper, we explore audio and kinetic sensing on earable devices with the commercial on-the-shelf form factor. For the study, we prototyped earbud devices with a 6-axis inertial measurement unit and a microphone. We systematically investigate the differential characteristics of the audio and inertial signals to assess their feasibility in human activity recognition. Our results demonstrate that earable devices have a superior signal-to-noise ratio under the influence of motion artefacts and are less susceptible to acoustic environment noise. We then present a set of activity primitives and corresponding signal processing pipelines to showcase the capabilities of earbud devices in converting accelerometer, gyroscope, and audio signals into the targeted human activities with a mean accuracy reaching up to 88% in varying environmental conditions.",https://dl.acm.org/doi/10.1145/3211960.3211970
45,Amesaka et al.,2019,"Face, Head Gestures and Pointing, Mouth","Facial Expression, Head","Facial Gesture (Mouth), Roll","Microphone, Speaker",Yes,5,Semantic,Yes,Yes,No,No,Medium,High,Medium (N=11),N/A,No,No,No,No,No,Yes (N=11),No,Sitting,Lab,Earbud,Research Prototype,No,No,"Device Control, Device Input, Music Player","Ear Canal Transfer Function, Facial Expression Recognition, Hearables, Ultrasound","In this study, we propose a new input method for mobile and wearable computing using facial expressions. Facial muscle movements induce physical deformation in the ear canal. Our system utilizes such characteristics and estimates facial expressions using the ear canal transfer function (ECTF). Herein, a user puts on earphones with an equipped microphone that can record an internal sound of the ear canal. The system transmits ultrasonic band-limited swept sine signals and acquires the ECTF by analyzing the response. An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes. We investigated the performance of our proposed method for 21 facial expressions with 11 participants. Moreover, we proposed a signal correction method that reduces positional errors caused by attaching/detaching the device. The evaluation results confirmed that the f-score was 40.2% for the uncorrected signal method and 62.5% for the corrected signal method. We also investigated the practical performance of six facial expressions and confirmed that the f-score was 74.4% for the uncorrected signal method and 90.0% for the corrected signal method. We found the ECTF can be used for recognizing facial expressions with high accuracy equivalent to other related work.",https://dl.acm.org/doi/10.1145/3341163.3347747
46,Ferlini et al.,2019,Head Gestures and Pointing,Head,Yaw,"Accelerometer, Gyroscope, Magnetometer",Yes,1,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=10),"Eating, Speaking, Standing",Lab,Earbud,Commercial,Yes,No,"Accessibility, Motion Tracking","Earables, Head Motion Tracking, Visual Attention","Head tracking is a fundamental component in visual attention detection, which, in turn, can improve the state of the art of hearing aid devices. A multitude of wearable devices for the ear (so called earables) exist. Current devices lack a magnetometer which, as we will show, represents a big challenge when one tries to use them for accurate head tracking. In this work we evaluate the performance of eSense, a representative earable device, to track head rotations. By leveraging two different streams (one per earbud) of inertial data (from the accelerometer and the gyroscope), we achieve an accuracy up to a few degrees. We further investigate the interference generated by a magnetometer in an earable to understand the barriers to its use in these types of devices.",https://dl.acm.org/doi/10.1145/3345615.3361131
47,Hoelzemann et al.,2019,Ear and Earable,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=7),No,No,No,No,No,"Sitting, Sports, Walking, Walking Stairs","Lab, Sports Site",Earbud,Commercial,Yes,No,Data Annotation,N/A,"Wearable activity recognition research needs benchmark data, which rely heavily on synchronizing and annotating the inertial sensor data, in order to validate the activity classifiers. Such validation studies become challenging when recording outside the lab, over longer stretches of time. This paper presents a method that uses an inconspicuous, earworn device that allows the wearer to annotate his or her activities as the recording takes place. Since the ear-worn device has integrated inertial sensors, we use cross-correlation over all wearable inertial signals to propagate the annotations over all sensor streams. In a feasibility study with 7 participants performing 6 different physical activities, we show that our algorithm is able to synchronize signals between sensors worn on the body using cross-correlation, typically within a second. A comfort rating scale study has shown that attachment is critical. Button presses can thus define markers in synchronized activity data, resulting in a fast, comfortable, and reliable annotation method.",https://dl.acm.org/doi/10.1145/3345615.3361136
51,Odoemelem et al.,2019,Head Gestures and Pointing,Head,"Pitch, Roll","Accelerometer, Gyroscope",Yes,2,Fine,Yes,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Commercial,Yes,No,"Accessibility, Device Control, Motion Tracking",N/A,"Head motion-based interfaces for controlling robot arms in real time have been presented in both medical-oriented research as well as human-robot interaction. We present an especially minimal and low-cost solution that uses the eSense [1] ear-worn prototype as a small head-worn controller, enabling direct control of an inexpensive robot arm in the environment. We report on the hardware and software setup, as well as the experiment design and early results.",https://dl.acm.org/doi/10.1145/3345615.3361138
54,Yan et al.,2019,Hand Gestures and Location,Hand,Cover (Face),Microphone,Yes,1,Semantic,No,Yes,No,No,Medium,Medium,High (N=12),Medium (N=12),No,Yes (N=12),Yes (N=12),Yes (N=17),Yes (N=17),Yes (N=12),No,Sitting,"Lab, Office",Earbud,Commercial,Yes,No,"AR/VR, Device Input, Privacy","Hand Gesture, Voice Input","We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is per formed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the differ ence of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.",https://dl.acm.org/doi/10.1145/3332165.3347950
55,Cao et al.,2020,Ear and Earable,Wearable State,Wear (Earable),"Microphone, Speaker",Partly,1,Fine,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,Yes (N=5),"Distance, Obstacles, Walking",Lab,Earbud,Commercial,Yes,No,"AR/VR, Device Control, Device Input, Motion Tracking","Earable Sensing, Earphone-Based Acoustic Sensing, Motion Tracking","Acoustic motion tracking is an exciting new research area with promising progress in the last few years. Due to the inherent low propagation speed in the air, acoustic signals have the unique advantage of fine sensing granularity compared to RF signals. Speakers and microphones nowadays are pervasively available in devices surrounding us, such as smartphones and voice-controlled smart speakers. Though promising, one fundamental issue hindering the adoption of acoustic-based motion tracking is that the positions of microphones and speakers inside a device are fixed, which greatly limits the flexibility of acoustic motion tracking. In this work, we propose a new modality of acoustic motion tracking using earphones. Earphone-based tracking mitigates the constraints associated with traditional smartphone-based tracking. With novel designs and comprehensive experiments, we show earphone-based motion tracking can achieve a great flexibility and a high accuracy at the same time. We believe this is an important step towards “earable” sensing.",https://dl.acm.org/doi/10.1145/3384419.3430730
61,Xu et al.,2020,"Ear and Earable, Hand Gestures and Location",Hand,"Slide (Ear), Slide (Face), Tap (Ear), Tap (Face)",Microphone,Yes,8,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=18),Medium (N=18),Yes (N=16),Yes (N=12),Yes (N=16),No,Yes (N=16),Yes (N=18),No,"Noise, Sitting",Lab,Earbud,Commercial,Yes,No,"AR/VR, Communication, Device Input, Music Player, Phone Calls","Face and Ear Interaction, Gesture Recognition, Wireless Earbuds","Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.",https://dl.acm.org/doi/10.1145/3313831.3376836
62,Yang et al.,2020,Head Gestures and Pointing,Head,"Pitch, Roll, Yaw","Accelerometer, Gyroscope",No,1,Fine,Yes,Visual Attention,No,No,Medium,High,Medium (N=7),N/A,No,No,No,No,No,Yes (N=7),Yes (N=7),"Standing, Walking",University Building,Headphone,Commercial,Yes,No,"AR/VR, Activity Recognition","Acoustics, Augmented Reality, Dead Reckoning, Head Related Transfer Function (Hrtf), Indoor Localization, Inertial Measurement Unit (Imu), Motion Tracking, Sensor Fusion, Smart Earphones, Spatial Audio, Wearable Computing","This paper aims to use modern earphones as a platform for acoustic augmented reality (AAR). We intend to play 3D audio-annotations in the user’s ears as she moves and looks at AAR objects in the environment. While companies like Bose and Microsoft are beginning to release such capabilities, they are intended for outdoor environments. Our system aims to explore the challenges indoors, without requiring any infrastructure deployment. Our core idea is two-fold. (1) We jointly use the inertial sensors (IMUs) in earphones and smartphones to estimate a user’s indoor location and gazing orientation. (2) We play 3D sounds in the earphones and exploit the human’s responses to (re)calibrate errors in location and orientation. We believe this fusion of IMU and acoustics is novel, and could be an important step towards indoor AAR. Our system, Ear-AR, is tested on 7 volunteers invited to an AAR exhibition – like a museum – that we set up in our building’s lobby and lab. Across 60 different test sessions, the volunteers browsed different subsets of 24 annotated objects as they walked around. Results show that Ear-AR plays the correct audio-annotations with good accuracy. The user-feedback is encouraging and points to further areas of research and applications.",https://dl.acm.org/doi/10.1145/3372224.3419213
63,Fan et al.,2021,Ear and Earable,"Hand, Wearable State","Slide (Earable), Tap (Earable)",Pressure Sensor,Yes,2,Semantic,No,Yes,No,No,Medium,High,High (N=1),Medium (N=1),No,No,No,No,No,Yes (N=1),No,"Music, Sitting",Lab,Custom Device,Research Prototype,No,No,"Device Control, Music Player","Earable Computing, Heartrate Monitoring, Touch Gesture Control, User Identification, Voice Communication, Wearable Devices","Headphones continue to become more intelligent as new functions (e.g., touch-based gesture control) appear. These functions usually rely on auxiliary sensors (e.g., accelerometer and gyroscope) that are available in smart headphones. However, for those headphones that do not have such sensors, supporting these functions becomes a daunting task. This paper presents HeadFi, a new design paradigm for bringing intelligence to headphones. Instead of adding auxiliary sensors into headphones, HeadFi turns the pair of drivers that are readily available inside all headphones into a versatile sensor to enable new applications spanning across mobile health, user-interface, and context-awareness. HeadFi works as a plug-in peripheral connecting the headphones and the pairing device (e.g., a smartphone). The simplicity (can be as simple as only two resistors) and small form factor of this design lend itself to be embedded into the pairing device as an integrated circuit. We envision HeadFi can serve as a vital supplementary solution to existing smart headphone design by directly transforming large amounts of existing “dumb” headphones into intelligent ones. We prototype HeadFi on PCB and conduct extensive experiments with 53 volunteers using 54 pairs of non-smart headphones under the institutional review board (IRB) protocols. The results show that HeadFi can achieve 97.2%–99.5% accuracy on user identification, 96.8%–99.2% accuracy on heart rate monitoring, and 97.7%–99.3% accuracy on gesture recognition.",https://dl.acm.org/doi/10.1145/3447993.3448624
65,Gashi et al.,2021,"Face, Head Gestures and Pointing","Facial Expression, Head","Facial Gesture (Mouth), Pitch, Smile, Yaw","Accelerometer, Gyroscope",Yes,4,Semantic,Yes,Yes,Yes,No,Low,Medium,Low (N=21),N/A,No,No,No,No,No,Yes (N=21),No,Sitting,Lab,Earbud,Commercial,No,No,"Activity Recognition, Feedback System, Social Interaction, Video Conference","Earable Computing, Facial Expressions Recognition, Head Gestures Detection, Hierarchical Classification, Transfer Learning","Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions. Several researchers have thus tackled the problem of automatically recognizing head gestures and facial expressions, mainly leveraging video data. In this paper, we instead consider inertial signals collected from unobtrusive, earmounted devices. We focus on typical activities performed during social interactions – head shaking, nodding, smiling, talking and yawning – and propose a hierarchical classification approach to discriminate them from each other. Further, we investigate whether the transfer of knowledge learned from publicly available datasets leads to further performance improvements. Our results show that the combined use of our hierarchical approach and transfer learning allows the classifier to discriminate head and mouth activities with an F1 score of 84.79, smile, talk and yawn with an F1 score of 45.42, and nodding and head shaking with an F1 score of 88.24, outperforming shallow classifiers by 2-9 percentage points.",https://dl.acm.org/doi/10.1145/3462244.3479921
66,Hashem et al.,2021,Head Gestures and Pointing,Head,Yaw,Bluetooth,No,1,Coarse,Yes,Visual Attention,No,No,Medium,High,High (N=1),Low (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Standing",Lab,Earbud,Commercial,Yes,No,Device Control,"Ble, Earable Computing, Earables, Head Orientation, Iot, Smart Earphones, Wearables","We present Look&Lock: a novel ubiquitous system that utilizes BLE communication between smart appliances in the environment and earables worn by the user to track head orientation and determine which device she is looking at. By harnessing this information, it is possible to lock on that device in a multi-device environment and allow only it to respond to user gestures without ambiguity. Our system leverages commercial off-the-shelf earables to provide accurate training-free head orientation tracking that is robust in different room settings. We implement a prototype using Android phones and eSense, a commercially available multi-sensory personal earable device. Our evaluation shows that Look&Lock can correctly identify devices inside a user’s field of view with accuracy up to 100% and is robust to different configurations and room settings.",https://dl.acm.org/doi/10.1145/3446382.3448653
67,Islam et al.,2021,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Gyroscope",No,2,Semantic,Yes,Yes,N/A,No,Medium,Medium,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Earbud,Commercial,No,No,"Accessibility, Activity Recognition, Health","Earables, Esense, Health Care, Wearable; Activity Recognition","Detecting head- and mouth-related human activities of elderly people are very important for nurse care centers. They need to track different types of activities of elderly people like swallowing, eating, etc., to measure the health status of elderly people. In this regard, earable devices open up interesting possibilities for monitoring personal-scale behavioral activities. Here, we introduce activity recognition based on an earable device called ‘eSense’. It has multiple sensors that can be used for human activity recognition. ‘eSense’ has a 6-axis inertial measurement unit with a microphone and Bluetooth. In this paper, we propose an activity recognition framework using eSense device. We collect accelerometer and gyroscope sensor data from eSense device to detect head- and mouth-related activities along with other normal human activities. We evaluated the classification performance of the classifier using both accelerometer and gyroscope data. For this work, we develop a smartphone application for data collection from the eSense. Several statistical features are exploited to recognize head- and mouth-related activities (e.g., head nodding, head shaking, eating, and speaking), and regular activities (e.g., stay, walk, and speaking while walking). We explored different types of machine learning approaches like Convolutional Neural Network (CNN), Random Forest (RnF), K-Nearest Neighbor (KNN), Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), etc., for classifying activities. We have achieved satisfactory results. Our results show that using both accelerometer and gyroscope sensors can improve performance. We achieve accuracy of 80.45% by LDA, 93.34% by SVM, 91.92% by RnF, 91.64% by KNN, and 93.76% by CNN while we exploit both accelerometer and gyroscope sensor data together. The results demonstrate the prospect of eSense device for detecting human activities in various healthcare monitoring system.",https://link.springer.com/chapter/10.1007/978-981-15-8944-7_11
68,Jin et al.,2021,Hand Gestures and Location,Hand,"Close (Mid-Air), Sign Language (Sentences), Sign Language (Words), Tap (Mid-Air)","Microphone, Speaker",Yes,74,Semantic,No,Yes,No,No,Low,High,Medium (N=8),High (N=2),No,No,No,No,No,Yes (N=8),No,"Distance, Orientation, Standing","Office, Outdoors, University Building",Headphone,Research Prototype,Yes,No,Accessibility,"Acoustic Sensing, Earphones, Sign Language Gesture Recognition","We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians.",https://dl.acm.org/doi/10.1145/3463519
71,Laporte et al.,2021,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,Yes,No,Medium,Medium,Low(N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Data Annotation, Social Interaction","Datasets, Earable Computing, Head Gestures Recognition, Memory Recall","Verbal and non-verbal activities convey insightful information about people’s affect, empathy, and engagement during social interactions. In this paper, we investigate the usage of inertial sensors to recognize verbal (e.g., speaking), non-verbal (e.g., head nodding, shaking) and other activities (e.g., eating, no movement). We implement an end-to-end deep neural network to distinguish among these activities. We then explore the generalizability of the approach in three scenarios: (1) using new data to detect a known activity from a known user, (2) detecting a novel activity of a known user and (3) detecting the activity of an unknown user. Results show that using accelerometer and gyroscope sensors, the model achieves a balanced accuracy of 55% when tested on data from a new user, 41% on a new activity of an existing user, and 80% on new data of a known activity from an existing user. The results are between 7-47 percentage points higher than baseline classifiers.",https://dl.acm.org/doi/10.1145/3460418.3479322
72,Ma et al.,2021,Hand Gestures and Location,Hand,Tap (Face),Microphone,Yes,5,Semantic,No,Yes,No,Yes,Medium,Medium,High (N=29),High (N=4),No,No,No,No,No,Yes (N=29),No,"Music, Sitting","Lab, Outdoors",Earbud,Research Prototype,Yes,No,"Activity Recognition, Health, Motion Tracking",N/A,"Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3% step counting recall, 98.3% recognition recall for 5 activities, and 97.0% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds’ fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.",https://dl.acm.org/doi/10.1145/3458864.3467680
75,Röddiger et al.,2021,Ear and Earable,Tensor Tympani,Contract,Pressure Sensor,Yes,3,Semantic,Yes,Yes,Yes,No,High,High,High (N=16),Medium (N=16),No,Yes (N=8),Yes (N=16),No,No,Yes (N=16),No,"Music, Sitting",Lab,Earbud,Research Prototype,Yes,No,"Music Player, Phone Calls","Discreet Interaction, Earables, Hearables, In-Ear Barometry, Subtle Gestures, Tensor Tympani Muscle","We explore how discreet input can be provided using the tensor tympani - a small muscle in the middle ear that some people can voluntarily contract to induce a dull rumbling sound. We investigate the prevalence and ability to control the muscle through an online questionnaire (N=192) in which 43.2% of respondents reported the ability to “ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can be used to detect voluntary tensor tympani contraction in the sealed ear canal. This data was used to train a classifer based on three simple ear rumble “gestures” which achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction, grounded in three manual, dual-task application scenarios (N=8). This highlights the applicability of EarRumble as a low-efort and discreet eyes- and hands-free interaction technique that users found “magical” and “almost telepathic”.",https://dl.acm.org/doi/10.1145/3411764.3445205
79,Alkiek et al.,2022,Hand Gestures and Location,Hand,"Approach (Mid-Air), Recede (Mid-Air), Slide (Mid-Air)",Bluetooth,Yes,8,Semantic,No,Yes,Yes,No,Low,Low,High (N=1),High (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Orientation, Sitting, Standing","Living Room, Office",Earbud,Research Prototype,Yes,No,"Device Control, Music Player, Phone Calls","Earables, Gesture Recognition, Hci; Sensing","Earables have been increasingly gaining attention from consumers and manufacturers alike due to their small footprint, ease of use, and the added accessibility they bring. However, the limited interface of these devices, usually being a single button or force-sensor, inhibits their potential. Earables can provide a much richer experience by extending the ways in which users can interact with them. In this paper, we present EarGest, a novel earable-based hand gesture recognition system that does not require calibration or training, and works with commercially available BLE-enabled earphones and devices. Our proposed system is unique in its ability to leverage Bluetooth to detect hand motion near the ear to recognize gestures. By harnessing information from BLE connections between the wireless earphones and a host device, we accurately detect and classify different hand gestures performed by users, while also determining discrete levels of hand speed. EarGest operates without interfering with the regular functionality of the earphones and introduces minimal energy overhead on the host device. We implement a prototype of the system using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and settings. Results show that our system can detect and classify seven near-ear hand gestures with an accuracy up to 98.5%, as well as identify hand motion speed with 96% accuracy.",https://ieeexplore.ieee.org/abstract/document/9918622
80,Bi & Liu,2022,Head Gestures and Pointing,Head,"Pitch, Roll, Yaw",Accelerometer,Yes,12,Semantic,Yes,Yes,No,Yes,Medium,High,Medium (N=30),High,No,No,No,No,No,Yes (N=30),No,"Ground Material, Sitting, Slope, Speed, Walking",Lab,Earbud,Commercial,No,No,"Health, Safety","Earphones, Head Gesture, Internet of Healthcare Things (Ioht), Metalearning","With the popularity of personal computing devices, people often keep long-term head immobility in front of screens, resulting in the emergence of “phubbers” and “office workers.” The early warning solutions in the Internet of Healthcare Things (IoHT) have brought hope to protect users’ health and safety. However, most existing works cannot recognize the different head gestures during walking, which is also a common cause of text neck and traffic accidents. In addition, they also need a large amount of data to update the model to adapt to the new environment, which reduces the practicality of the model. To solve these problems, we propose a system, CSEar, based on builtin accelerometers of off-the-shelf wireless earphones, which can recognize 12 kinds of head gestures both in resting and walking states. First, an innovative algorithm is designed to detect head gesture signals, especially for the signals mixed with gait. Then, we propose the MetaSensing, a head gesture recognition model that can improve the recognition ability with few samples compared with the existing metalearning algorithms. Finally, the experimental results prove the effectiveness and robustness of the CSEar.",https://ieeexplore.ieee.org/document/9815053
81,Bi et al.,2022,Ear and Earable,Hand,Tap (Earable),Accelerometer,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),"Sitting, Standing","Lab, Train",Earbud,Commercial,No,No,Authentification,"Accelerometer, Earphone, Tap Gesture, User Authentication","The rapid development of the information-centric wireless sensor network (ICWSN) has solved the challenges of information transmission and processing caused by the accelerated growth of wearable devices and the wide deployment of the Internet of Things (IoT) recently. The privacy security is also a growing problem. The existing works use earphones, covert, and user-friendly wearable devices, for user authentication. However, some of the earphone-based authentication solutions need to customize special earphones, which are not universal. Other solutions use microphones and speakers of earphones for authentication, which are susceptible to changes in the auricle’s internal environment, resulting in a decline in performance. To solve this problem, a new authentication solution based on the existing commercial earphones is proposed to authenticate a user by tapping on the earphone rhythmically. This rhythmic tap behavior causes a change of the signal waveform of the built-in accelerometer in the earphone. Based on this, we design a pipeline to authenticate the user’s identity. We first design an event detection algorithm to segment the tap signal accurately. Then, we use the global features calculated based on the event detection algorithm and local features extracted from the convolutional neural network (CNN) for building an authentication model using the Naive Bayes (NB) classifier. Finally, 20 users are recruited to evaluate the experiment and the recognition accuracy reaches 98%. Moreover, we extend the experiment to prove that it has a good performance against the different attacks and is robust in different scenarios.",https://ieeexplore.ieee.org/document/9367286
88,Wang et al.,2022,Head Gestures and Pointing,Head,"Pitch, Yaw",Microphone,No,2,Fine,Yes,Visual Attention,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=12),Sitting,Lab,Headphone,Research Prototype,Yes,No,"Activity Recognition, Device Control, Device Input, Motion Tracking","Acoustic Ranging, Earphone, Head Orientation, Head Pose Estimation","Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7◦ in yaw, and 5.8◦ in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.",https://dl.acm.org/doi/10.1145/3491102.3517698
91,Alkiek et al.,2023,Ear and Earable,Hand,"Pinch (Ear), Press (Ear), Pull (Ear), Slide (Ear), Tap (Ear)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,Medium,High (N=4),N/A,No,No,No,No,No,Yes (N=4),No,Sitting,Lab,Earbud,Commercial,Yes,No,"Device Control, Device Input, Music Player, Phone Calls","Earables, Gesture Recognition, Inertial Sensors, Natural Interface, On-Body Interaction","Earables have been gaining popularity over the past few years for their ease of use and convenience over wired earphones. However, modern-day earables usually have a limited interface, inhibiting their potential as an accessible medium of input. To this end, we present EarBender: an ear-based real-time system that bridges the gap between earables and on-body interaction, providing a more diverse and natural form of interaction with devices. EarBender enables touch-based hand-to-ear gestures on mobile devices by leveraging inertial sensors in commercially available earable devices. Our proposed system detects the slight deformation in a user’s ear resulting from different ear-based actions including swiping and tapping and classifies the action performed. EarBender is designed to be energy-efficient, easy to deploy and robust to different users, requiring little to no calibration. We implement a prototype of EarBender using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and parameter settings. Results show that the system can detect the occurrence of gestures with a 96.8% accuracy and classify seven different hand-to-ear gestures with an accuracy up to 97.4% maintained across four subjects.",https://dl.acm.org/doi/10.1145/3594739.3610671
94,Li et al.,2023,"Ear and Earable, Hand Gestures and Location",Hand,"Calling Gesture, Cover (Ear), Cover (Face), Hold (Face), Pinch (Ear), Support (Face), Thinking Gesture",Microphone,No,8,Semantic,No,Yes,Yes (Performance Loss),No,Low,Medium,High (N=10),N/A,Yes (N=10),No,Yes (N=25),No,Yes (N=25),Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Device Control, Device Input","Acoustic Sensing, Hand Gestures, Sensor Fusion","Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield signifcant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we frst gathered candidate gestures and then applied a structural analysis to them in diferent dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3% for recognizing 3 gestures and 91.5% for recognizing 8 gestures (excluding the ""empty"" gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.",https://dl.acm.org/doi/10.1145/3544548.3581008
95,Panda et al.,2023,"Ear and Earable, Hand Gestures and Location, Head Gestures and Pointing","Hand, Head","Cover (Face), Cup (Mid-Air), Lift (Earable), Pitch, Press (Earable), Roll, Yaw","Accelerometer, Button, Gyroscope, Lidar Sensor, Magnetometer",Yes,11,"Coarse, Fine, Semantic",Partly,Visual Attention,No,No,Medium,High,N/A,N/A,Yes,No,No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,Yes,Yes,Video Conference,"Design Space, Headphones, Research Through Design, Sensing, Wearables","Via Research through Design (RtD), we explore the potential of headphones as a general-purpose input device for both foreground motion-gestures as well as background sensing of user activity. As a familiar wearable device, headphones ofer a compelling site for head-situated interaction and sensing. Using emerging sensing modalities such as inertial motion, capacitive touch sensing, and depth cameras, our implemented prototypes explore sensing and interaction techniques that ofer a range of compelling capabilities. User scenarios include context-aware privacy, gestural audiovisual control, and co-opting natural body language as context to drive animated avatars for ""camera-of"" scenarios in remote workor to co-opt (oft-subconscious) head movements such as dodging attacks in video games to enhance the gameplay experience. Drawing from literature and other frameworks, we situate our prototypes and related techniques in a design space across the dual dimensions of (1) type of input (touch, mid-air, or head orientation); and (2) the context of user action (application, body, or environment). In particular, interactions that combine multiple inputs and contexts at the same time ofer a rich design space of headphonesituated wearable interactions and sensing techniques.",https://dl.acm.org/doi/10.1145/3563657.3596022
99,Zhang et al.,2023,Ear and Earable,Hand,Touch (Ear),"Microphone, Speaker",Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,No,"Music Player, Phone Calls","Acoustic Sensing, Bone Conduction, Human Interface, Positioning","The advancement of semiconductor and battery technologies popularized tiny acoustic wearable devices such as bone conduction wireless headsets. However, this small form factor poses inconvenience when controlling these devices, as they cannot equip large footprint intuitive interfaces such as volume sliders and touch screens. This paper presents a technique using acoustic responses measured by a bone conduction speaker and a microphone to utilize the ear as a touch input interface. We discovered that a finger placed on different parts of the ear affects the acoustic radiation characteristic of the ear, modulating the leaked sound, and by leveraging this effect, the touch position can be estimated. Experimental results show that five distinct frequency responses with five different finger positions can be obtained, which indicates that our method could allow bone conduction headsets to capture continuous finger positions without additional hardware.",https://dl.acm.org/doi/10.1145/3560905.3568075
102,Zhu et al.,2023,Head Gestures and Pointing,Head,"Pitch, Roll, Yaw","Accelerometer, Gyroscope",Yes,12,Semantic,Yes,Yes,Yes,Yes,Medium,High,High (N=15),N/A,No,No,No,No,No,Yes (N=15),No,"Sports, Standing, Walking, Walking Stairs",Lab,Earbud,Research Prototype,Yes,No,"Activity Recognition, Device Control, Music Player, Phone Calls","Composite Activity Recognition, Earable Device, Multi-Task Learning","The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations. We implement a real-time prototype and conduct extensive experiments to evaluate it. The results show that CHAR can recognize 60 kinds of composite activities (12 head gestures and 5 body movements) with high accuracies of 97.0% and 89.7% in user- dependent and independent cases, respectively.",https://ieeexplore.ieee.org/abstract/document/10916516
104,Ge at al.,2024,Head Gestures and Pointing,Head,Yaw,Microphone,No,1,Fine,Yes,Visual Attention,No,No,Medium,High,High (N=6),High (N=6),No,No,No,No,No,Yes (N=6),Yes,"Distance, Noise, Standing","Outdoors, University Building",Earbud,Research Prototype,No,No,"Activity Recognition, Customer Analytics","Acoustic Signal Processing, Human Computer Interaction, Signal Processing, Systems, User Interfaces","Head tracking is a technique that allows for the measurement and analysis of human focus and attention, thus enhancing the experience of human–computer interaction (HCI). Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakers generates a periodically fluctuating sound field, which the user’s two earphones detect. By assessing the distance and angle alterations between the earphones and speakers, we propose a model to determine the user’s head movement and orientation. Our evaluation results indicate a high degree of accuracy in both head movement tracking, with an average tracking error of 2.98 cm,  and head orientation tracking, with an average error of 1.83◦. Furthermore, in a deployed exhibition scenario, we attained an accuracy of 89.2% in estimating the user’s focus direction.",https://ieeexplore.ieee.org/document/10192901
108,Ronco et al.,2024,Hand Gestures and Location,Hand,"Circle (Mid-Air), Hold (Mid-Air), Pinch (Mid-Air), Pull (Mid-Air), Push (Mid-Air), Rub (Mid-Air), Slide (Mid-Air), Tilt (Mid-Air)",mmWaveRadar,Yes,11,Semantic,No,Yes,Yes,No,Low,Medium,High (N=20),N/A,No,No,No,No,No,Yes (N=20),No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,Device Control,"Embedded, Gesture Recognition, Low-Power, Mm-Wave, Radar, Sensor","Smart Internet of Things (IoT) devices are on the rise in popularity, with innovative use cases and applications emerging every year. Including intelligence in these novel systems presents the challenge of integrating interaction and communication in scenarios where traditional interfaces are not viable. Hand Gesture Recognition (HGR) has been proposed as an intuitive Human-Machine Interface, potentially suitable for controlling several classes of devices in the context of the Internet of Things. This paper proposes a low-power in-ear HGR system based on mm-wave radars, efficient spatial and temporal Convolutional Neural Networks and an energy-optimized hardware design. The design is suitable for battery-operated devices, with stringent size and energy constraints, enabling user interaction with wearable devices, but also suitable for home appliances and industrial applications. The proposed machine learning model is characterized thoroughly for robustness and generalization capabilities, achieving 94.9% (single subject) and 86.1% (LeaveOne-Out Cross-validation) accuracy on a set of 11+1 gestures with a model size of only 36 KiB and inference latency of 32.4 ms on a 64 MHz Cortex-M33 microcontroller, making it compatible with real-time applications. The system is demonstrated in a fully integrated, miniaturized in-ear device with a full-system average power consumption of 18.4 mW, a more than 6x improvement on the current state of the art.",https://ieeexplore.ieee.org/document/10562162
109,Sato et al.,2024,Ear and Earable,Hand,"Fold (Ear), Pinch (Ear), Pull (Ear), Swipe (Ear)","Accelerometer, Gyroscope, Magnetometer",Yes,15,Semantic,No,Yes,No,No,Medium,Medium,Low(N=10),Low (N=10),Yes (N=19),No,No,No,No,Yes (N=10),No,"Sitting, Walking",Lab,Earbud,Commercial,No,No,"Communication, Data Annotation, Device Control, Device Input, Health, Music Player, Phone Calls","Gesture Elicitation Study, Hands Gesture Recognition, Hearables, Imu, User-Defined Gesture","Hearables are highly functional earphone-type wearables; however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures. In previous research on hearables for hand input, user understanding and gesture recognition systems have been developed. However, in the realm of user understanding, investigation concerning hand input with hearables remains incomplete, and existing recognition systems have not demonstrated proficiency in discerning user-defined gestures. In this study, we conducted a gesture elicitation study (GES) assuming hand input using hearables under six conditions (three interaction areas × two device shapes). Then, we extracted ear-level gestures that the device’s built-in IMU sensor could recognize from the user-defined gestures and investigated the recognition performance. The results of sitting experiments showed that the gesture recognition rate for in-ear devices was 91.0% and that for ear-hook devices was 74.7%.",https://dl.acm.org/doi/10.1145/3676503
114,Suzuki et al.,2024,Hand Gestures and Location,Hand,"Calling Gesture, Grip (Mid-Air), Squeeze (Mid-Air), Swipe (Mid-Air), Twist (Mid-Air)","Microphone, Speaker",Yes,7,Semantic,No,Yes,Yes,No,Low,Medium,Low (N=20),Low (N=20),Yes (N=11),No,Yes (N=11),No,Yes (N=11),Yes (N=20),No,"Gloves, Music, Sitting, Surplus Person, Walking",Lab,Earbud,Research Prototype,No,No,Device Input,"Acoustic Sensing, Deep Learning, Doppler Efect, Hearables, Mid-Air Gesture Recognition, Sound Leakage","We introduce EarHover, an innovative system that enables mid-air gesture input for hearables. Mid-air gesture input, which eliminates the need to touch the device and thus helps to keep hands and the device clean. However, existing mid-air gesture input methods for hearables have been limited to adding cameras or infrared sensors. By focusing on the sound leakage phenomenon unique to hearables, we have realized mid-air gesture recognition using a speaker and an external microphone that are highly compatible with hearables. The signal leaked to the outside of the device due to sound leakage can be measured by an external microphone, which detects the diferences in refection characteristics caused by the hand’s speed and shape during mid-air gestures. Among 27 types of gestures, we determined the seven suitable gestures for EarHover in terms of signal discrimination and user acceptability. We then evaluated the gesture detection and classifcation performance of two prototype devices (in-ear type/open-ear type) for real-world application scenarios.",https://dl.acm.org/doi/10.1145/3654777.3676367
115,Wang et al.,2024,Hand Gestures and Location,Hand,Slide (Face),Microphone,Yes,5,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,Yes (N=20),No,No,Yes (N=20),No,Yes (N=26),"Make-Up, Music, Noise, Sitting, Sports, Standing, Walking","Car, Living Room, Office, Outdoors",Earbud,Research Prototype,No,No,Authentification,"Biometrics, Earable, Fingerprint, Friction, User Authentication","Ear wearables (earables) are emerging platforms that are broadly adopted in various applications. There is an increasing demand for robust earables authentication because of the growing amount of sensitive information and the IoT devices that the earable could access. Traditional authentication methods become less feasible due to the limited input interface of earables. Nevertheless, the rich head-related sensing capabilities of earables can be exploited to capture human biometrics. In this paper, we propose EarSlide, an earable biometric authentication system utilizing the advanced sensing capacities of earables and the distinctive features of acoustic fingerprints when users slide their fingers on the face. It utilizes the inward-facing microphone of the earables and the face-ear channel of the ear canal to reliably capture the acoustic fingerprint. In particular, we study the theory of friction sound and categorize the characteristics of the acoustic fingerprints into three representative classes, pattern-class, ridge-groove-class, and coupling-class. Different from traditional fingerprint authentication only utilizes 2D patterns, we incorporate the 3D information in acoustic fingerprint and indirectly sense the fingerprint for authentication. We then design representative sliding gestures that carry rich information about the acoustic fingerprint while being easy to perform. It then extracts multi-class acoustic fingerprint features to reflect the inherent acoustic fingerprint characteristic for authentication. We also adopt an adaptable authentication model and a user behavior mitigation strategy to effectively authenticate legit users from adversaries. The key advantages of EarSlide are that it is resistant to spoofing attacks and its wide acceptability. Our evaluation of EarSlide in diverse real-world environments with intervals over one year shows that EarSlide achieves an average balanced accuracy rate of 98.37% with only one sliding gesture.",https://dl.acm.org/doi/10.1145/3643515
116,Wang et al.,2024,Hand Gestures and Location,Hand,"Slide (Face), Tap (Face)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),Sitting,Lab,Earbud,Commercial,No,No,Authentification,"Adversarial Learning, Implicit Authentication, Wearable Computing","The surge in popularity of wireless headphones, particularly wireless earbuds, as smart wearables, has been notable in recent years. These devices, empowered by artificial intelligence (AI), are broadening their utility in areas such as speech recognition, augmented reality, pose recognition, and health care monitoring, thereby enriching user experiences through novel interactive interfaces driven by embedded sensors. However, the widespread adoption of wireless earbuds has spurred concerns regarding security and privacy, necessitating robust bespoke security measures. Despite the miniaturization of mobile chips enabling the integration of sophisticated algorithms into smart wearables, the research and industrial communities have yet to accord adequate attention to earbud security. This article focuses on empowering wireless earbuds to authenticate their legitimate users, tackling the challenges associated with conventional authentication methods. Instead of relying on input interface authentication methods like PIN or lock patterns, this research delves into leveraging inertial measurement unit (IMU) data collected during interactions with devices to extract novel biometric features, presenting an alternative approach that nonetheless confronts challenges related to signal capture and interference. Consequently, we propose and design BudsAuth, an implicit user authentication framework that harnesses built-in IMU sensors in smart earbuds to capture vibration signals induced by onface touching interactions with the earbuds. These vibrations are utilized to deliver continuous and implicit user authentication with high precision and compatibility across various earbud models. Extensive evaluation demonstrates BudsAuth’s capability to achieve an equal error rate (EER) of 0.0003, representing an approximate 99.97% accuracy with seven consecutive samples of interactive gestures for implicit authentication.",https://ieeexplore.ieee.org/abstract/document/10478100
117,Wang et al.,2024,Hand Gestures and Location,Hand,Write (Face),Microphone,Yes,36,Semantic,No,Yes,Yes,Yes,Low,Medium,High (N=10),High,No,Yes (N=20),No,No,No,Yes (N=10),No,"Head Movement, Sitting, Standing, Walking","Car, Living Room, Office, Outdoors",Earbud,Research Prototype,No,No,"Device Input, Privacy","Acoustic Sensing, Earable, Face and Ear Interaction, Gestures Recognition","As earables gain popularity, there emerges a need for intuitive user interfaces that adapt to diverse daily scenarios. Traditional methods like touchscreens and voice control often fall short in environments like movie theatres, where silence and darkness are required, or on busy streets where visual distraction introduces extra risk. We propose an innovative earable-based system utilizing unique acoustic friction generated by fingers for alphanumeric input. Our approach digs into the acoustic friction theory, applying this knowledge to better understand the transformation from 2D handwriting into a 1D acoustic time series. This theoretical foundation guides our system design and feature extraction. Specifically, we have redesigned certain characters to enhance their acoustic distinctiveness without compromising the natural handwriting style of users, ensuring the system userfriendly. Our system combines DenseNet and GRU architectures in a multimodal model, refined through transfer learning to adapt to diverse user behaviors. Tested in real-world scenarios with 10 participants, our system achieves a 95% accuracy in recognizing both letters and numbers.",https://ieeexplore.ieee.org/document/10637602
119,Yang et al.,2024,"Ear and Earable, Hand Gestures and Location",Hand,"Approach (Mid-Air), Click (Mid-Air), Close (Mid-Air), Cover (Face), Open (Mid-Air), Pinch (Ear), Pinch (Face), Press (Face), Scratch (Face), Slide (Mid-Air)","Microphone, Speaker",Yes,10,Semantic,No,Yes,No,No,Low,Medium,High (N=22),Medium (N=1),No,Yes (N=22),Yes (N=22),No,Yes (N=22),Yes (N=22),No,"Distance, Eating, Hydration, Music, Noise, Sitting, Speaking, Sports, Standing, Walking",Lab,Earbud,Research Prototype,No,No,"AR/VR, Device Control, Health, Music Player","Acoustic Sensing, Gesture Detection, Wearable Computing","We present MAF, a novel acoustic sensing approach that leverages the commodity hardware in bone conduction earphones for handto-face gesture interactions. Briefly, by shining audio signals with bone conduction earphones, we observe that these signals not only propagate along the surface of the human face but also dissipate into the air, creating an acoustic field that envelops the individual’s head. We conduct benchmark studies to understand how various handto-face gestures and human factors influence this acoustic field. Building on the insights gained from these initial studies, we then propose a deep neural network combined with signal preprocessing techniques. This combination empowers MAF to effectively detect, segment, and subsequently recognize a variety of hand-to-face gestures, whether in close contact with the face or above it. Our comprehensive evaluation based on 22 participants demonstrates that MAF achieves an average gesture recognition accuracy of 92% across ten different gestures tailored to users’ preferences.",https://dl.acm.org/doi/10.1145/3613904.3642437

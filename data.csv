ID,First Author,Year, Location,Input Body Part,Gesture,Sensing_PANEL_Sensors,Sensing_PANEL_No Additional Sensing,Interaction_PANEL_Number of Selected Gestures,Interaction_PANEL_Resolution,Interaction_PANEL_Hands-Free,Interaction_PANEL_Eyes-Free,Interaction_PANEL_Possible on One Ear,Interaction_PANEL_Adaptation of the Interaction Detection Algorithm to the Individual User,Interaction_PANEL_Discreetness of Interaction Techniques,Interaction_PANEL_Social Acceptability of Interaction Techniques,Interaction_PANEL_Accuracy of Interaction Recognition,Interaction_PANEL_Robustness of Interaction Detection,Study_PANEL_Elicitation Study,Study_PANEL_Usability Evaluations,Study_PANEL_Cognitive Ease Evaluations,Study_PANEL_Discreetness of Interactions Evaluations,Study_PANEL_Social Acceptability of Interactions Evaluations,Study_PANEL_Accuracy of Interactions Evaluations,Study_PANEL_Alternative Interaction Validity Evaluations,Study_PANEL_Evaluation of Different Conditions,Study_PANEL_Evaluation of Different Settings,Device_PANEL_Earphone Type,Device_PANEL_Development Stage,Device_PANEL_Real-Time Processing,Device_PANEL_On-Device Processing,Applications_PANEL_Intended Applications,Keywords,Abstract,Study Link
1,Weisenberger et al.,1987,Actuation,N/A,Vibration (Actuation),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,N/A,N/A,"Accessibility, Health",N/A,"A binaural earmold sound-to-tactile aid was constructed by inserting a vibrating element into a Lucite earmold. The earmold could be vibrated at either 80 Hz (when incoming acoustic signals were below 2000 I-Ir), at 300 Hz (when incoming acoustic signals were above 2000 Hz), or both (when incoming acoustic signals were broadband). Subjects were fitted with one of these bimodal vibrating earmolds in each ear. Normal-hearing and hearing-impaired subjects were tested in three tasks: sound localization, errvironmental sound identification, and syllable rhythm and stress. The device provided some benefit to performance, although the amounts of improvement varied across tasks and subjects. Possible modifications in device design, and potential combinations of auditory and tactile input via earmold systems, are discussed.",https://www.researchgate.net/publication/19575713_Development_and_preliminary_evaluation_of_an_earmold_sound-to-tactile_aid_for_the_hearing-impaired
2,Brewster et al.,2003,Head Gestures and Pointing,Head,"Pitch, Roll","Accelerometer, Gyroscope, Magnetometer",Yes,1,Coarse,Yes,Yes,No,No,Medium,High,N/A,N/A,No,Yes (N=18),No,No,No,No,No,Sitting,Lab,Custom Device,Commercial,Yes,No,Device Input,"Gestural Interaction, Wearable Computing","Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users’ gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.",https://dl.acm.org/doi/abs/10.1145/642611.642694
3,Metzger et al.,2004,"Hand Gestures and Location, Head Gestures and Pointing","Hand, Head","Hold (Mid-Air), Roll, Slide (Mid-Air)","Accelerometer, Proximity Sensor",Yes,7,"Coarse, Semantic",No,Yes,No,No,Low,Low,High (N=1),N/A,No,No,No,No,No,Yes (N=1),No,Sitting,Lab,Headphone,Research Prototype,Yes,No,"Accessibility, Device Control, Music Player, Phone Calls",N/A,"We present FreeDigiter, an interface for mobile devices which enables rapid entry of digits using finger gestures. FreeDigiter is an infrared proximity sensor with a dual axis accelerometer and requires little signal processing. Initial laboratory experiments attain accuracy rates of 99.0%; and the system is tolerant to highly varying lighting conditions. The FreeDigiter system requires little power and could be implemented in a very small form factor appropriate for controlling in–ear hearing aids, small MP3 players, and hands–free mobile phone headsets.",https://ieeexplore.ieee.org/document/1364684
4,Buil & Hollemans,2005,Ear and Earable,Hand,Press (Earable),Button,Yes,3,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,Music Player,N/A,"The touch headphones are a solution for providing playback and volume controls on in-ear type headphones.  One of the issues with placing controls on earpieces is that applied pressure is transferred to the inner ear,  which potentially creates discomfort. The experiment described in this short paper shows that conventional button switches  are not well accepted. Users preferred to operate a button on an earpiece with a force of around 85 grams.",https://ieeexplore.ieee.org/abstract/document/1550805
5,Buil et al.,2005,Ear and Earable,"Hand, Wearable State","Attach Earbud, Hold (Earable), Remove Earbud, Tap (Earable)",Capacitive Sensor,Yes,5,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Research Prototype,Yes,No,"Device Control, Music Player","Capacitive Touch Control, Headphones, MP3, Music Playback, User Interface, User System Interaction","The Touch Headphones are meant for portable music players and aim to present an improvement to the conventional remote control in the headphone wire, and a solution for controls on wireless in-ear type headphones. Two capacitive touch sensors per earpiece sense when earpieces are being tapped on, and being put in or out.",https://dl.acm.org/doi/abs/10.1145/1085777.1085877
6,Manabe & Fukumoto,2006,Eye-Tracking,Eye,"Horizontal Gaze, Vertical Gaze",EOG,Yes,2,Coarse,Yes,No,No,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=6),Sitting,Lab,Headphone,Research Prototype,No,No,"AR/VR, Device Control, Music Player","EOG, Gaze Interface, Headphone, Kalman Filter, Wearable","A headphone-type gaze detector for a full-time  wearable interface is proposed. It uses a Kalman filter  to analyze multiple channels of EOG signals measured  at the locations of headphone cushions to estimate  gaze direction. Evaluations show that the average  estimation error is 4.4° (horizontal) and 8.3° (vertical),  and that the drift is suppressed to the same level as in  ordinary EOG. The method is especially robust against  signal anomalies. Selecting a real object from among  many surrounding ones is one possible application of  this headphone gaze detector.",https://dl.acm.org/doi/abs/10.1145/1125451.1125655
7,Simpson et al.,2008,Mouth,Teeth,Click,Accelerometer,Yes,1,Semantic,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"Accessibility, Device Control","Assistive Technology, Computer Interfaces, Spinal Cord Injury, Tooth-Click","People with severe upper limb paralysis use devices that monitor head movements to control computer cursors. The three most common methods for producing mouse button clicks are dwell-time, sip-and-puff control, and voice-recognition. Here, we tested a new method in which small tooth-clicks were detected by an accelerometer contacting the side of the head. The resulting signals were paired with head tracking technology to provide combined cursor and button control. This system was compared with sip-andpuff control and dwell-time selection. A group of 17 people with disabilities and ten people without disabilities tested each system by producing mouse clicks as inputs to two software programs. Tooth-click/head-mouse control was much faster than dwell-time control and not quite as fast as sip-and-puff control, but it was more reliable and less cumbersome than the latter.",https://ieeexplore.ieee.org/document/4473368
8,Tamaki et al.,2009,Hand Gestures and Location,Hand,"Close (Mid-Air), Hold (Mid-Air), Open (Mid-Air), Point (Mid-Air), Tilt (Mid-Air)","Camera, Laser, Projector",Yes,4,"Fine, Semantic",No,Visual Attention,Yes,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=1),Standing,Lab,Earbud,Research Prototype,Yes,No,"Device Control, Device Input, Music Player, Phone Calls","Audio Feedback, Hand Gesture, Input Device, Interaction Device, Laser, Projector, Wearable","Existing wearable hand gesture interaction devices are very bulky and cannot be worn in everyday life, because of the presence of a large visual feedback device. In particular, an eyeglass-type head-mounted display is very large for constant usage. To solve this problem, we propose Brainy Hand, which is a simple wearable device that adopts laser line, or more specifically, a mini-projector as a visual feedback device. Brainy Hand consists of a color camera, an earphone, and a laser line or mini-projector. This device uses a camera to detect 3D hand gestures. The earphone is used for receiving audio feedback. In this study, we introduce several user interfaces using Brainy Hand. (e.g., music player, phone)",https://dl.acm.org/doi/abs/10.1145/1520340.1520649?casa_token=6gSF3eNSnpoAAAAA:CQWgiFfxP5gooWAqsVc4zg96VRrgY5i6-nh85Ta7eNyBjDuutYUhqRo5eGOiDyhDejo3WqiH2xrvHw
9,Simpson et al.,2010,Mouth,Teeth,Click,Accelerometer,Yes,1,Semantic,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"Accessibility, Device Control","Assistive Technology, Computer Access, Spinal Cord Injury, Tetraplegia","Background. Computer access can play an important role in employment and leisure activities following spinal cord injury. The authors’ prior work has shown that a tooth-click detecting device, when paired with an optical head mouse, may be used by people with tetraplegia for controlling cursor movement and mouse button clicks. Objective. To compare the efficacy of tooth clicks to speech recognition and that of an optical head mouse to a gyrometer head mouse for cursor and mouse button control of a computer. Methods. Six able-bodied and 3 tetraplegic subjects used the devices listed above to produce cursor movements and mouse clicks in response to a series of prompts displayed on a computer.The time taken to move to and click on each target was recorded. Results. The use of tooth clicks in combination with either an optical head mouse or a gyrometer head mouse can provide hands-free cursor movement and mouse button control at a speed of up to 22% of that of a standard mouse.Tooth clicks were significantly faster at generating mouse button clicks than speech recognition when paired with either type of head mouse device. Conclusions. Tooth-click detection performed better than speech recognition when paired with both the optical head mouse and the gyrometer head mouse. Such a system may improve computer access for people with tetraplegia.",https://journals.sagepub.com/doi/10.1177/1545968309341647
10,Gamper et al.,2011,Head Gestures and Pointing,Head,Yaw,Microphone,Yes,1,Fine,Yes,Yes,No,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=3),Sitting,Lab,Headphone,Commercial,No,No,"AR/VR, Motion Tracking, Video Conference",N/A,"A head orientation tracking system using binaural headset microphones is proposed. Unlike previous approaches, the proposed method does not require anchor sources, but relies on speech signals of the wearers of the binaural headsets. From the binaural microphone signals, time of arrival (TOA) and time difference of arrival (TDOA) estimates are obtained. The tracking is performed using a particle filter integrated with a maximum likelihood estimation function. In a case study, the proposed method is used to track the head orientations of three conferees in a meeting scenario. With an accuracy of about 10 degrees, the proposed method is shown to outperform a reference method which achieves an accuracy of about 35 degrees.",https://hannesgamper.com/wp-content/papercite-data/pdf/gamper2011b.pdf
11,Manabe & Fukumoto,2011,Ear and Earable,Hand,Tap (Earable),Speaker,Yes,2,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=6),Medium (N=6),No,No,No,No,No,Yes (N=6),No,"Head Movement, Jumping, Sitting, Walking, Walking Stairs",Lab,"Earbud, Headphone",Research Prototype,Yes,No,"Device Control, Music Player","Headphones, Input Device, Tap, Wearable","A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.",https://doi.org/10.1145/2047196.2047236
12,Matsumura & Fukumoto,2012,Ear and Earable,Wearable State,"Share (Earable), Wear (Earable)","EMG, Proximity Sensor",Yes,2,Semantic,Yes,Yes,No,No,High,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,"Earbud, Headphone",Research Prototype,Yes,Yes,Music Player,"Earphones, Implicit Interaction, Intelligent Interface","We present universal earphones that use both a proximity sensor and a skin conductance sensor and we demonstrate several implicit interaction techniques they achieve by automatically detecting the context of use. The universal earphones have two main features. The first involves detecting the left and right sides of ears, which provides audio to either ear, and the second involves detecting the shared use of earphones and this provides mixed stereo sound to both earphones. These features not merely free users from having to check the left and right sides of earphones, but they enable them to enjoy sharing stereo audio with other people.",https://dl.acm.org/doi/abs/10.1145/2166966.2167025
13,Sano et al.,2010,Face,Facial Expression,Smile,EMG,Yes,1,Semantic,Yes,Yes,Yes,No,Low,Medium,Low (N=3),N/A,No,No,No,No,No,Yes (N=3),No,Sitting,Lab,Earbud,Research Prototype,No,No,"Communication, Data Annotation, Music Player","Accelometer, Elecromyogram, Photoplethysmographm, Pulse Sensor, Wearable Sensor","We propose novel ways of interactions using biosignals. We developed a prototype earphone with three kinds of biosignal sensors: pulse wave, electromyogram (EMG) and acceleration sensors. Using this sensor system, we implemented three new applications - automatic music selection, tactile and visual communication and automatic metadata annotation. We also present results of preliminary subjective and objective evaluations.",https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=194115d845f69c4ad23430dacc278f99e0cd5dcd
14,Tessendorf and Derleth,2012,Ear and Earable,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=21),Yes (N=21),No,Yes (N=21),No,No,"Head Movement, Jumping, Walking, Walking Stairs",Lab,Custom Device,Research Prototype,Yes,Yes,"Accessibility, Data Annotation",N/A,"In this work we present a newly developed earworn sensing and annotation device to unobtrusively capture head movements in real life situations. It has been designed in the context of developing multimodal hearing instruments (HIs), but is not limited to this application domain. The ear-worn device captures triaxial acceleration, rate of turn and magnetic field and features a one-button-approach for real-time data annotation through the user. The system runtime is over 5 hours at a sampling rate of 128 Hz. In a user study with 21 participants the device was perceived as comfortable and showed a robust hold at the ear. On the example of head acceleration data we perform unsupervised clustering to demonstrate the benefit of head movements for multimodal HIs. We believe the novel technology will help to push the boundaries of HI technology.",https://ieeexplore.ieee.org/abstract/document/6346464
15,Akiyama et al.,2013,Actuation,N/A,Thermal (Actuaction),N/A,N/A,1,Fine,Yes,Yes,No,N/A,High,High,N/A,N/A,No,Yes (N=8),No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,N/A,N/A,"AR/VR, Music Player","Cross-Modal Interface, Musical Interface, Thermal Display","This report proposes a thermal media system, ThermOn, which enables users to feel dynamic hot and cold sensations on their body corresponding to the sound of music. Thermal sense plays a significant role in the human recognition of environments and influences human emotions. By employing thermal sense in the music experience, which also greatly affects human emotions, we have successfully created a new medium with an unprecedented emotional experience. With ThermOn, a user feels enhanced excitement and comfort, among other responses. For the initial prototype, headphone-type interfaces were implemented using a Peltier device, which allows users to feel thermal stimuli on their ears. Along with the hardware, a thermal-stimulation model that takes into consideration the characteristics of human thermal perception was designed. The prototype device was verified using two methods: the psychophysical method, which measures the skin potential response and the psychometric method using a Likert-scale questionnaire and open-ended interviews. The experimental results suggest that ThermOn (a) changes the impression of music, (b) provides comfortable feelings, and (c) alters the listener’s ability to concentrate on music in the case of a rock song. Moreover, these effects were shown to change based on the methods with which thermal stimuli were added to music (such as temporal correspondence) and on the type of stimuli (warming or cooling). From these results, we have concluded that the ThermOn system has the potential to enhance the emotional experience when listening to music.",https://dl.acm.org/doi/abs/10.1145/2493988.2494326
16,Manabe et al.,2013,Eye-Tracking,Eye,Vertical Gaze,EOG,Yes,2,Semantic,Yes,No,No,No,Medium,Medium,N/A,N/A,No,Yes (N=6),No,No,No,No,Yes (N=6),Sitting,Lab,Earbud,Research Prototype,No,No,"Accessibility, Music Player","Conductive Rubber, EOG, Earphone, Electrode, Eye Gesture","An eartip made of conductive rubber that also realizes biopotential electrodes is proposed for a daily-use earphonebased eye gesture input interface. Several prototypes, each with three electrodes to capture Electrooculogram (EOG), are implemented on earphones and examined. Experiments with one subject over a 10 day period reveal that all prototypes capture EOG similarly but they differ as regards stability of the baseline and the presence of motion artifacts. Another experiment conducted on a simple eye-controlled application with six subjects shows that the proposed prototype minimizes motion artifacts and offers good performance. We conclude that conductive rubber with mixed Ag filler is the most suitable setup for daily-use.",http://dx.doi.org/10.1145/2493988.2494329
17,Lissermann et al.,2014,Ear and Earable,Hand,Touch (Ear),Capacitive Sensor,Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,Yes (N=27),No,No,No,No,No,No,"Music, Sitting",Lab,Custom Device,Research Prototype,Yes,No,"Device Control, Gaming, Music Player","Device Augmentation, Ear-Based Interaction, Ear-Worn, Eyes-Free, Mobile Interaction, Multi-Touch, Touch","One of the pervasive challenges in mobile interaction is  decreasing the visual demand of interfaces towards eyes-free  interaction. In this paper, we focus on the unique affordances  of the human ear to support one-handed and eyes-free mobile  interaction. We present EarPut, a novel interface concept and  hardware prototype, which unobtrusively augments a variety  of accessories that are worn behind the ear (e.g. headsets  or glasses) to instrument the human ear as an interactive  surface. The contribution of this paper is three-fold. We  contribute (i) results from a controlled experiment with 27  participants, providing empirical evidence that people are  able to target salient regions on their ear effectively and  precisely, (ii) a first, systematically derived design space  for ear-based interaction and (iii) a set of proof of concept  EarPut applications that leverage on the design space and  embrace mobile media navigation, mobile gaming and smart  home interaction.",https://dl.acm.org/doi/10.1145/2686612.2686655
18,Sahni et al.,2014,Mouth,Speech Apparatus,Silent Speech (Words),"Magnetometer, Proximity Sensor",No,11,Semantic,Yes,Yes,No,No,High,High,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"Accessibility, Device Input, Silent Speech","Mobile Interfaces, Silent Speech Recognition, Wearable Computing","We address the problem of performing silent speech recognition where vocalized audio is not available (e.g. due to a user’s medical condition) or is highly noisy (e.g. during firefighting or combat). We describe our wearable system to capture tongue and jaw movements during silent speech. The system has two components: the Tongue Magnet Interface (TMI), which utilizes the 3-axis magnetometer aboard Google Glass to measure the movement of a small magnet glued to the user’s tongue, and the Outer Ear Interface (OEI), which measures the deformation in the ear canal caused by jaw movements using proximity sensors embedded in a set of earmolds. We collected a data set of 1901 utterances of 11 distinct phrases silently mouthed by six able-bodied participants. Recognition relies on using hidden Markov modelbased techniques to select one of the 11 phrases. We present encouraging results for user dependent recognition.",https://dl.acm.org/doi/10.1145/2634317.2634322
19,Bedri et al.,2015,Mouth,Jaw,General Jaw Movement,Proximity Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,Yes (N=27),No,No,No,No,No,"Eating, Sitting, Speaking, Walking, Walking Stairs",Lab,Earbud,Research Prototype,No,No,"Activity Recognition, Silent Speech","Food Intake, Jaw Gestures, Jaw Motion, Outer Ear, Proximity Sensor, Silent Speech","The human ear seems to be a rigid anatomical part with no apparent activity, yet many facial and body activity can be measured from it. Research apparatuses and commercial products have demonstrated the capability of monitoring hart rate, tongue activities, jaw motion and eye blinking from the ear. In this paper we describe the design and the implementation of the Outer Ear Interface (OEI) which utilizes a set of infrared proximity sensors to measure the deformation in the ear canal caused by the lower jaw movement. OEI has been used in different applications that requires tracking of jaw activity which includes silent speech recognition, jaw gesture detection and food intake monitoring.",https://dl.acm.org/doi/10.1145/2800835.2807933
20,Bleichner et al.,2015,"Brain, Eye-Tracking",Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,Yes,No,Medium,Medium,High (N=12),N/A,No,No,No,No,No,Yes (N=12),No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,Bci-Application,"Ear EEG, Miniaturized, P300 Speller, Ton-Electrodes","Electroencephalography (EEG) allows the study of the brain–behavior relationship in humans. Most of what we have learned with EEG was through observing the brain–behavior relationship under well-controlled laboratory conditions. However, by reducing “normal” behavior to a minimum the ecological validity of the results can be limited. Recent developments toward mobile EEG solutions allow to study the brain–behavior relationship outside the laboratory in more natural situations. Besides mobility and robustness with respect to motion, mobile EEG systems should also interfere as little as possible with the participant’s behavior. For example, natural interaction with other people could be hindered when it is obvious that a participant wears an EEG cap. This study evaluates the signal quality obtained with an unobtrusive solution for EEG monitoring through the integration of miniaturized EEG ton-electrodes into both a discreet baseball cap and an individualized ear piece. We show that such mini electrodes located at scalp and ear locations can reliably record event related potentials in a P300 brain–computer–interface application.",https://physoc.onlinelibrary.wiley.com/doi/full/10.14814/phy2.12362
21,Norton et al.,2015,"Brain, Eye-Tracking",Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,Yes,No,Medium,Medium,High (N=3),N/A,No,No,No,No,No,Yes (N=3),No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,Bci-Application,"Auricle Integration, Brain–Computer Interface, Soft Electronics, Text Speller","Recent advances in electrodes for noninvasive recording of electroencephalograms expand opportunities collecting such data for diagnosis of neurological disorders and brain–computer interfaces. Existing technologies, however, cannot be used effectively in continuous, uninterrupted modes for more than a few days due to irritation and irreversible degradation in the electrical and mechanical properties of the skin interface. Here we introduce a soft, foldable collection of electrodes in open, fractal mesh geometries that can mount directly and chronically on the complex surface topology of the auricle and the mastoid, to provide highfidelity and long-term capture of electroencephalograms in ways that avoid any significant thermal, electrical, or mechanical loading of the skin. Experimental and computational studies establish the fundamental aspects of the bending and stretching mechanics that enable this type of intimate integration on the highly irregular and textured surfaces of the auricle. Cell level tests and thermal imaging studies establish the biocompatibility and wearability of such systems, with examples of high-quality measurements over periods of 2 wk with devices that remain mounted throughout daily activities including vigorous exercise, swimming, sleeping, and bathing. Demonstrations include a text speller with a steadystate visually evoked potential-based brain–computer interface and elicitation of an event-related potential (P300 wave).",https://www.pnas.org/doi/full/10.1073/pnas.1424875112
22,Wang et al.,2015,"Brain, Eye-Tracking",Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,No,No,Medium,Medium,Medium (N=4),N/A,No,No,No,No,No,Yes (N=4),No,Sitting,Lab,Earbud,Research Prototype,No,No,"Bci-Application, Device Control",N/A,"The purpose of this study is to demonstrate an online steady-state visual evoked potential (SSVEP)-based BCI system using EarEEG. EarEEG is a novel recording concept where electrodes are embedded on the surface of earpieces customized to the individual anatomical shape of users’ ear. It has been shown that the EarEEG can be used to record SSVEPs in previous studies. However, a long distance between the visual cortex and the ear makes the signal-to-noise ratio (SNR) of SSVEPs acquired by the EarEEG relatively low. Recently, filter bank- and training data-based canonical correlation analysis algorithms have shown significant performance improvement in terms of accuracy of target detection and information transfer rate (ITR). This study implemented an online four-class SSVEPbased BCI system using EarEEG. Four subjects participated in offline and online BCI experiments. For the offline classification, an average accuracy of 82.71±11.83 % was obtained using 4 seclong SSVEPs acquired from earpieces. In the online experiment, all subjects successfully completed the tasks with an average accuracy of 87.92±12.10 %, leading to an average ITR of 16.60±6.55 bits/min. The results suggest that EarEEG can be used to perform practical BCI applications. The EarEEG has the potential to be used as a portable EEG recordings platform, that could enable real-world BCI applications.",https://ieeexplore.ieee.org/abstract/document/7318845
23,Weigel et al.,2015,Ear and Earable,Hand,Touch (Earable),"Capacitive Sensor, Resistive Sensor",Yes,1,"Coarse, Semantic",No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Custom Device,Research Prototype,Yes,No,"Device Input, Music Player","Electronic Skin, Flexible Sensor, Mobile Computing, On-Body Input, Stretchable Sensor, Touch Input, Wearable Computing","We propose iSkin, a novel class of skin-worn sensors for touch input on the body. iSkin is a very thin sensor overlay, made of biocompatible materials, and is flexible and stretchable. It can be produced in different shapes and sizes to suit various locations of the body such as the finger, forearm, or ear. Integrating capacitive and resistive touch sensing, the sensor is capable of detecting touch input with two levels of pressure, even when stretched by 30% or when bent with a radius of 0.5 cm. Furthermore, iSkin supports single or multiple touch areas of custom shape and arrangement, as well as more complex widgets, such as sliders and click wheels. Recognizing the social importance of skin, we show visual design patterns to customize functional touch sensors and allow for a visually aesthetic appearance. Taken together, these contributions enable new types of on-body devices. This includes finger-worn devices, extensions to conventional wearable devices, and touch input stickers, all fostering direct, quick, and discreet input for mobile computing.",https://dl.acm.org/doi/10.1145/2702123.2702391
24,Ashbrook et al.,2016,Mouth,Teeth,Click,Bone Conduction Microphone,Yes,5,Semantic,Yes,Yes,No,No,High,High,Low (N=20),Medium (N=20),No,No,No,No,No,Yes (N=20),No,"Head Movement, Sitting, Speaking",Lab,Custom Device,Commercial,Yes,No,"Communication, Device Control, Device Input, Music Player, Phone Calls","Audio Interfaces, Bio-Acoustics, Gestures, Subtle Interfaces, Tooth Input, Wearable Computing","We present Bitey, a subtle, wearable device for enabling input via tooth clicks. Based on a bone-conduction microphone worn just above the ears, Bitey recognizes the click sounds from up to five different pairs of teeth, allowing fully handsfree interface control. We explore the space of tooth input and show that Bitey allows for a high degree of accuracy in distinguishing between different tooth clicks, with up to 94% accuracy under laboratory conditions for five different tooth pairs. Finally, we illustrate Bitey’s potential through two demonstration applications: a list navigation and selection interface and a keyboard input method.",http://dx.doi.org/10.1145/2935334.2935389
25,Laput et al.,2016,Ear and Earable,Wearable State,"Attach Earbud, Remove Earbud","Microphone, Speaker",Yes,2,Semantic,No,Yes,Yes,No,Medium,High,High (N=12),High (N=12),No,No,No,No,No,Yes (N=12),No,Music,"Office, University Building",Earbud,Commercial,Yes,No,"Device Control, Music Player, Phone Calls","Acoustic Sensing, Interaction Techniques, Mobile Devices, Novel Input","Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration. However, adding extra, special purpose sensors increases size, price and build complexity. Instead, we use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos, and conclude with an evaluation that quantifies their performance and demonstrates high accuracy.",https://dl.acm.org/doi/10.1145/2856767.2856812
26,Merrill et al.,2016,Brain,Brain Activity,Mental Gesture,EEG,Yes,5,Semantic,Yes,Yes,Yes,No,High,High,N/A,N/A,No,Yes (N=9),Yes (N=9),No,No,No,Yes (N=12),Sitting,Lab,Custom Device,Research Prototype,No,No,"Bci-Application, Device Control",N/A,"While brain-computer interfaces (BCI) based on electroencephalography (EEG) have improved dramatically over the past five years, their inconvenient, headworn form factor has challenged their wider adoption. In this paper, we investigate how EEG signals collected from the ear could be used for “gestural” control of a braincomputer interface (BCI). Specifically, we investigate the efficacy of a support vector classifier (SVC) in distinguishing between mental tasks, or gestures, recorded by a modified, consumer headset. We find that an SVC reaches acceptable BCI accuracy for nine of the subjects in our pool (n=12), and distinguishes at least one pair of gestures better than chance for all subjects. User surveys highlight the need for longer-term research on user attitudes toward in-ear EEG devices, for discreet, non-invasive BCIs.",https://ieeexplore.ieee.org/abstract/document/7516246
27,Nguyen et al.,2016,Eye-Tracking,Eye,"Horizontal Gaze, Vertical Gaze",EOG,Yes,4,Semantic,Yes,No,No,No,Medium,Medium,N/A,N/A,No,Yes (N=8),No,No,No,No,No,Lying,Lab,Custom Device,Research Prototype,No,No,"Accessibility, Device Control, Health","Biosignals, Health Care, In-Ear Wearables, Libs, Sleep Staging","This paper introduces LIBS, a light-weight and inexpensive wearable sensing system, that can capture electrical activities of human brain, eyes, and facial muscles with two pairs of custom-built flexible electrodes each of which is embedded on an off-the-shelf foam earplug. A supervised nonnegative matrix factorization algorithm to adaptively analyze and extract these bioelectrical signals from a single mixed in-ear channel collected by the sensor is also proposed. While LIBS can enable a wide class of low-cost selfcare, human computer interaction, and health monitoring applications, we demonstrate its medical potential by developing an autonomous whole-night sleep staging system utilizing LIBS’s outputs. We constructed a hardware prototype from off-the-shelf electronic components and used it to conduct 38 hours of sleep studies on 8 participants over a period of 30 days. Our evaluation results show that LIBS can monitor biosignals representing brain activities, eye movements, and muscle contractions with excellent fidelity such that it can be used for sleep stage classification with an average of more than 95% accuracy.",http://dx.doi.org/10.1145/2994551.2994562
28,Ando et al.,2017,"Face, Head Gestures and Pointing","Head, Jaw","Facial Gesture (Mouth), Pitch, Roll, Yaw",Pressure Sensor,Yes,11,"Coarse, Semantic",Yes,Yes,Yes (Performance Loss),No,Medium,High,High (N=12),Medium (N=6),No,No,No,No,No,Yes (N=12),No,"Music, Sitting",Lab,Earbud,Research Prototype,No,No,"Device Control, Music Player","Barometer, Earphones, Eyes-Free, Facial Movement, Hands-Free, Head Movement, Jaw Movement, Mouth Movement, Outer Ear Interface, Wearable Computing","We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four Open Mouth levels.",https://dl.acm.org/doi/10.1145/3126594.3126649
29,Dim & Ren,2017,Actuation,N/A,Vibration (Actuation),N/A,N/A,1,Coarse,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,Yes (N=43),No,No,No,No,No,"Standing, Walking","Lab, Outdoors",Custom Device,Research Prototype,N/A,N/A,"Accessibility, Military, Navigation","Navigation, Wearable Vibration","Many studies have demonstrated the benefits of wearable vibration devices for walking navigation (Tsukada and Yasumura, 2004). Despite the potential benefits, suitable body parts for wearable vibration devices have not been defined or evaluated until now. We conducted three experiments to identify suitable body parts in terms of perceivability, wearability and user body location preferences for vibration devices. We tested vibration feedback on nine body parts (the ear, neck, chest, waist, wrist, hand, finger, ankle and foot). Experiment 1 and Experiment 2 were conducted in the lab and in real-world walking settings in order to find suitable body parts. Our results indicate that the finger, wrist, ear, neck and feet had the highest perceivability and user preferences. Experiment 3 was conducted to understand the practical usability of those vibration positions in walking navigation. Our study results suggested that the feet are not suitable locations for vibration feedback in walking navigation. Based on the study results, we present design implications and guidelines for wearable vibration devices.",http://dx.doi.org/10.1016/j.ijhcs.2016.08.002
30,Kikuchi et al.,2017,Ear and Earable,Hand,Pull (Ear),Proximity Sensor,Yes,9,Semantic,No,Yes,Yes,No,Medium,Medium,Low (N=8),Medium (N=8),No,No,No,No,No,Yes (N=8),No,"Sitting, Walking",Lab,Earbud,Research Prototype,Yes,No,"Device Control, Device Input, Music Player","Earphone, Photo Reflective Sensor, Skin Deformation","In this paper, we propose EarTouch, a new sensing technology for ear-based input for controlling applications by slightly pulling the ear and detecting the deformation by an enhanced earphone device. It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an “eyes-free” interface. As for the operation of EarTouch, the shape deformation of the ear is measured by optical sensors. Deformation of the skin caused by touching the ear with the fingers is recognized by attaching optical sensors to the earphone and measuring the distance from the earphone to the skin inside the ear. EarTouch supports recognition of multiple gestures by applying a support vector machine (SVM). EarTouch was validated through a set of user studies.",https://dl.acm.org/doi/10.1145/3098279.3098538
31,Maag et al.,2017,Mouth,Tongue,Press,Pressure Sensor,Yes,3,Semantic,Yes,Yes,No,No,High,High,Medium (N=1),N/A,No,No,No,No,No,Yes (N=1),No,"Head Movement, Music, Sitting",Lab,Earbud,Research Prototype,No,No,"Device Input, Music Player","Human Computer Interaction, Pressure Sensors, Ubiquitous Computing","Sensing tongue movements enables various applications in hands-free interaction and alternative communication. We propose BARTON, a BARometer based low-power and robust TONgue movement sensing system. Using a low sampling rate of below 50 Hz, and only extracting simple temporal features from in-ear pressure signals, we demonstrate that it is plausible to distinguish important tongue gestures (left, right, forward) at low power consumption. We prototype BARTON with commodity earpieces integrated with COTS barometers for in-ear pressure sensing and an ARM micro-controller for signal processing. Evaluations show that BARTON yields 94% classification accuracy and 8.4 mW power consumption, which achieves comparable accuracy, but consumes 44 times lower energy than the stateof-the-art microphone-based solutions. BARTON is also robust to head movements and operates with music played directly from earphones.",https://ieeexplore.ieee.org/document/8368342
32,Matthies et al.,2017,"Face, Head Gestures and Pointing","Facial Expression, Head","Facial Gesture (Eye), Facial Gesture (Mouth), Pitch, Smile, Yaw","Capacitive Sensor, EMG, Electrical Field Sensing",Yes,15,Semantic,Yes,Visual Attention,No,No,Low,Low,Low (N=1),Medium (N=3),No,No,No,No,No,Yes (N=1),No,Sitting,Lab,"Custom Device, Earbud",Research Prototype,No,No,Device Input,"Body Potential Sensing, Electric Field Sensing, Eyes-Free, Facial Expression Control, Hands-Fee, Wearable Computing","EarFieldSensing (EarFS) is a novel input method for mobile and wearable computing using facial expressions. Facial muscle movements induce both electric field changes and physical deformations, which are detectable with electrodes placed inside the ear canal. The chosen ear-plug form factor is rather unobtrusive and allows for facial gesture recognition while utilizing the close proximity to the face. We collected 25 facial-related gestures and used them to compare the performance levels of several electric sensing technologies (EMG, CS, EFS, EarFS) with varying electrode setups. Our developed wearable fine-tuned electric field sensing employs differential amplification to effectively cancel out environmental noise while still being sensitive towards small facial-movement-related electric field changes and artifacts from ear canal deformations. By comparing a mobile with a stationary scenario, we found that EarFS continues to perform better in a mobile scenario. Quantitative results show EarFS to be capable of detecting a set of 5 facial gestures with a precision of 90% while sitting and 85.2% while walking. We provide detailed instructions to enable replication of our low-cost sensing device. Applying it to different positions of our body will also allow to sense a variety of other gestures and activities.",https://dl.acm.org/doi/10.1145/3025453.3025692
33,Wang et al.,2017,Eye-Tracking,Eye,"Blink, Vertical Gaze","EMG, EOG",Yes,3,Semantic,Yes,No,No,No,Medium,Medium,High (N=20),N/A,No,No,No,No,No,Yes (N=20),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"Accessibility, Device Control",N/A,"This paper proposed a method using eye saccade and facial expression physiological signals to interpret human intentions as a mean to operate wearable robots. Our approach used only two electrodes placed on top of the left and right ears to identify high fidelity physiological signals. With the developed machine learning algorithms, we can achieve over 97% accuracy of the classified various human eye-facial gestures, which allows us to build up intuitive Human-Machine Interaction (HMI) strategies to let disable people operate exoskeletons or wheelchairs easily and naturally. This method also enables the design of earbud-like wearable device, which can be worn comfortably for long hours to provide ubiquitous controllability to operate external smart devices. The ultimate goal of this research is to seamlessly merge human and machine all together by just using a simple wearable device.",https://ieeexplore.ieee.org/abstract/document/8383845
34,Ahn et al.,2018,"Brain, Eye-Tracking",Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=6),Sitting,Lab,Custom Device,Research Prototype,No,No,"Bci-Application, Device Control",N/A,"Steady-state visual evoked potential (SSVEP) has been widely used in electroencephalogram (EEG)-based brain–computer interfaces (BCIs) due to its high information transfer rate (ITR) and short training time. Current methods usually measure SSVEP from electrodes on the scalp, which is an uncomfortable and time-consuming method. Furthermore, most research relies on expensive and non-portable EEG devices. To utilise BCIs in daily life, however, these are critical issues to address. Hence, a wearable EEG device for in-ear SSVEP detection is proposed. The system is 40 × 21 × 10.5 mm3 and weighs 14.2 g, thus being light weight and wearable. Moreover, the system has a noise level of 0.11 μVrms, which is comparable with commercial EEG systems. Six subjects participated in an offline BCI experiment that consisted of six visual targets using the developed in-the-ear EEG system. The results showed a highest ITR of 11.03 ± 4.18 bits/min with an accuracy of 79.9 ± 13.1%, and the experiments demonstrated that the system can be utilised for unobtrusive monitoring of SSVEP in BCI applications.",https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/el.2017.3970
35,Carioli et al.,2018,Mouth,Jaw,General Jaw Movement,Piezoelectric Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=4),Sitting,Lab,Earbud,Research Prototype,No,No,Motion Tracking,"Bending of Curved Surfaces, Earcanal Deformation, Piezoelectric Sensor","The earcanal shape is unique for each human being and temporarily changes when the jaw moves due to eating, chewing, or speaking. The earcanal deformation can be studied by the geometrical analysis of a distorted earpiece custom-fitted inside the earcanal, but the distortion of the earpiece is complex in nature and complicated to analyze. An earcanal bending sensor consisting of a thin piezoelectric strip attached to a customfitted earpiece is presented in this paper. An analytical approach based on computing the geometrical parameters of distorted and undistorted earpieces is developed to: 1) estimate the average bending moment and the resulting stress applied to the customfitted earpiece while opening the jaw and 2) calculate the sensitivity of the piezoelectric earcanal bending sensor. The theoretical model is experimentally validated. The proposed approach can be applied to measure the bending of any curved body in general, and custom-fitted earpieces in particular. It, therefore, enables the designing of versatile in-ear sensors capable of tracking jaw activity and evaluating the energy capacity of earcanal deformation for in-ear energy harvesting purposes.",https://ieeexplore.ieee.org/document/8194832
36,Favre-Felix et al.,2018,Eye-Tracking,Eye,Vertical Gaze,EOG,Yes,1,Coarse,Yes,No,No,No,Medium,Medium,Low (N=7),N/A,No,No,No,No,No,Yes (N=7),No,Sitting,Lab,Custom Device,Commercial,Yes,No,Accessibility,"Electrooculography, Eye Tracking, Hearing Device, Sound Perception","The behavior of a person during a conversation typically involves both auditory and visual attention. Visual attention implies that the person directs his or her eye gaze toward the sound target of interest, and hence, detection of the gaze may provide a steering signal for future hearing aids. The steering could utilize a beamformer or the selection of a specific audio stream from a set of remote microphones. Previous studies have shown that eye gaze can be measured through electrooculography (EOG). To explore the precision and real-time feasibility of the methodology, seven hearing-impaired persons were tested, seated with their head fixed in front of three targets positioned at 30 , 0 , and þ30 azimuth. Each target presented speech from the Danish DAT material, which was available for direct input to the hearing aid using head-related transfer functions. Speech intelligibility was measured in three conditions: a reference condition without any steering, a condition where eye gaze was estimated from EOG measures to select the desired audio stream, and an ideal condition with steering based on an eye-tracking camera. The ‘‘EOG-steering’’ improved the sentence correct score compared with the ‘‘no-steering’’ condition, although the performance was still significantly lower than the ideal condition with the eye-tracking camera. In conclusion, eye-gaze steering increases speech intelligibility, although real-time EOG-steering still requires improvements of the signal processing before it is feasible for implementation in a hearing aid.",https://journals.sagepub.com/doi/10.1177/2331216518814388
37,Huang et al.,2018,Actuation,N/A,Deformation (Actuation),N/A,N/A,4,Semantic,Yes,Yes,Yes,N/A,Medium,Medium,N/A,N/A,Yes (N=10),Yes (N=20),No,No,Yes (N=20),No,No,Sitting,Café,Custom Device,Research Prototype,N/A,N/A,"Accessibility, Communication, Social Interaction","Actuating Human Body, Auricle, Body Language, Emotion, Wearable Earpiece","In this paper, we propose using the auricle – the visible part of the ear – as a means of expressive output to extend body language to convey emotional states. With an initial exploratory study, we provide an initial set of dynamic and static auricular postures. Using these results, we examined the relationship between emotions and auricular postures, noting that dynamic postures involving stretching the top helix in fast (e.g., 2Hz) and slow speeds (1Hz) conveyed intense and mild pleasantness while static postures involving bending the side or top helix towards the center of the ear were associated with intense and mild unpleasantness. Based on the results, we developed a prototype (called Orrechio) with miniature motors, custommade robotic arms and other electronic components. A preliminary user evaluation showed that participants feel more comfortable using expressive auricular postures with people they are familiar with, and that it is a welcome addition to the vocabulary of human body language.",https://doi.org/10.1145/3242587.3242629
38,Lee et al.,2018,Ear and Earable,Hand,"Drag (Ear), Dwell (Ear), Joystick (Ear), Lift (Ear), Tap (Ear), Toggle (Ear)",Camera,Yes,6,"Coarse, Semantic",No,Yes,No,No,Medium,Medium,N/A,N/A,Yes (N=20),No,No,Yes (N=32),Yes (N=32),No,No,Sitting,"Café, Lab",Custom Device,Research Prototype,Yes,No,"AR/VR, Device Control, Device Input","Augmented Reality, Hand-To-Face Input, Head Mounted Display, Social Acceptability, User Elicitation","Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable handto-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",https://dl.acm.org/doi/10.1145/3242587.3242642
39,Min et al. ,2018,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,No,No,Medium,Medium,Low (N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Activity Recognition, Communication, Data Annotation, Emotion Recognition, Health","Audio Sensing, Earable, Earbud, Kinetic Sensing","In this paper, we explore audio and kinetic sensing on earable devices with the commercial on-the-shelf form factor. For the study, we prototyped earbud devices with a 6-axis inertial measurement unit and a microphone. We systematically investigate the differential characteristics of the audio and inertial signals to assess their feasibility in human activity recognition. Our results demonstrate that earable devices have a superior signal-to-noise ratio under the influence of motion artefacts and are less susceptible to acoustic environment noise. We then present a set of activity primitives and corresponding signal processing pipelines to showcase the capabilities of earbud devices in converting accelerometer, gyroscope, and audio signals into the targeted human activities with a mean accuracy reaching up to 88% in varying environmental conditions.",https://dl.acm.org/doi/10.1145/3211960.3211970
40,Nguyen et al.,2018,Mouth,"Teeth, Tongue","Grit, Press","Capacitive Sensor, EEG, EMG",Yes,10,Semantic,Yes,Yes,No,No,High,High,Medium (N=15),N/A,No,Yes (N=9),No,No,No,Yes (N=15),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"AR/VR, Accessibility, Authentification, Device Control, Driving, Privacy","Brain-Muscles Sensing, Human Computer Interaction, Skin Deformation Sensing, Tongue-Teeth Typing, Wearable Devices","This paper explores a new wearable system, called TYTH, that enables a novel form of human computer interaction based on the relative location and interaction between the user’s tongue and teeth. TYTH allows its user to interact with a computing system by tapping on their teeth. This form of interaction is analogous to using a finger to type on a keypad except that the tongue substitutes for the finger and the teeth for the keyboard. We study the neurological and anatomical structures of the tongue to design TYTH so that the obtrusiveness and social awkwardness caused by the wearable is minimized while maximizing its accuracy and sensing sensitivity. From behind the user’s ears, TYTH senses the brain signals and muscle signals that control tongue movement sent from the brain and captures the miniature skin surface deformation caused by tongue movement. We model the relationship between tongue movement and the signals recorded, from which a tongue localization technique and tongue-teeth tapping detection technique are derived. Through a prototyping implementation and an evaluation with 15 subjects, we show that TYTH can be used as a form of hands-free human computer interaction with 88.61% detection rate and promising adoption rate by users.",https://doi.org/10.1145/3210240.3210322
41,Taniguchi et al.,2018,Mouth,Tongue,Press,Proximity Sensor,Yes,1,Semantic,Yes,Yes,Yes,No,High,High,High (N=5),Medium (N=5),No,No,No,No,No,Yes (N=5),No,"Chewing Gum, Sitting, Walking",Lab,Earbud,Research Prototype,Yes,No,Music Player,"Ear Canal, Hand-Free Controller, Non-Invasive, Optical Measurement, Tongue","In this study, an earphone-type interface named “earable TEMPO” was developed for hands-free operation, wherein the user can control the device by simply pushing the tongue against the roof of the mouth for about one second. This interface can be used to start and stop the music from a portable audio player. The earable TEMPO uses an earphone-type sensor equipped with a light emitting diode (LED) and a phototransistor to optically measure shape variations that occur in the external auditory meatus when the tongue is pressed against the roof of the mouth. To evaluate the operation of the earable TEMPO, experiments were performed on five subjects (men and women aged 22–58) while resting, chewing gum (representing mastication), and walking. The average accuracy was 100% while resting and chewing and 99% while walking. The precision was 100% under all conditions. The average recall value of the five subjects was 92%, 90%, and 48% while resting, masticating, and walking, respectively. All subjects were reliably able to perform the action of pressing the tongue against the roof of the mouth. The measured shape variations in the ear canal were highly reproducible, indicating that this method is suitable for various applications such as controlling a portable audio player.",https://www.mdpi.com/1424-8220/18/3/733
42,Amesaka et al.,2019,"Face, Head Gestures and Pointing, Mouth","Facial Expression, Head","Facial Gesture (Mouth), Roll","Microphone, Speaker",Yes,5,Semantic,Yes,Yes,No,No,Medium,High,Medium (N=11),N/A,No,No,No,No,No,Yes (N=11),No,Sitting,Lab,Earbud,Research Prototype,No,No,"Device Control, Device Input, Music Player","Ear Canal Transfer Function, Facial Expression Recognition, Hearables, Ultrasound","In this study, we propose a new input method for mobile and wearable computing using facial expressions. Facial muscle movements induce physical deformation in the ear canal. Our system utilizes such characteristics and estimates facial expressions using the ear canal transfer function (ECTF). Herein, a user puts on earphones with an equipped microphone that can record an internal sound of the ear canal. The system transmits ultrasonic band-limited swept sine signals and acquires the ECTF by analyzing the response. An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes. We investigated the performance of our proposed method for 21 facial expressions with 11 participants. Moreover, we proposed a signal correction method that reduces positional errors caused by attaching/detaching the device. The evaluation results confirmed that the f-score was 40.2% for the uncorrected signal method and 62.5% for the corrected signal method. We also investigated the practical performance of six facial expressions and confirmed that the f-score was 74.4% for the uncorrected signal method and 90.0% for the corrected signal method. We found the ECTF can be used for recognizing facial expressions with high accuracy equivalent to other related work.",https://dl.acm.org/doi/10.1145/3341163.3347747
43,Ferlini et al.,2019,Head Gestures and Pointing,Head,Yaw,"Accelerometer, Gyroscope, Magnetometer",Yes,1,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=10),"Eating, Speaking, Standing",Lab,Earbud,Commercial,Yes,No,"Accessibility, Motion Tracking","Earables, Head Motion Tracking, Visual Attention","Head tracking is a fundamental component in visual attention detection, which, in turn, can improve the state of the art of hearing aid devices. A multitude of wearable devices for the ear (so called earables) exist. Current devices lack a magnetometer which, as we will show, represents a big challenge when one tries to use them for accurate head tracking. In this work we evaluate the performance of eSense, a representative earable device, to track head rotations. By leveraging two different streams (one per earbud) of inertial data (from the accelerometer and the gyroscope), we achieve an accuracy up to a few degrees. We further investigate the interference generated by a magnetometer in an earable to understand the barriers to its use in these types of devices.",https://dl.acm.org/doi/10.1145/3345615.3361131
44,Hoelzemann et al.,2019,Ear and Earable,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=7),No,No,No,No,No,"Sitting, Sports, Walking, Walking Stairs","Lab, Sports Site",Earbud,Commercial,Yes,No,Data Annotation,N/A,"Wearable activity recognition research needs benchmark data, which rely heavily on synchronizing and annotating the inertial sensor data, in order to validate the activity classifiers. Such validation studies become challenging when recording outside the lab, over longer stretches of time. This paper presents a method that uses an inconspicuous, earworn device that allows the wearer to annotate his or her activities as the recording takes place. Since the ear-worn device has integrated inertial sensors, we use cross-correlation over all wearable inertial signals to propagate the annotations over all sensor streams. In a feasibility study with 7 participants performing 6 different physical activities, we show that our algorithm is able to synchronize signals between sensors worn on the body using cross-correlation, typically within a second. A comfort rating scale study has shown that attachment is critical. Button presses can thus define markers in synchronized activity data, resulting in a fast, comfortable, and reliable annotation method.",https://dl.acm.org/doi/10.1145/3345615.3361136
45,Lee et al.,2019,Actuation,N/A,Vibration (Actuation),N/A,N/A,40,"Coarse, Semantic",Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,Yes (N=14),No,No,No,No,Walking,Lab,Custom Device,Research Prototype,N/A,N/A,"AR/VR, Accessibility, Gaming, Music Player, Navigation","Digital Jewelry, Ear, Haptics, Smart Earring, Spatiotemporal Patterns, Wearable","The symmetric configuration and the sensitivity of ears in addition to the long tradition of earrings as adornment open up the possibility for smart ear-worn devices. Taking advantage of these attributes, past research mostly focused on creating novel unobtrusive sensing input devices and auditory displays placed on the ear. Meanwhile, the tactile sensitivity of the ear has long been overshadowed by its auditory capacity, presenting the opportunity to investigate how ears can be exploited for unobtrusive tactile information transfer. With three studies and a total of 38 participants, we suggest the design of ActivEarring, a ear-worn device capable of imparting information by stimulating six different locations on both ears. We evaluated the performance of ActivEarring in a semi-realistic mobile condition and its practical use for information transfer with spatiotemporal patterns. Finally, we demonstrate that ActivEarring can be incorporated in common jewelry design, and present three applications that illustrate promising usage scenarios.",https://ieeexplore.ieee.org/document/8750808
46,Lee et al.,2019,Face,Facial Expression,"Facial Gesture (Other), Smile","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,Yes,No,Low,Medium,Medium (N=9),Medium (N=9),No,No,No,No,No,Yes (N=9),No,"Sitting, Standing, Walking",Lab,Earbud,Commercial,No,No,"Music Player, Phone Calls","Earable, Facs, Frown Recognition, Kinetic Modeling, Smile Recognition","In this paper, we introduce inertial signals obtained from an earable placed in the ear canal as a new compelling sensing modality for recognising two key facial expressions: smile and frown. Borrowing principles from Facial Action Coding Systems, we first demonstrate that an inertial measurement unit of an earable can capture facial muscle deformation activated by a set of temporal microexpressions. Building on these observations, we then present three different learning schemes - shallow models with statistical features, hidden Markov model, and deep neural networks to automatically recognise smile and frown expressions from inertial signals. The experimental results show that in controlled non-conversational settings, we can identify smile and frown with high accuracy (F1 score: 0.85).",https://doi.org/10.1145/3311823.3311869
47,Nasser et al.,2019,Actuation,N/A,Thermal (Actuaction),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,N/A,"AR/VR, Accessibility","Earable Display, Earable Interfaces, Thermal Earable, Thermal Wearable","Earables/hearables (ear-worn devices) are on the rise as nextgeneration wearables which are capable of streaming audio, modifying soundscapes, and collecting body vitals. This paper introduces a novel concept of an earable device that can provide thermal (hot and cold) haptic cues at multiple areas around the ear. The concept with discrete thermal encoding can help the hearing and visually impaired individuals to obtain directional and other notifications from mobile and IoT devices. Embedding thermal haptic feedback into an earable form factor have a better edge in terms of privacy of notifications when compared to audio, visual, and vibrohaptic modalities. We conducted a pilot study using a proof-of-concept prototype and confirmed the feasibility of the concept.",https://doi.org/10.1145/3308561.3354636
48,Odoemelem et al.,2019,Head Gestures and Pointing,Head,"Pitch, Roll","Accelerometer, Gyroscope",Yes,2,Fine,Yes,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Commercial,Yes,No,"Accessibility, Device Control, Motion Tracking",N/A,"Head motion-based interfaces for controlling robot arms in real time have been presented in both medical-oriented research as well as human-robot interaction. We present an especially minimal and low-cost solution that uses the eSense [1] ear-worn prototype as a small head-worn controller, enabling direct control of an inexpensive robot arm in the environment. We report on the hardware and software setup, as well as the experiment design and early results.",https://dl.acm.org/doi/10.1145/3345615.3361138
49,Shirota et al.,2019,Actuation,N/A,Deformation (Actuation),N/A,N/A,1,Coarse,Yes,Yes,No,N/A,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,N/A,N/A,"AR/VR, Navigation","Actuating Human Body, Ear, Sound Illusion,Sound Direction, Wearable Pinna Device","This study demonstrates that by opening and closing the human pinna, we can change the direction of sound perceived by humans. Each ear was independently transformed into a 100% open, 50% open, and 100% closed state, and all 9 combinations of these ear transformations were tested to evaluate the perceived direction of the sound output from 7 speakers placed 180 degrees around the subject. We demonstrate that by deforming the pinna, we could change the perception of the direction of sound, or make it illusory. We also found that except for 1 out of 7 speakers (or directions of sound), closing 100% of the ear on the side of the speaker where the sound is coming from and 50% of the ear on the other side of the speaker tends produce the most alteration to the perceived direction of sound.",https://doi.org/10.1145/3341163.3347725
50,Vega Gálvez et al.,2019,Mouth,Teeth,Click,"Accelerometer, Gyroscope",Yes,4,Semantic,Yes,Yes,Yes,No,High,High,Medium,N/A,No,No,No,No,No,Yes,No,Sitting,Lab,Custom Device,Research Prototype,No,No,Device Control,"Discreet Interfaces, Hands-Free Interaction, Microgestures, Teeth Gestures","Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teethclicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.",https://doi.org/10.1145/3290607.3312925
51,Yan et al.,2019,Hand Gestures and Location,Hand,Cover (Face),Microphone,Yes,1,Semantic,No,Yes,No,No,Medium,Medium,High (N=12),Medium (N=12),No,Yes (N=12),Yes (N=12),Yes (N=17),Yes (N=17),Yes (N=12),No,Sitting,"Lab, Office",Earbud,Commercial,Yes,No,"AR/VR, Device Input, Privacy","Hand Gesture, Voice Input","We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is per formed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the differ ence of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.",https://dl.acm.org/doi/10.1145/3332165.3347950
52,Cao et al.,2020,Ear and Earable,Wearable State,Wear (Earable),"Microphone, Speaker",Partly,1,Fine,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,Yes (N=5),"Distance, Obstacles, Walking",Lab,Earbud,Commercial,Yes,No,"AR/VR, Device Control, Device Input, Motion Tracking","Earable Sensing, Earphone-Based Acoustic Sensing, Motion Tracking","Acoustic motion tracking is an exciting new research area with promising progress in the last few years. Due to the inherent low propagation speed in the air, acoustic signals have the unique advantage of fine sensing granularity compared to RF signals. Speakers and microphones nowadays are pervasively available in devices surrounding us, such as smartphones and voice-controlled smart speakers. Though promising, one fundamental issue hindering the adoption of acoustic-based motion tracking is that the positions of microphones and speakers inside a device are fixed, which greatly limits the flexibility of acoustic motion tracking. In this work, we propose a new modality of acoustic motion tracking using earphones. Earphone-based tracking mitigates the constraints associated with traditional smartphone-based tracking. With novel designs and comprehensive experiments, we show earphone-based motion tracking can achieve a great flexibility and a high accuracy at the same time. We believe this is an important step towards “earable” sensing.",https://dl.acm.org/doi/10.1145/3384419.3430730
53,Chen et al.,2020,"Ear and Earable, Hand Gestures and Location",N/A,N/A,N/A,N/A,0,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,Yes (N=27),Yes (N=27),No,No,Yes (N=27),No,No,Sitting,Lab,N/A,N/A,N/A,N/A,"Device Control, Device Input, Music Player, Phone Calls","Ear-Based Input, Gestures, Guessability, Think-Aloud, User-Defined","The human ear is highly sensitive and accessible, making it especially suitable for being used as an interface for interacting with smart earpieces or augmented glasses. However, previous works on ear-based input mainly address gesture sensing technology and researcher-designed gestures. This paper aims to bring more understandings of gesture design. Thus, for a user elicitation study, we recruited 28 participants, each of whom designed gestures for 31 smart device-related tasks. This resulted in a total of 868 gestures generated. Upon the basis of these gestures, we compiled a taxonomy and concluded the considerations underlying the participants’ designs that also offer insights into their design rationales and preferences. Thereafter, based on these study results, we propose a set of user-defined gestures and share interesting findings. We hope this work can shed some light on not only sensing technologies of ear-based input, but also the interface design of future wearable interfaces.",https://dl.acm.org/doi/10.1145/3427314
54,Chen et al.,2020,Face,Facial Expression,"Facial Gesture (Eye), Facial Gesture (Mouth), Facial Gesture (Other), Smile",Camera,Yes,30,Semantic,Yes,Visual Attention,No,Yes,Low,Low,Medium (N=9),N/A,No,No,No,No,No,Yes (N=9),Yes (N=9),"Glasses, Mask, Sitting, Wearing Styles",Lab,"Earbud, Headphone",Research Prototype,No,No,"Communication, Customer Analytics, Emotion Recognition, Feedback System, Silent Speech","Computer Vision, Deep Learning, Ear Sensing, Emoji Recognition, Facial Expression Reconstruction and Tracking, Silent Speech, Wearable Computing","C-Face (Contour-Face) is an ear-mounted wearable sensing technology that uses two miniature cameras to continuously reconstruct facial expressions by deep learning contours of the face. When facial muscles move, the contours of the face change from the point of view of the ear-mounted cameras. These subtle changes are fed into a deep learning model which continuously outputs 42 facial feature points representing the shapes and positions of the mouth, eyes and eyebrows. To evaluate C-Face, we embedded our technology into headphones and earphones. We conducted a user study with nine participants. In this study, we compared the output of our system to the feature points outputted by a state of the art computer vision library (Dlib1) from a font facing camera. We found that the mean error of all 42 feature points was 0.77 mm for earphones and 0.74 mm for headphones. The mean error for 20 major feature points capturing the most active areas of the face was 1.43 mm for earphones and 1.39 mm for headphones. The ability to continuously reconstruct facial expressions introduces new opportunities in a variety of applications. As a demonstration, we implemented and evaluated C-Face for two applications: facial expression detection (outputting emojis) and silent speech recognition. We further discuss the opportunities and challenges of deploying C-Face in real-world applications.",http://dx.doi.org/10.1145/3379337.3415879
55,Kaveh et al.,2020,"Brain, Eye-Tracking",Eye,Blink,EEG,Yes,1,Semantic,Yes,No,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=3),Sitting,Lab,Custom Device,Research Prototype,No,No,"Bci-Application, Device Input","BCI, Dry Electrodes, EEG, Ear EEG, User-Generic, Wireless Neural Recording","In the past few years it has been demonstrated that electroencephalography (EEG) can be recorded from inside the ear (in-ear EEG). To open the door to low-profile earpieces as wearable brain-computer interfaces (BCIs), this work presents a practical in-ear EEG device based on multiple dry electrodes, a user-generic design, and a lightweight wireless interface for streaming data and device programming. The earpiece is designed for improved ear canal contact across a wide population of users and is fabricated in a low-cost and scalable manufacturing process based on standard techniques such as vacuum forming, plasma-treatment, and spray coating. A 2.5 × 2.5 cm2 wireless recording module is designed to record and stream data wirelessly to a host computer. Performance was evaluated on three human subjects over three months and compared with clinical-grade wet scalp EEG recordings. Recordings of spontaneous and evoked physiological signals, eye-blinks, alpha rhythm, and the auditory steady-state response (ASSR), are presented. This is the first wireless in-ear EEG to our knowledge to incorporate a dry multielectrode, user-generic design. The user-generic ear EEG recorded a mean alpha modulation of 2.17, outperforming the state-of-the-art in dry electrode in-ear EEG systems.",https://ieeexplore.ieee.org/abstract/document/9115876
56,Prakash et al.,2020,Mouth,Teeth,"Click, Slide",Speaker,Yes,9,Semantic,Yes,Yes,No,No,High,High,Medium (N=18),Medium (N=18),No,No,No,No,No,Yes (N=18),No,"Cooking, Cycling, Device, Head Movement, Sitting, Speaking, Walking",Lab,Earbud,Commercial,Yes,No,"Accessibility, Authentification, Device Control, Device Input, Health, Privacy","Earable, Earphones, Headphones, Teeth Gestures, User Interface, Vibroacoustics","This paper finds that actions of the teeth, namely tapping and sliding, produce vibrations in the jaw and skull. These vibrations are strong enough to propagate to the edge of the face and produce vibratory signals at an earphone. By re-tasking the earphone speaker as an input transducer – a software modification in the sound card – we are able to sense teeth-related gestures across various models of ear/headphones. In fact, by analyzing the signals at the two earphones, we show the feasibility of also localizing teeth gestures, resulting in a human-to-machine interface. Challenges range from coping with weak signals, distortions due to different teeth compositions, lack of timing resolution, spectral dispersion, etc. We address these problems with a sequence of sensing techniques, resulting in the ability to detect 6 distinct gestures in real-time. Results from 18 volunteers exhibit robustness, even though our system – EarSense does not depend on per-user training. Importantly, EarSense also remains robust in the presence of concurrent user activities, like walking, nodding, cooking and cycling. Our ongoing work is focused on detecting teeth gestures even while music is being played in the earphone; once that problem is solved, we believe EarSense could be even more compelling.",https://doi.org/10.1145/3372224.3419197
57,Xu et al.,2020,"Ear and Earable, Hand Gestures and Location",Hand,"Slide (Ear), Slide (Face), Tap (Ear), Tap (Face)",Microphone,Yes,8,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=18),Medium (N=18),Yes (N=16),Yes (N=12),Yes (N=16),No,Yes (N=16),Yes (N=18),No,"Noise, Sitting",Lab,Earbud,Commercial,Yes,No,"AR/VR, Communication, Device Input, Music Player, Phone Calls","Face and Ear Interaction, Gesture Recognition, Wireless Earbuds","Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.",https://dl.acm.org/doi/10.1145/3313831.3376836
58,Yang et al.,2020,Head Gestures and Pointing,Head,"Pitch, Roll, Yaw","Accelerometer, Gyroscope",No,1,Fine,Yes,Visual Attention,No,No,Medium,High,Medium (N=7),N/A,No,No,No,No,No,Yes (N=7),Yes (N=7),"Standing, Walking",University Building,Headphone,Commercial,Yes,No,"AR/VR, Activity Recognition","Acoustics, Augmented Reality, Dead Reckoning, Head Related Transfer Function (HRTF), Indoor Localization, Inertial Measurement Unit (IMU), Motion Tracking, Sensor Fusion, Smart Earphones, Spatial Audio, Wearable Computing","This paper aims to use modern earphones as a platform for acoustic augmented reality (AAR). We intend to play 3D audio-annotations in the user’s ears as she moves and looks at AAR objects in the environment. While companies like Bose and Microsoft are beginning to release such capabilities, they are intended for outdoor environments. Our system aims to explore the challenges indoors, without requiring any infrastructure deployment. Our core idea is two-fold. (1) We jointly use the inertial sensors (IMUs) in earphones and smartphones to estimate a user’s indoor location and gazing orientation. (2) We play 3D sounds in the earphones and exploit the human’s responses to (re)calibrate errors in location and orientation. We believe this fusion of IMU and acoustics is novel, and could be an important step towards indoor AAR. Our system, Ear-AR, is tested on 7 volunteers invited to an AAR exhibition – like a museum – that we set up in our building’s lobby and lab. Across 60 different test sessions, the volunteers browsed different subsets of 24 annotated objects as they walked around. Results show that Ear-AR plays the correct audio-annotations with good accuracy. The user-feedback is encouraging and points to further areas of research and applications.",https://dl.acm.org/doi/10.1145/3372224.3419213
59,Fan et al.,2021,Ear and Earable,"Hand, Wearable State","Slide (Earable), Tap (Earable)",Pressure Sensor,Yes,2,Semantic,No,Yes,No,No,Medium,High,High (N=1),Medium (N=1),No,No,No,No,No,Yes (N=1),No,"Music, Sitting",Lab,Custom Device,Research Prototype,No,No,"Device Control, Music Player","Earable Computing, Heartrate Monitoring, Touch Gesture Control, User Identification, Voice Communication, Wearable Devices","Headphones continue to become more intelligent as new functions (e.g., touch-based gesture control) appear. These functions usually rely on auxiliary sensors (e.g., accelerometer and gyroscope) that are available in smart headphones. However, for those headphones that do not have such sensors, supporting these functions becomes a daunting task. This paper presents HeadFi, a new design paradigm for bringing intelligence to headphones. Instead of adding auxiliary sensors into headphones, HeadFi turns the pair of drivers that are readily available inside all headphones into a versatile sensor to enable new applications spanning across mobile health, user-interface, and context-awareness. HeadFi works as a plug-in peripheral connecting the headphones and the pairing device (e.g., a smartphone). The simplicity (can be as simple as only two resistors) and small form factor of this design lend itself to be embedded into the pairing device as an integrated circuit. We envision HeadFi can serve as a vital supplementary solution to existing smart headphone design by directly transforming large amounts of existing “dumb” headphones into intelligent ones. We prototype HeadFi on PCB and conduct extensive experiments with 53 volunteers using 54 pairs of non-smart headphones under the institutional review board (IRB) protocols. The results show that HeadFi can achieve 97.2%–99.5% accuracy on user identification, 96.8%–99.2% accuracy on heart rate monitoring, and 97.7%–99.3% accuracy on gesture recognition.",https://dl.acm.org/doi/10.1145/3447993.3448624
60,Ferlini et al.,2021,Head Gestures and Pointing,Head,Yaw,Magnetometer,No,1,Fine,Yes,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes,"Head Movement, Music, Standing, Walking","Lab, Outdoors",Earbud,Research Prototype,No,No,"Authentification, Navigation",N/A,"Earables (in-ear wearables) are a new frontier in wearables. Acting both as leisure devices, providing personal audio, as well as sensing platforms, earables could collect sensor data for the upper part of the body, subject to fewer vibrations and random movement variations than the lower parts of the body, due to inherent damping in the musculoskeletal system. These data may enable application domains such as augmented/virtual reality, medical rehabilitation, and health condition screening. Unfortunately, earables have inherent size, shape, and weight constraints limiting the type and position of the sensors on such platforms. For instance, lacking a magnetometer in all earables reference platforms, earables lack reference points. Thus, it becomes harder to work with absolute orientations. Embedding magnetometers in earables is challenging, as these rely heavily on radio (mostly Bluetooth) communication (RF) and contain magnets for magnetic-driven speakers and docking. We explore the feasibility of adding a built-in magnetometer in an earbud, presenting the first comprehensive study of the magnetic interference impacting the magnetometer when placed in an earable: both that caused by the speaker and by RF (music streaming and voice calls) are considered. We find that appropriate calibration of the magnetometer removes the offsets induced by the magnets, the speaker, and the variable interference due to BT. Further, we present an automatic, user-transparent adaptive calibration that obviates the need for alternative, expensive, and error-prone manual, or robotics, calibration procedures. Our evaluation shows how our calibration approach performs under different conditions, achieving convincing results with errors below 3° for the majority of the experiments.",https://ieeexplore.ieee.org/abstract/document/9439112?casa_token=YXCX9klMfrsAAAAA:6sZ70_-2Tioadkkv_2a21sktU8J4HUREXtdAgsuFsizYlG2MPe9t_9-6XwPlhLbQiati-jmUtA
61,Gashi et al.,2021,"Face, Head Gestures and Pointing","Facial Expression, Head","Facial Gesture (Mouth), Pitch, Smile, Yaw","Accelerometer, Gyroscope",Yes,4,Semantic,Yes,Yes,Yes,No,Low,Medium,Low (N=21),N/A,No,No,No,No,No,Yes (N=21),No,Sitting,Lab,Earbud,Commercial,No,No,"Activity Recognition, Feedback System, Social Interaction, Video Conference","Earable Computing, Facial Expressions Recognition, Head Gestures Detection, Hierarchical Classification, Transfer Learning","Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions. Several researchers have thus tackled the problem of automatically recognizing head gestures and facial expressions, mainly leveraging video data. In this paper, we instead consider inertial signals collected from unobtrusive, earmounted devices. We focus on typical activities performed during social interactions – head shaking, nodding, smiling, talking and yawning – and propose a hierarchical classification approach to discriminate them from each other. Further, we investigate whether the transfer of knowledge learned from publicly available datasets leads to further performance improvements. Our results show that the combined use of our hierarchical approach and transfer learning allows the classifier to discriminate head and mouth activities with an F1 score of 84.79, smile, talk and yawn with an F1 score of 45.42, and nodding and head shaking with an F1 score of 88.24, outperforming shallow classifiers by 2-9 percentage points.",https://dl.acm.org/doi/10.1145/3462244.3479921
62,Hashem et al.,2021,Head Gestures and Pointing,Head,Yaw,Bluetooth,No,1,Coarse,Yes,Visual Attention,No,No,Medium,High,High (N=1),Low (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Standing",Lab,Earbud,Commercial,Yes,No,Device Control,"Ble, Earable Computing, Earables, Head Orientation, IoT, Smart Earphones, Wearables","We present Look&Lock: a novel ubiquitous system that utilizes BLE communication between smart appliances in the environment and earables worn by the user to track head orientation and determine which device she is looking at. By harnessing this information, it is possible to lock on that device in a multi-device environment and allow only it to respond to user gestures without ambiguity. Our system leverages commercial off-the-shelf earables to provide accurate training-free head orientation tracking that is robust in different room settings. We implement a prototype using Android phones and eSense, a commercially available multi-sensory personal earable device. Our evaluation shows that Look&Lock can correctly identify devices inside a user’s field of view with accuracy up to 100% and is robust to different configurations and room settings.",https://dl.acm.org/doi/10.1145/3446382.3448653
63,Islam et al.,2021,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Gyroscope",No,2,Semantic,Yes,Yes,N/A,No,Medium,Medium,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Earbud,Commercial,No,No,"Accessibility, Activity Recognition, Health","Earables, Esense, Health Care, Wearable; Activity Recognition","Detecting head- and mouth-related human activities of elderly people are very important for nurse care centers. They need to track different types of activities of elderly people like swallowing, eating, etc., to measure the health status of elderly people. In this regard, earable devices open up interesting possibilities for monitoring personal-scale behavioral activities. Here, we introduce activity recognition based on an earable device called ‘eSense’. It has multiple sensors that can be used for human activity recognition. ‘eSense’ has a 6-axis inertial measurement unit with a microphone and Bluetooth. In this paper, we propose an activity recognition framework using eSense device. We collect accelerometer and gyroscope sensor data from eSense device to detect head- and mouth-related activities along with other normal human activities. We evaluated the classification performance of the classifier using both accelerometer and gyroscope data. For this work, we develop a smartphone application for data collection from the eSense. Several statistical features are exploited to recognize head- and mouth-related activities (e.g., head nodding, head shaking, eating, and speaking), and regular activities (e.g., stay, walk, and speaking while walking). We explored different types of machine learning approaches like Convolutional Neural Network (CNN), Random Forest (RnF), K-Nearest Neighbor (KNN), Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), etc., for classifying activities. We have achieved satisfactory results. Our results show that using both accelerometer and gyroscope sensors can improve performance. We achieve accuracy of 80.45% by LDA, 93.34% by SVM, 91.92% by RnF, 91.64% by KNN, and 93.76% by CNN while we exploit both accelerometer and gyroscope sensor data together. The results demonstrate the prospect of eSense device for detecting human activities in various healthcare monitoring system.",https://link.springer.com/chapter/10.1007/978-981-15-8944-7_11
64,Jin et al.,2021,Hand Gestures and Location,Hand,"Close (Mid-Air), Sign Language (Sentences), Sign Language (Words), Tap (Mid-Air)","Microphone, Speaker",Yes,74,Semantic,No,Yes,No,No,Low,High,Medium (N=8),High (N=2),No,No,No,No,No,Yes (N=8),No,"Distance, Orientation, Standing","Office, Outdoors, University Building",Headphone,Research Prototype,Yes,No,Accessibility,"Acoustic Sensing, Earphones, Sign Language Gesture Recognition","We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians.",https://dl.acm.org/doi/10.1145/3463519
65,Kakaraparthi et al.,2021,Hand Gestures and Location,Hand,Touch (Face),"EMG, Impedence Sensor, Thermal Camera",Yes,1,Coarse,No,Yes,Yes (Performance Loss),Yes,Medium,Medium,Medium (N=14),Medium (N=1),No,Yes (N=14),No,No,No,Yes (N=14),No,"Drinking, Eating, Running, Speaking, Walking",Office,Custom Device,Research Prototype,Yes,No,Health,"Face Touch Detection, Multimodal Deep Learning, Thermo-Physiological Sensing","Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSense integrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch. Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model.",https://doi.org/10.1145/3478129
66,Khanna et al.,2021,Mouth,Speech Apparatus,Silent Speech (Phonemes),Accelerometer,Yes,9,Semantic,Yes,Yes,No,No,High,High,Medium (N=6),Medium (N=6),No,No,No,No,No,Yes (N=6),No,"Head Movement, Music, Sitting, Speaking, Yawn",Lab,Headphone,Research Prototype,Yes,No,"Accessibility, Device Control, Health, Privacy","Accelerometer Sensing, Unvoiced Sound Recognition, Wearable Devices","This paper explores a new wearable system, called JawSense, that enables a novel form of human-computer interaction based on unvoiced jaw movement tracking. JawSense allows its user to interact with computing machine just by moving their jaw. We study the neurological and anatomical structure of the human cheek and jaw to design JawSense so that jaw movement can be reliably captured under the strong impact of noises from human artifacts. In particular, JawSense senses the muscle deformation and vibration caused by unvoiced speaking to decode the unvoiced phonemes spoken by the user. We model the relationship between jaw movements and phonemes to develop a classification algorithm to recognize nine phonemes. Through a prototyping implementation and evaluation with six subjects, we show that JawSense can be used as a form of hands-free and privacy-preserving human-computer interaction with 92% phoneme classification rate.",https://dl.acm.org/doi/10.1145/3446382.3448363
67,Laporte et al.,2021,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,Yes,No,Medium,Medium,Low (N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Data Annotation, Social Interaction","Datasets, Earable Computing, Head Gestures Recognition, Memory Recall","Verbal and non-verbal activities convey insightful information about people’s affect, empathy, and engagement during social interactions. In this paper, we investigate the usage of inertial sensors to recognize verbal (e.g., speaking), non-verbal (e.g., head nodding, shaking) and other activities (e.g., eating, no movement). We implement an end-to-end deep neural network to distinguish among these activities. We then explore the generalizability of the approach in three scenarios: (1) using new data to detect a known activity from a known user, (2) detecting a novel activity of a known user and (3) detecting the activity of an unknown user. Results show that using accelerometer and gyroscope sensors, the model achieves a balanced accuracy of 55% when tested on data from a new user, 41% on a new activity of an existing user, and 80% on new data of a known activity from an existing user. The results are between 7-47 percentage points higher than baseline classifiers.",https://dl.acm.org/doi/10.1145/3460418.3479322
68,Ma et al.,2021,Hand Gestures and Location,Hand,Tap (Face),Microphone,Yes,5,Semantic,No,Yes,No,Yes,Medium,Medium,High (N=29),High (N=4),No,No,No,No,No,Yes (N=29),No,"Music, Sitting","Lab, Outdoors",Earbud,Research Prototype,Yes,No,"Activity Recognition, Health, Motion Tracking",N/A,"Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3% step counting recall, 98.3% recognition recall for 5 activities, and 97.0% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds’ fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.",https://dl.acm.org/doi/10.1145/3458864.3467680
69,Nasser et al.,2021,Actuation,N/A,Thermal (Actuaction),N/A,N/A,26,"Coarse, Semantic",Yes,Yes,Yes (Performance Loss),N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,N/A,"Accessibility, Communication, Gaming, Health, Navigation","Earable, Earhook, Spatial Thermal Pattern, Thermotactile","Haptic feedbacks are widely adopted in mobile and wearable devices to convey various types of notifications to the users. This paper investigates the design and the evaluation of thermal haptic feedback on an earable form factor with multiple thermoelectric (i.e. Peltier) modules. We propose ThermEarhook, a wearable device that can provide hot and cold stimuli at multiple points on the auricular skin area. To investigate users’ thermal perception on the auricular area, we develop a series of ThermEarhook prototypes with 3, 4, and 5 Peltier modules. While most existing research utilized the constant level of haptic signal for different users, our pilot study with ThermEarhook shows that the auricular thermohaptic threshold varies across the feedback locations and the users. With the user-customized thermohaptic signals around the ear, our first study with 12 participants reports on the selection of the auricular configuration with four TEC modules on each side, considering the users’ identification accuracy (averagely 99.3%) and preference. We then conduct three follow-up studies and a total of 36 participants to further evaluate users’ perception of spatial thermal patterns with ThermEarhook, and finalize a set of multi-points auricular thermal patterns that can be reliably perceived by the users with the average accuracy of 85.3%. Lastly, we discuss the user-proposed potential applications of the thermal haptic feedback with ThermEarhook.",https://doi.org/10.1145/3462244.3479922
70,Peng,2021,Actuation,N/A,Deformation (Actuation),N/A,N/A,1,Semantic,Yes,Yes,Yes,N/A,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Daily Life,Custom Device,Research Prototype,N/A,N/A,Social Interaction,"Biodata Interfacing, Playful Expression","Human-Computer Integration is an extension of the HumanComputer Interaction paradigm that explores systems in which the boundary between user and computer is blurred. We build on this and explore its intersection with the felds of biodata and playful expression by presenting “Wigglears”, a wearable system that will wiggle the wearer’s ears based on skin conductance, aiming to explore playful solutions towards integrated biodata-based selfexpression. Through an autobiographical study, we demonstrate the system’s ability to fuel social dialogue, amplify positive emotions, and triggering refocus. We intend for our system to be a novel solution to expressing emotions within social interactions, and hope to ofer insights towards the social implications of biodatabased integration as a social cue, to help further the research within Human-Computer Integration.",https://doi.org/10.1145/3411763.3451846
71,Röddiger et al.,2021,Ear and Earable,Tensor Tympani,Contract,Pressure Sensor,Yes,3,Semantic,Yes,Yes,Yes,No,High,High,High (N=16),Medium (N=16),No,Yes (N=8),Yes (N=16),No,No,Yes (N=16),No,"Music, Sitting",Lab,Earbud,Research Prototype,Yes,No,"Music Player, Phone Calls","Discreet Interaction, Earables, Hearables, In-Ear Barometry, Subtle Gestures, Tensor Tympani Muscle","We explore how discreet input can be provided using the tensor tympani - a small muscle in the middle ear that some people can voluntarily contract to induce a dull rumbling sound. We investigate the prevalence and ability to control the muscle through an online questionnaire (N=192) in which 43.2% of respondents reported the ability to “ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can be used to detect voluntary tensor tympani contraction in the sealed ear canal. This data was used to train a classifer based on three simple ear rumble “gestures” which achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction, grounded in three manual, dual-task application scenarios (N=8). This highlights the applicability of EarRumble as a low-efort and discreet eyes- and hands-free interaction technique that users found “magical” and “almost telepathic”.",https://dl.acm.org/doi/10.1145/3411764.3445205
72,Sun et al.,2021,Mouth,Teeth,Click,"Gyroscope, Microphone",Yes,13,Semantic,Yes,Yes,Yes (Performance Loss),No,High,High,Medium (N=11),N/A,No,No,No,No,No,Yes (N=11),No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"AR/VR, Accessibility, Device Control, Music Player, Phone Calls","Acoustic Sensing, Earpiece, Eyes-Free Input, Hands-Free Input, Motion Sensing, Teeth Gestures","Teeth gestures become an alternative input modality for different situations and accessibility purposes. In this paper, we present TeethTap, a novel eyes-free and hands-free input technique, which can recognize up to 13 discrete teeth tapping gestures. TeethTap adopts a wearable 3D printed earpiece with an IMU sensor and a contact microphone behind both ears, which works in tandem to detect jaw movement and sound data, respectively. TeethTap uses a support vector machine to classify gestures from noise by fusing acoustic and motion data, and implements K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW) distance measurement using motion data for gesture classification. A user study with 11 participants demonstrated that TeethTap could recognize 13 gestures with a real-time classification accuracy of 90.9% in a laboratory environment. We further uncovered the accuracy differences on different teeth gestures when having sensors on single vs. both sides. Moreover, we explored the activation gesture under real-world environments, including eating, speaking, walking and jumping. Based on our findings, we further discussed potential applications and practical challenges of integrating TeethTap into future devices.",https://doi.org/10.1145/3397481.3450645
73,Verma et al.,2021,Face,Facial Expression,"Facial Gesture (Eye), Facial Gesture (Mouth), Facial Gesture (Nose), Facial Gesture (Other)","Accelerometer, Gyroscope",Yes,32,Semantic,Yes,Visual Attention,Yes (Performance Loss),Yes,Low,Low,Medium (N=12),High (N=3),No,No,No,No,No,Yes (N=12),No,"Eating, Noise, Sitting, Speaking, Walking","Controlled Room, Train",Earbud,Commercial,No,No,"AR/VR, Accessibility, Device Control, Device Input, Emotion Recognition, Feedback System, Video Conference","Earable Computing, Facial Expressions, Facs, IMU Sensing","Continuous and unobtrusive monitoring of facial expressions holds tremendous potential to enable compelling applications in a multitude of domains ranging from healthcare and education to interactive systems. Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our approach and found that ExpressEar can detect and distinguish between 32 Facial AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for any given user. We further quantify the performance across different mobile scenarios in presence of additional face-related activities. Our results demonstrate ExpressEar’s applicability in the real world and open up research opportunities to advance its practical adoption.",https://doi.org/10.1145/3478085
74,Wu et al.,2021,"Eye-Tracking, Face, Mouth","Eye, Facial Expression, Speech Apparatus","Ekman 7, Horizontal Gaze, Silent Speech (Words), Vertical Gaze","EMG, EOG",Yes,21,Semantic,Yes,No,Yes,No,Low,Low,N/A,N/A,No,Yes (N=16),Yes (N=16),No,No,No,Yes (N=24),"Head Movement, Mask, Sitting, Time",Lab,Custom Device,Research Prototype,No,No,"AR/VR, Accessibility, Device Control, Driving, Emotion Recognition, Feedback System, Silent Speech","3D Facial Reconstruction, Mobile Computing, Singleear Biosensing, Wearable Sensing","Over the last decade, facial landmark tracking and 3D reconstruction have gained considerable attention due to their numerous applications such as human-computer interactions, facial expression analysis, and emotion recognition, etc. Traditional approaches require users to be confined to a particular location and face a camera under constrained recording conditions (e.g., without occlusions and under good lighting conditions). This highly restricted setting prevents them from being deployed in many application scenarios involving human motions. In this paper, we propose the first single-earpiece lightweight biosensing system, BioFace3D, that can unobtrusively, continuously, and reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations. Our single-earpiece biosensing system takes advantage of the cross-modal transfer learning model to transfer the knowledge embodied in a high-grade visual facial landmark detection model to the low-grade biosignal domain. After training, our BioFace-3D can directly perform continuous 3D facial reconstruction from the biosignals, without any visual input. Without requiring a camera positioned in front of the user, this paradigm shift from visual sensing to biosensing would introduce new opportunities in many emerging mobile and IoT applications. Extensive experiments involving 16 participants under various settings demonstrate that BioFace-3D can accurately track 53 major facial landmarks with only 1.85 mm average error and 3.38% normalized mean error, which is comparable with most state-of-the-art camerabased solutions. The rendered 3D facial animations, which are in consistency with the real human facial movements, also validate the system’s capability in continuous 3D facial reconstruction.",https://doi.org/10.1145/3447993.3483252
75,Alkiek et al.,2022,Hand Gestures and Location,Hand,"Approach (Mid-Air), Recede (Mid-Air), Slide (Mid-Air)",Bluetooth,Yes,8,Semantic,No,Yes,Yes,No,Low,Low,High (N=1),High (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Orientation, Sitting, Standing","Living Room, Office",Earbud,Research Prototype,Yes,No,"Device Control, Music Player, Phone Calls","Earables, Gesture Recognition, Hci; Sensing","Earables have been increasingly gaining attention from consumers and manufacturers alike due to their small footprint, ease of use, and the added accessibility they bring. However, the limited interface of these devices, usually being a single button or force-sensor, inhibits their potential. Earables can provide a much richer experience by extending the ways in which users can interact with them. In this paper, we present EarGest, a novel earable-based hand gesture recognition system that does not require calibration or training, and works with commercially available BLE-enabled earphones and devices. Our proposed system is unique in its ability to leverage Bluetooth to detect hand motion near the ear to recognize gestures. By harnessing information from BLE connections between the wireless earphones and a host device, we accurately detect and classify different hand gestures performed by users, while also determining discrete levels of hand speed. EarGest operates without interfering with the regular functionality of the earphones and introduces minimal energy overhead on the host device. We implement a prototype of the system using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and settings. Results show that our system can detect and classify seven near-ear hand gestures with an accuracy up to 98.5%, as well as identify hand motion speed with 96% accuracy.",https://ieeexplore.ieee.org/abstract/document/9918622
76,Bi & Liu,2022,Head Gestures and Pointing,Head,"Pitch, Roll, Yaw",Accelerometer,Yes,12,Semantic,Yes,Yes,No,Yes,Medium,High,Medium (N=30),High,No,No,No,No,No,Yes (N=30),No,"Ground Material, Sitting, Slope, Speed, Walking",Lab,Earbud,Commercial,No,No,"Health, Safety","Earphones, Head Gesture, Internet of Healthcare Things (IoHT), Metalearning","With the popularity of personal computing devices, people often keep long-term head immobility in front of screens, resulting in the emergence of “phubbers” and “office workers.” The early warning solutions in the Internet of Healthcare Things (IoHT) have brought hope to protect users’ health and safety. However, most existing works cannot recognize the different head gestures during walking, which is also a common cause of text neck and traffic accidents. In addition, they also need a large amount of data to update the model to adapt to the new environment, which reduces the practicality of the model. To solve these problems, we propose a system, CSEar, based on builtin accelerometers of off-the-shelf wireless earphones, which can recognize 12 kinds of head gestures both in resting and walking states. First, an innovative algorithm is designed to detect head gesture signals, especially for the signals mixed with gait. Then, we propose the MetaSensing, a head gesture recognition model that can improve the recognition ability with few samples compared with the existing metalearning algorithms. Finally, the experimental results prove the effectiveness and robustness of the CSEar.",https://ieeexplore.ieee.org/document/9815053
77,Bi et al.,2022,Ear and Earable,Hand,Tap (Earable),Accelerometer,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),"Sitting, Standing","Lab, Train",Earbud,Commercial,No,No,Authentification,"Accelerometer, Earphone, Tap Gesture, User Authentication","The rapid development of the information-centric wireless sensor network (ICWSN) has solved the challenges of information transmission and processing caused by the accelerated growth of wearable devices and the wide deployment of the Internet of Things (IoT) recently. The privacy security is also a growing problem. The existing works use earphones, covert, and user-friendly wearable devices, for user authentication. However, some of the earphone-based authentication solutions need to customize special earphones, which are not universal. Other solutions use microphones and speakers of earphones for authentication, which are susceptible to changes in the auricle’s internal environment, resulting in a decline in performance. To solve this problem, a new authentication solution based on the existing commercial earphones is proposed to authenticate a user by tapping on the earphone rhythmically. This rhythmic tap behavior causes a change of the signal waveform of the built-in accelerometer in the earphone. Based on this, we design a pipeline to authenticate the user’s identity. We first design an event detection algorithm to segment the tap signal accurately. Then, we use the global features calculated based on the event detection algorithm and local features extracted from the convolutional neural network (CNN) for building an authentication model using the Naive Bayes (NB) classifier. Finally, 20 users are recruited to evaluate the experiment and the recognition accuracy reaches 98%. Moreover, we extend the experiment to prove that it has a good performance against the different attacks and is robust in different scenarios.",https://ieeexplore.ieee.org/document/9367286
78,Choi et al.,2022,Face,Facial Expression,Ekman 7,"Accelerometer, PPG",Yes,7,Semantic,Yes,Visual Attention,Yes,No,Low,Low,Medium (N=20),Medium (N=5),No,Yes (N=11),No,No,No,Yes (N=20),No,"Hand Motion, Head Movement, Mask, Sitting, Standing, Walking, Wearing Styles",Lab,Custom Device,Research Prototype,No,No,"Accessibility, Customer Analytics, Device Control","Blood Vessel Deformation, Ear Canal, Facial Expression, PPG, Photoplethysmogram","Recognition of facial expressions has been widely explored to represent people’s emotional states. Existing facial expression recognition systems primarily rely on external cameras which make it less accessible and efficient in many real-life scenarios to monitor an individual’s facial expression in a convenient and unobtrusive manner. To this end, we propose PPGface, a ubiquitous, easy-to-use, user-friendly facial expression recognition platform that leverages earable devices with built-in PPG sensor. PPGface understands the facial expressions through the dynamic PPG patterns resulting from facial muscle movements. With the aid of the accelerometer sensor, PPGface can detect and recognize the user’s seven universal facial expressions and relevant body posture unobtrusively. We conducted an user study (N=20) using multimodal ResNet to evaluate the performance of PPGface, and showed that PPGface can detect different facial expressions with 93.5 accuracy and 0.93 f1-score. In addition, to explore the robustness and usability of our proposed platform, we conducted several comprehensive experiments under real-world settings. Overall results of this work validate a great potential to be employed in future commodity earable devices.",https://doi.org/10.1145/3534597
79,Futami et al,2022,Face,Facial Expression,"Facial Gesture (Eye), Facial Gesture (Mouth)",Proximity Sensor,Yes,8,Semantic,Yes,Visual Attention,Yes,No,Low,Low,High (N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"Device Input, Music Player","Ear Accessories, Earable, Facial Gesture Recognition, Hands-Free Input Interface, Infrared Distance Sensor, Photo Reflector, Skin Movement","Simple hands-free input methods using ear accessories have been proposed to broaden the range of scenarios in which information devices can be operated without hands. Although many previous studies use canal-type earphones, few studies focused on the following two points: (1) A method applicable to ear accessories other than canal-type earphones. (2) A method enabling various ear accessories with different styles to have the same hands-free input function. To realize these two points, this study proposes a method to recognize the user’s facial gesture using an infrared distance sensor attached to the ear accessory. The proposed method detects skin movement around the ear and face, which differs for each facial expression gesture. We created a prototype system for three ear accessories for the root of the ear, earlobe, and tragus. The evaluation results for nine gestures and 10 subjects showed that the F-value of each device was 0.95 or more, and the F-value of the pattern combining multiple devices was 0.99 or more, which showed the feasibility of the proposed method. Although many ear accessories could not interact with information devices, our findings enable various ear accessories with different styles to have eye-free and hands-free input ability based on facial gestures.",https://doi.org/10.3390/electronics11091480
80,Jin et al.,2022,"Head Gestures and Pointing, Mouth","Head, Speech Apparatus","Pitch, Silent Speech (Sentences), Silent Speech (Words), Yaw","Accelerometer, Microphone, Speaker",Yes,59,Semantic,Yes,Yes,No,Yes,High,High,N/A,N/A,No,No,No,No,No,No,Yes (N=12),"Head Movement, Mask, Noise, Sitting, Standing, Walking, Wearing Styles",Lab,Earbud,Research Prototype,Yes,No,"Device Control, Privacy, Silent Speech","Acoustic Sensing, Ear Canal Deformation, Earphone, Silent Speech","Intelligent speech interfaces have been developing vastly to support the growing demands for convenient control and interaction with wearable/earable and portable devices. To avoid privacy leakage during speech interactions and strengthen the resistance to ambient noise, silent speech interfaces have been widely explored to enable people’s interaction with mobile/wearable devices without audible sounds. However, most existing silent speech solutions require either restricted background illuminations or hand involvement to hold device or perform gestures. In this study, we propose a novel earphonebased, hand-free silent speech interaction approach, named EarCommand. Our technique discovers the relationship between the deformation of the ear canal and the movements of the articulator and takes advantage of this link to recognize different silent speech commands. Our system can achieve a WER (word error rate) of 10.02% for word-level recognition and 12.33% for sentence-level recognition, when tested in human subjects with 32 word-level commands and 25 sentence-level commands, which indicates the effectiveness of inferring silent speech commands. Moreover, EarCommand shows high reliability and robustness in a variety of configuration settings and environmental conditions. It is anticipated that EarCommand can serve as an efficient, intelligent speech interface for hand-free operation, which could significantly improve the quality and convenience of interactions.",https://doi.org/10.1145/3534613
81,Li et al.,2022,Face,Facial Expression,"Facial Gesture (Eye), Facial Gesture (Mouth), Smile","Microphone, Speaker",Yes,9,Semantic,Yes,Visual Attention,No,Yes,Low,Medium,N/A,N/A,No,Yes (N=16),No,No,No,No,Yes (N=12),"Noise, Sitting, Speaking, Walking, Wearing Styles","Lab, Outdoors",Custom Device,Research Prototype,No,No,Data Annotation,"Acoustic Sensing, Deep Learning, Facial Expression Reconstruction, Low-Power","This paper presents EarIO, an AI-powered acoustic sensing technology that allows an earable (e.g., earphone) to continuously track facial expressions using two pairs of microphone and speaker (one on each side), which are widely available in commodity earphones. It emits acoustic signals from a speaker on an earable towards the face. Depending on facial expressions, the muscles, tissues, and skin around the ear would deform differently, resulting in unique echo profiles in the reflected signals captured by an on-device microphone. These received acoustic signals are processed and learned by a customized deep learning pipeline to continuously infer the full facial expressions represented by 52 parameters captured using a TruthDepth camera. Compared to similar technologies, it has significantly lower power consumption, as it can sample at 86 Hz with a power signature of 154 mW. A user study with 16 participants under three different scenarios, showed that EarIO can reliably estimate the detailed facial movements when the participants were sitting, walking or after remounting the device. Based on the encouraging results, we further discuss the potential opportunities and challenges on applying EarIO on future ear-mounted wearables.",https://doi.org/10.1145/3534621
82,Song et al.,2022,Face,Facial Expression,Ekman 7,"Microphone, Speaker",Yes,6,Semantic,Yes,Visual Attention,No,No,Low,Low,Low (N=5),High (N=5),No,No,No,No,No,Yes (N=5),Yes (N=5),"Device, Movie, Music, Sitting, Skin Condition, Speaking, Walking","Lab, Outdoors","Earbud, Headphone",Commercial,No,No,"AR/VR, Customer Analytics, Device Control","Acoustic Sensing, Face Skin Deformation, Headphone, Human Facial Expressions, Knowledge Distillation","Facial expressions are important indicators of user needs that can be used in many interactive computing applications to adapt the system behaviors and settings. Current computing approaches to recognizing human facial expressions, however, either rely on continuous camera recordings that are energy consuming, or require custom sensing hardware that are expensive and difficult to use on commodity systems. In this paper, we present FaceListener, a new sensing system that recognizes human facial expressions by only using commodity headphones. The basic idea of FaceListener is to transform the commodity headphone into an acoustic sensing device, which captures the face skin deformations caused by facial muscle movements with different facial expressions. To ensure the recognition accuracy, FaceListener leverages the knowledge distillation technique to learn the subtle correlation between face skin deformation and the acoustic signal changes. Experiment results over multiple human beings demonstrate that FaceListener can accurately recognize more than 80% of different facial expressions. FaceListener is highly energy efficient, and can well adapt to different headphone models, host systems and user activities.",https://ieeexplore.ieee.org/document/9825944/
83,Rateau et al.,2022,"Ear and Earable, Hand Gestures and Location",N/A,N/A,N/A,N/A,0,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,Yes (N=20),No,No,No,Yes (N=50),No,No,Sitting,Remote Setting,Earbud,Commercial,N/A,N/A,"Communication, Device Control, Device Input, Music Player, Phone Calls","Earbuds, Elicitation Study, Smartwatch, Wearables","Due to the proliferation of smart wearables, it is now the case that designers can explore novel ways that devices can be used in combination by end-users. In this paper, we explore the gestural input enabled by the combination of smart earbuds coupled with a proximal smartwatch. We identify a consensus set of gestures and a taxonomy of the types of gestures participants create through an elicitation study. In a follow-on study conducted on Amazon’s Mechanical Turk, we explore the social acceptability of gestures enabled by watch+earbud gesture capture. While elicited gestures continue to be simple, discrete, in-context actions, we find that elicited input is frequently abstract, varies in size and duration, and is split almost equally between on-body, proximal, and more distant actions. Together, our results provide guidelines for on-body, near-ear, and in-air input using earbuds and a smartwatch to support gesture capture.",https://dl.acm.org/doi/10.1145/3567710
84,Srivastava et al.,2022,Mouth,Speech Apparatus,Silent Speech (Phonemes),"Accelerometer, Gyroscope",Yes,100,Semantic,Yes,Yes,Yes,No,High,High,High (N=20),High (N=20),No,Yes (N=20),No,No,No,Yes (N=20),No,"Head Movement, Music, Noise, Walking","Bus, Lab",Custom Device,Research Prototype,No,No,"AR/VR, Device Control, Privacy, Silent Speech","IMU Sensing, Signal Processing, Unvoiced Speech","In this paper, we present MuteIt, an ear-worn system for recognizing unvoiced human commands. MuteIt presents an intuitive alternative to voice-based interactions that can be unreliable in noisy environments, disruptive to those around us, and compromise our privacy. We propose a twin-IMU set up to track the user’s jaw motion and cancel motion artifacts caused by head and body movements. MuteIt processes jaw motion during word articulation to break each word signal into its constituent syllables, and further each syllable into phonemes (vowels, visemes, and plosives). Recognizing unvoiced commands by only tracking jaw motion is challenging. As a secondary articulator, jaw motion is not distinctive enough for unvoiced speech recognition. MuteIt combines IMU data with the anatomy of jaw movement as well as principles from linguistics, to model the task of word recognition as an estimation problem. Rather than employing machine learning to train a word classifier, we reconstruct each word as a sequence of phonemes using a bi-directional particle filter, enabling the system to be easily scaled to a large set of words. We validate MuteIt for 20 subjects with diverse speech accents to recognize 100 common command words. MuteIt achieves a mean word recognition accuracy of 94.8% in noise-free conditions. When compared with common voice assistants, MuteIt outperforms them in noisy acoustic environments, achieving higher than 90% recognition accuracy. Even in the presence of motion artifacts, such as head movement, walking, and riding in a moving vehicle, MuteIt achieves mean word recognition accuracy of 91% over all scenarios.",https://doi.org/10.1145/3550281
85,Wang et al.,2022,Head Gestures and Pointing,Head,"Pitch, Yaw",Microphone,No,2,Fine,Yes,Visual Attention,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=12),Sitting,Lab,Headphone,Research Prototype,Yes,No,"Activity Recognition, Device Control, Device Input, Motion Tracking","Acoustic Ranging, Earphone, Head Orientation, Head Pose Estimation","Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7◦ in yaw, and 5.8◦ in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.",https://dl.acm.org/doi/10.1145/3491102.3517698
86,Wang et al.,2022,Mouth,Teeth,"Click, Slide",Microphone,Yes,10,Semantic,Yes,Yes,No,No,High,High,N/A,N/A,No,Yes (N=25),No,No,No,No,Yes (N=25),"Sitting, Standing, Walking","Car, Lab, Living Room, Outdoors, Supermarket",Earbud,Research Prototype,No,No,"AR/VR, Authentification","Acoustic Sensing, Biometrics, Ear Canal, Earable Authentication, Tooth, Wearable","Earables (ear wearables) are rapidly emerging as a new platform encompassing a diverse range of personal applications. The traditional authentication methods hence become less applicable and inconvenient for earables due to their limited input interface. Nevertheless, earables often feature rich around-the-head sensing capability that can be leveraged to capture new types of biometrics. In this work, we propose ToothSonic that leverages the toothprint-induced sonic effect produced by a user performing teeth gestures for earable authentication. In particular, we design representative teeth gestures that can produce effective sonic waves carrying the information of the toothprint. To reliably capture the acoustic toothprint, it leverages the occlusion effect of the ear canal and the inward-facing microphone of the earables. It then extracts multi-level acoustic features to reflect the intrinsic toothprint information for authentication. The key advantages of ToothSonic are that it is suitable for earables and is resistant to various spoofing attacks as the acoustic toothprint is captured via the user’s private teeth-ear channel that modulates and encrypts the sonic waves. Our experiment studies with 25 participants show that ToothSonic achieves up to 95% accuracy with only one of the users’ tooth gestures.",https://doi.org/10.1145/3534606
87,Yang et al.,2022,Actuation,N/A,Vibration (Actuation),N/A,N/A,70,Semantic,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,N/A,Accessibility,"Artificial Ear, Hearing Aid, Vibrotactile","Hearing aid devices have been around for decades, while most of them focus on sound amplification and SNR improvement. This paper proposes an artificial ear based on the vibrotactile feedback. The speech signal is converted into the vibrotactile devices placed around the subject’s ear through the speech recognition algorithm and pattern coding method. Preliminary experiments on the prototype consisting of six motors which has shown that the recognition accuracy of letters and daily sentences reached 90%. The learning time of interpreting the vibrotactile signals could be less than four times that in realtime conversation, proving the feasibility of the proposed device for real-life application.",https://ieeexplore.ieee.org/abstract/document/9928488
88,Alkiek et al.,2023,Ear and Earable,Hand,"Pinch (Ear), Press (Ear), Pull (Ear), Slide (Ear), Tap (Ear)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,Medium,High (N=4),N/A,No,No,No,No,No,Yes (N=4),No,Sitting,Lab,Earbud,Commercial,Yes,No,"Device Control, Device Input, Music Player, Phone Calls","Earables, Gesture Recognition, Inertial Sensors, Natural Interface, On-Body Interaction","Earables have been gaining popularity over the past few years for their ease of use and convenience over wired earphones. However, modern-day earables usually have a limited interface, inhibiting their potential as an accessible medium of input. To this end, we present EarBender: an ear-based real-time system that bridges the gap between earables and on-body interaction, providing a more diverse and natural form of interaction with devices. EarBender enables touch-based hand-to-ear gestures on mobile devices by leveraging inertial sensors in commercially available earable devices. Our proposed system detects the slight deformation in a user’s ear resulting from different ear-based actions including swiping and tapping and classifies the action performed. EarBender is designed to be energy-efficient, easy to deploy and robust to different users, requiring little to no calibration. We implement a prototype of EarBender using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and parameter settings. Results show that the system can detect the occurrence of gestures with a 96.8% accuracy and classify seven different hand-to-ear gestures with an accuracy up to 97.4% maintained across four subjects.",https://dl.acm.org/doi/10.1145/3594739.3610671
89,Chugh et al.,2023,"Face, Head Gestures and Pointing","Facial Expression, Head","Facial Gesture (Eye), Facial Gesture (Other), Pitch, Yaw","Accelerometer, Gyroscope",Yes,5,Semantic,Yes,Visual Attention,No,No,Low,Medium,Low (N=17),N/A,No,No,No,No,No,Yes (N=17),No,Sitting,Lab,Earbud,Commercial,Yes,No,Video Conference,"Earables, Inertial Sensing, Online Meeting","Understanding the level of participation for a remote attendee in an online meeting setup could significantly improve the quality of experience for virtual interaction. However, gauging audience involvement over an online meeting becomes particularly challenging when the attendees prefer to turn off the cameras. IMU data have shown promising results in the past to pervasively monitor users’ body language, including the determination of various bodily gestures, postures, and facial expressions. This paper demonstrates how earables could help address the stated problem. We provide a motivational study to assess earables for detecting body language corresponding to involved listeners. We further compare it with other sensing modalities like smartwatches and smartphones and accordingly develop a platform called enVolve. A lab-scale study with 17 participants demonstrates the efficacy of the proposed system with an average F1 score of more than 80%.",https://doi.org/10.1145/3544793.3563419
90,Jin et al.,2023,Mouth,"Facial Expression, Head",Silent Speech (Morphemes),"Accelerometer, Gyroscope",No,6,Semantic,Yes,Yes,Yes (Performance Loss),No,High,High,Medium (N=10),N/A,No,Yes (N=5),No,No,No,Yes (N=10),Yes (N=10),Sitting,Lab,Earbud,Research Prototype,Yes,No,Accessibility,"Comprehensive Asl Recognition, Earbuds, Manual Markers, Non-Manual Markers, Smartwatch","Sign language builds up an important bridge between the d/Deaf and hard-of-hearing (DHH) and hearing people. Regrettably, most hearing people face challenges in comprehending sign language, necessitating sign language translation. However, stateof-the-art wearable-based techniques mainly concentrate on recognizing manual markers (e.g., hand gestures), while frequently overlooking non-manual markers, such as negative head shaking, question markers, and mouthing. This oversight results in the loss of substantial grammatical and semantic information in sign language. To address this limitation, we introduce SmartASL, a novel proof-of-concept system that can 1) recognize both manual and non-manual markers simultaneously using a combination of earbuds and a wrist-worn IMU, and 2) translate the recognized American Sign Language (ASL) glosses into spoken language. Our experiments demonstrate the SmartASL system’s significant potential to accurately recognize the manual and non-manual markers in ASL, effectively bridging the communication gaps between ASL signers and hearing people using commercially available devices.",https://doi.org/10.1145/3596255
91,Li et al.,2023,"Ear and Earable, Hand Gestures and Location",Hand,"Calling Gesture, Cover (Ear), Cover (Face), Hold (Face), Pinch (Ear), Support (Face), Thinking Gesture",Microphone,No,8,Semantic,No,Yes,Yes (Performance Loss),No,Low,Medium,High (N=10),N/A,Yes (N=10),No,Yes (N=25),No,Yes (N=25),Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Device Control, Device Input","Acoustic Sensing, Hand Gestures, Sensor Fusion","Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield signifcant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we frst gathered candidate gestures and then applied a structural analysis to them in diferent dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3% for recognizing 3 gestures and 91.5% for recognizing 8 gestures (excluding the ""empty"" gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.",https://dl.acm.org/doi/10.1145/3544548.3581008
92,Panda et al.,2023,"Ear and Earable, Hand Gestures and Location, Head Gestures and Pointing","Hand, Head","Cover (Face), Cup (Mid-Air), Lift (Earable), Pitch, Press (Earable), Roll, Yaw","Accelerometer, Button, Gyroscope, Lidar Sensor, Magnetometer",Yes,11,"Coarse, Fine, Semantic",Partly,Visual Attention,No,No,Medium,High,N/A,N/A,Yes,No,No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,Yes,No,Video Conference,"Design Space, Headphones, Research Through Design, Sensing, Wearables","Via Research through Design (RtD), we explore the potential of headphones as a general-purpose input device for both foreground motion-gestures as well as background sensing of user activity. As a familiar wearable device, headphones ofer a compelling site for head-situated interaction and sensing. Using emerging sensing modalities such as inertial motion, capacitive touch sensing, and depth cameras, our implemented prototypes explore sensing and interaction techniques that ofer a range of compelling capabilities. User scenarios include context-aware privacy, gestural audiovisual control, and co-opting natural body language as context to drive animated avatars for ""camera-of"" scenarios in remote workor to co-opt (oft-subconscious) head movements such as dodging attacks in video games to enhance the gameplay experience. Drawing from literature and other frameworks, we situate our prototypes and related techniques in a design space across the dual dimensions of (1) type of input (touch, mid-air, or head orientation); and (2) the context of user action (application, body, or environment). In particular, interactions that combine multiple inputs and contexts at the same time ofer a rich design space of headphonesituated wearable interactions and sensing techniques.",https://dl.acm.org/doi/10.1145/3563657.3596022
93,Paul et al.,2023,"Brain, Eye-Tracking",Eye,"Blink, Horizontal Gaze, Vertical Gaze","EEG, EOG",Yes,3,"Fine, Semantic",Yes,No,No,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,No,No,"Bci-Application, Device Control, Health","BCI, Body Sensor Network, Dry Electrodes, EMG, EOG, Health Sensing, In-Ear EEG, PCB Electrodes, WiFi","To enable continuous, mobile health monitoring, body-worn sensors need to offer comparable performance to clinical devices in a lightweight, unobtrusive package. This work presents a complete versatile wireless electrophysiology data acquisition system (weDAQ) that is demonstrated for in-ear electroencephalography (EEG) and other on-body electrophysiology with user-generic dry-contact electrodes made from standard printed circuit boards (PCBs). Each weDAQ device provides 16 recording channels, driven right leg (DRL), a 3-axis accelerometer, local data storage, and adaptable data transmission modes. The weDAQ wireless interface supports deployment of a body area network (BAN) capable of aggregating various biosignal streams over multiple worn devices simultaneously, on the 802.11n WiFi protocol. Each channel resolves biopotentials ranging over 5 orders of magnitude with a noise level of 0.52 μVrms over a 1000-Hz bandwidth, and a peak SNDR of 119 dB and CMRR of 111 dB at 2 ksps. The device leverages in-band impedance scanning and an input multiplexer to dynamically select good skin contacting electrodes for reference and sensing channels. In-ear and forehead EEG measurements taken from subjects captured modulation of alpha brain activity, electrooculogram (EOG) characteristic eye movements, and electromyogram (EMG) from jaw muscles. Simultaneous ECG and EMG measurements were demonstrated on multiple, freelymoving subjects in their natural office environment during periods of rest and exercise. The small footprint, performance, and configurability of the open-source weDAQ platform and scalable PCB electrodes presented, aim to provide the biosensing community greater experimental flexibility and lower the barrier to entry for new health monitoring research.",https://ieeexplore.ieee.org/document/10115033
94,Stanke et al.,2023,Actuation,N/A,"Display (Actuation), Electrotactile (Actuation), Light (Actuation), Poke (Actuation), Sound (Actuation), Thermal (Actuaction), Vibration (Actuation)",N/A,N/A,8,Semantic,Yes,Yes,Yes,N/A,Medium,High,N/A,N/A,No,Yes (N=18),No,No,No,No,No,"Sitting, Sports","Fitness Center, Lab",Custom Device,Research Prototype,N/A,N/A,Communication,"Ear Clip, Ear-Worn, Earlobe, Earring, Electrotactile, Light, Notification, Poke, Sound, Thermal, Vibration, Wearable","The earlobe is a well-known location for wearing jewelry, but might also be promising for electronic output, such as presenting notifications. This work elaborates the pros and cons of different notification channels for the earlobe. Notifications on the earlobe can be private (only noticeable by the wearer) as well as public (noticeable in the immediate vicinity in a given social situation). A user study with 18 participants showed that the reaction times for the private channels (Poke, Vibration, Private Sound, Electrotactile) were on average less than 1 s with an error rate (missed notifications) of less than 1 %. Thermal Warm and Cold took significantly longer and Cold was least reliable (26 % error rate). The participants preferred Electrotactile and Vibration. Among the public channels the recognition time did not differ significantly between Sound (738 ms) and LED (828 ms), but Display took much longer (3175 ms). At 22 % the error rate of Display was highest. The participants generally felt comfortable wearing notification devices on their earlobe. The results show that the earlobe indeed is a suitable location for wearable technology, if properly miniaturized, which is possible for Electrotactile and LED. We present application scenarios and discuss design considerations. A small field study in a fitness center demonstrates the suitability of the earlobe notification concept in a sports context.",https://doi.org/10.1145/3610925
95,Yi et al.,2023,Mouth,Speech Apparatus,Silent Speech (Phonemes),EMG,Yes,10,Semantic,Yes,Yes,No,No,High,High,Medium (N=45),Medium (N=45),Yes (N=10),No,No,No,No,Yes (N=45),No,"Command Deformation, Head Movement, Sitting",Lab,Custom Device,Research Prototype,No,No,"Music Player, Phone Calls, Silent Speech","EMG Signals, Micro-Interaction, Silent Command Recognition","The prevalence of smart devices encourages increasing requirements of wearable human–computer interactions. To improve user acceptance, such interactions require easy-tomanipulate and unobtrusive characteristics. In this article, we, for the first time, propose to recognize silent commands through a lightweight and around-ear biosensing system Mordo that can be easily integrated with earphones, manipulate smart devices, and minimize social awkwardness. In particular, we first determine the empirical principles of constructing commands and experimentally screen the commands based on the around-ear configuration. Second, we select the optimal around-ear sensor configuration according to the single-channel signal-to-noise ratios (SNRs) and classification accuracies. Third, we propose a multistream CNN-LSTM network to learn the spatiotemporal mapping between the around-ear signals and commands. Finally, extensive experiments have been conducted to evaluate the feasibility and stability. The results indicate an averaged accuracy of 89.66% that outperforms other algorithms of similar tasks. The stability tests show that our system presents sufficient stability under command deformations and head motions. We demonstrate the necessity of collecting such scale of data by gradually reducing training data size. We also validate the generalization ability of our method toward other sensing parameters by reducing the spatial and temporal resolutions. The proof-of-concept design will aim the further development of the commercial products for silent command recognition.",https://ieeexplore.ieee.org/document/9878163
96,Zhang et al.,2023,Ear and Earable,Hand,Touch (Ear),"Microphone, Speaker",Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,No,"Music Player, Phone Calls","Acoustic Sensing, Bone Conduction, Human Interface, Positioning","The advancement of semiconductor and battery technologies popularized tiny acoustic wearable devices such as bone conduction wireless headsets. However, this small form factor poses inconvenience when controlling these devices, as they cannot equip large footprint intuitive interfaces such as volume sliders and touch screens. This paper presents a technique using acoustic responses measured by a bone conduction speaker and a microphone to utilize the ear as a touch input interface. We discovered that a finger placed on different parts of the ear affects the acoustic radiation characteristic of the ear, modulating the leaked sound, and by leveraging this effect, the touch position can be estimated. Experimental results show that five distinct frequency responses with five different finger positions can be obtained, which indicates that our method could allow bone conduction headsets to capture continuous finger positions without additional hardware.",https://dl.acm.org/doi/10.1145/3560905.3568075
97,Zhang et al.,2023,Face,Facial Expression,Ekman 7,Microphone,Yes,7,Semantic,Yes,Visual Attention,Yes (Performance Loss),Yes,Low,Low,N/A,N/A,No,Yes (N=20),No,No,No,No,Yes (N=20),"Head Movement, Sitting, Time, Walking, Wearing Styles",Lab,Earbud,Research Prototype,No,No,"AR/VR, Accessibility, Device Input, Driving, Emotion Recognition, Health","Earable Computing, Facial Reconstruction, IoT, Mobile Computing, Wearable Sensing","This article presents EARFace, a system that shows the feasibility of tracking facial landmarks for 3D facial reconstruction using in-ear acoustic sensors embedded within smart earphones. This enables a number of applications in the areas of facial expression tracking, user interfaces, AR/VR applications, affective computing, and accessibility, among others. Although conventional vision-based solutions break down under poor lighting and occlusions, and also suffer from privacy concerns, earphone platforms are robust to ambient conditions while being privacy-preserving. In contrast to prior work on earable platforms that perform outer-ear sensing for facial motion tracking, EARFace shows the feasibility of completely in-ear sensing with a natural earphone form factor, thus enhancing the comfort levels of wearing. The core intuition exploited by EARFace is that the shape of the ear canal changes due to the movement of facial muscles during facial motion. EARFace tracks the changes in shape of the ear canal by measuring ultrasonic channel frequency response of the inner ear, ultimately resulting in tracking of the facial motion. A transformer-based machine learning model is designed to exploit spectral and temporal relationships in the ultrasonic channel frequency response data to predict the facial landmarks of the user with an accuracy of 1.83 mm. Using these predicted landmarks, a 3D graphical model of the face that replicates the precise facial motion of the user is then reconstructed. Domain adaptation is further performed by adapting the weights of layers using a group-wise and differential learning rate. This decreases the training overhead in EARFace. The transformer-based machine learning model runs on smart phone devices with a processing latency of 13 ms and an overall low power consumption profile. Finally, usability studies indicate higher levels of comforts of wearing EARFace’s earphone platform in comparison with alternative form factors.",https://doi.org/10.1145/3614438
98,Zhang et al.,2023,Mouth,Speech Apparatus,Silent Speech (Words),"Microphone, Speaker",Yes,8,Semantic,Yes,Yes,Yes (Performance Loss),Yes,High,High,Medium (N=18),Medium (N=18),No,Yes (N=18),No,No,No,Yes (N=18),No,"Device, Music",Lab,Headphone,Research Prototype,No,No,"Music Player, Silent Speech","Acoustic Sensing, Commodity-Off-Theshelf, Headphones, Silent Speech","We present HPSpeech, a silent speech interface for commodity headphones. HPSpeech utilizes the existing speakers of the headphones to emit inaudible acoustic signals. The movements of the temporomandibular joint (TMJ) during speech modify the reflection pattern of these signals, which are captured by a microphone positioned inside the headphones. To evaluate the performance of HPSpeech, we tested it on two headphones with a total of 18 participants. The results demonstrated that HPSpeech successfully recognized 8 popular silent speech commands for controlling the music player with an accuracy over 90%. While our tests use modified commodity hardware (both with and without active noise cancellation), our results show that sensing the movement of the TMJ could be as simple as a firmware update for ANC headsets which already include a microphone inside the hear cup. This leaves us to believe that this technique has great potential for rapid deployment in the near future. We further discuss the challenges that need to be addressed before deploying HPSpeech at scale.",https://doi.org/10.1145/3594738.3611365
99,Zhu et al.,2023,Head Gestures and Pointing,Head,"Pitch, Roll, Yaw","Accelerometer, Gyroscope",Yes,12,Semantic,Yes,Yes,Yes,Yes,Medium,High,High (N=15),N/A,No,No,No,No,No,Yes (N=15),No,"Sports, Standing, Walking, Walking Stairs",Lab,Earbud,Research Prototype,Yes,No,"Activity Recognition, Device Control, Music Player, Phone Calls","Composite Activity Recognition, Earable Device, Multi-Task Learning","The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations. We implement a real-time prototype and conduct extensive experiments to evaluate it. The results show that CHAR can recognize 60 kinds of composite activities (12 head gestures and 5 body movements) with high accuracies of 97.0% and 89.7% in user- dependent and independent cases, respectively.",https://ieeexplore.ieee.org/abstract/document/10916516
100,Dong et al.,2024,Mouth,Speech Apparatus,Silent Speech (Words),"Microphone, Speaker",Yes,100,Semantic,Yes,Yes,No,Yes,High,High,Medium (N=4),Medium (N=9),No,No,No,No,No,Yes (N=4),No,"Head Movement, Noise, Sitting, Standing, Walking, Wearing Styles",Lab,Earbud,Research Prototype,No,No,"Accessibility, Device Control, Silent Speech","Acoustic Sensing, Autoregressive Model, Earable Computing, Silent Speech Interface, Text Entry","Silent speech interaction (SSI) allows users to discreetly input text without using their hands. Existing wearable SSI systems typically require custom devices and are limited to a small lexicon, limiting their utility to a small set of command words. This work proposes ReHEarSSE, an earbud-based ultrasonic SSI system capable of generalizing to words that do not appear in its training dataset, providing support for nearly an entire dictionary’s worth of words. As a user silently spells words, ReHEarSSE uses autoregressive features to identify subtle changes in ear canal shape. ReHEarSSE infers words using a deep learning model trained to optimize connectionist temporal classifcation (CTC) loss with an intermediate embedding that accounts for diferent letters and transitions between them. We fnd that ReHEarSSE recognizes 100 unseen words with an accuracy of 89.3%.",https://doi.org/10.1145/3613904.3642095
101,Ge et al.,2024,Head Gestures and Pointing,Head,Yaw,Microphone,No,1,Fine,Yes,Visual Attention,No,No,Medium,High,High (N=6),High (N=6),No,No,No,No,No,Yes (N=6),Yes,"Distance, Noise, Standing","Outdoors, University Building",Earbud,Research Prototype,No,No,"Activity Recognition, Customer Analytics","Acoustic Signal Processing, Human Computer Interaction, Signal Processing, Systems, User Interfaces","Head tracking is a technique that allows for the measurement and analysis of human focus and attention, thus enhancing the experience of human–computer interaction (HCI). Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakers generates a periodically fluctuating sound field, which the user’s two earphones detect. By assessing the distance and angle alterations between the earphones and speakers, we propose a model to determine the user’s head movement and orientation. Our evaluation results indicate a high degree of accuracy in both head movement tracking, with an average tracking error of 2.98 cm,  and head orientation tracking, with an average error of 1.83◦. Furthermore, in a deployed exhibition scenario, we attained an accuracy of 89.2% in estimating the user’s focus direction.",https://ieeexplore.ieee.org/document/10192901
102,Hu et al.,2024,Head Gestures and Pointing,Head,"Pitch, Yaw","Accelerometer, Microphone",No,2,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=50),"Device, Distance, Noise, Position, Running, Sitting, Standing, Walking","Car, Lab, Living Room, Office, Outdoors","Earbud, Headphone",Research Prototype,Yes,No,"AR/VR, Driving, Motion Tracking","Acoustic Signal, Head Motion Tracking, Humanmachine Interface","Head motion tracking is a promising research field with vast applications in ubiquitous human-computer interaction (HCI) scenarios. Unfortunately, solutions based on vision and wireless sensing have shortcomings in user privacy and tracking range, respectively. To address these issues, we propose IA-Track, a novel head motion tracking system that combines inertial measurement units (IMU) and acoustic sensing. Our wireless earphone-based method balances flexibility, computational complexity, and tracking accuracy, requiring only an earphone with an IMU and a smartphone. However, we still face two challenges. First, wireless earphones have limited hardware resources, making acoustic Doppler effect-based method unsuitable for acoustic tracking. Second, traditional Kalman filter-based trajectory restoration methods may introduce significant cumulative errors. To tackle these challenges, we rely on IMU sensor data to recover the trajectory and use smartphones to emit ”inaudible” acoustic signals that the earphone receives to adjust the IMU drift track. We conducted extensive experiments involving 50 volunteers in various potential IA-Track usage scenarios, demonstrating that our well-designed system achieves satisfactory head motion tracking performance.",https://ieeexplore.ieee.org/document/10288089
103,Hu et al.,2024,Head Gestures and Pointing,Head,"Pitch, Yaw",Microphone,No,2,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=32),"Device, Distance, Noise, Position","Café, Lab, Library, Outdoors",Earbud,Commercial,Yes,No,"AR/VR, Device Control, Privacy","Acoustic Ranging, Acoustic Sensing, Head Motion Tracking, Human–Computer Interaction","Accurate head movement tracking is crucial for virtual reality and Metaverse in ubiquitous human-computer interaction (HCI) applications. Existing works for head tracking with wearable VR kits and wireless signals require expensive devices and heavy algorithmic processing. To resolve this problem, we propose HeadTrack, a low-cost, high-precision head motion tracking system that uses commercially available wireless earphones to capture the user’s head motion in real-time. HeadTrack uses smartphones as ‘sound anchors’ and emits inaudible chirps picked up by the user’s wireless earphones. By measuring the time-of-flight of these signals from the smartphone to each microphone on the earphone, we can deduce the user’s face orientation and distance relative to the smartphone, enabling us to accurately track the user’s head movement. To realize HeadTrack, we use the cross-correlation method to optimize the Frequency Modulated Continuous Wave (FMCW) based acoustic ranging method, which solves the problem of insufficient wireless earphone bandwidth. Moreover, we solve the problems of asynchronous startup time between devices and the existence of sampling frequency offset. We conduct excessive experiments in real scenarios, and the results prove that HeadTrack can continuously track the direction of the user’s head, with an average error under 6.3◦ in pitch and 4.9◦ in yaw.",https://ieeexplore.ieee.org/document/10373032
104,Lepold et al.,2024,Eye-Tracking,Eye,Vertical Gaze,EOG,Yes,1,Fine,Yes,No,No,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,No,No,Health,"Bio-Potential, EEG, EMG, EOG, Earables, Electroencephalography, Electromyography, Electrooculography, Hearables, Open-Source","While traditional earphones primarily offer private audio spaces, so-called “earables” emerged to offer a variety of sensing capabilities. Pioneering platforms like OpenEarable have introduced novel sensing platforms targeted at the ears, incorporating various sensors. The proximity of the ears to the eyes, brain, and facial muscles has also sparked investigation into sensing biopotentials. However, currently there is no platform available that is targeted at the ears to sense biopotentials. To address this gap, we introduce OpenEarable ExG - an open-source hardware platform designed to measure biopotentials in and around the ears. OpenEarable ExG can be freely configured and has up to 7 sensing channels. We initially validate OpenEarable ExG in a study with a left-right in-ear dualelectrode montage setup with 3 participants. Our results demonstrate the successful detection of smooth pursuit eye movements via Electrooculography (EOG), alpha brain activity via Electroencephalography (EEG), and jaw clenching via Electromyography (EMG). OpenEarable ExG is part of the OpenEarable initiative and is fully open-source under MIT license.",https://doi.org/10.1145/3675094.3678480
105,Ronco et al.,2024,Hand Gestures and Location,Hand,"Circle (Mid-Air), Hold (Mid-Air), Pinch (Mid-Air), Pull (Mid-Air), Push (Mid-Air), Rub (Mid-Air), Slide (Mid-Air), Tilt (Mid-Air)",mmWaveRadar,Yes,11,Semantic,No,Yes,Yes,No,Low,Medium,High (N=20),N/A,No,No,No,No,No,Yes (N=20),No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,Device Control,"Embedded, Gesture Recognition, Low-Power, Mm-Wave, Radar, Sensor","Smart Internet of Things (IoT) devices are on the rise in popularity, with innovative use cases and applications emerging every year. Including intelligence in these novel systems presents the challenge of integrating interaction and communication in scenarios where traditional interfaces are not viable. Hand Gesture Recognition (HGR) has been proposed as an intuitive Human-Machine Interface, potentially suitable for controlling several classes of devices in the context of the Internet of Things. This paper proposes a low-power in-ear HGR system based on mm-wave radars, efficient spatial and temporal Convolutional Neural Networks and an energy-optimized hardware design. The design is suitable for battery-operated devices, with stringent size and energy constraints, enabling user interaction with wearable devices, but also suitable for home appliances and industrial applications. The proposed machine learning model is characterized thoroughly for robustness and generalization capabilities, achieving 94.9% (single subject) and 86.1% (LeaveOne-Out Cross-validation) accuracy on a set of 11+1 gestures with a model size of only 36 KiB and inference latency of 32.4 ms on a 64 MHz Cortex-M33 microcontroller, making it compatible with real-time applications. The system is demonstrated in a fully integrated, miniaturized in-ear device with a full-system average power consumption of 18.4 mW, a more than 6x improvement on the current state of the art.",https://ieeexplore.ieee.org/document/10562162
106,Sato et al.,2024,Ear and Earable,Hand,"Fold (Ear), Pinch (Ear), Pull (Ear), Swipe (Ear)","Accelerometer, Gyroscope, Magnetometer",Yes,15,Semantic,No,Yes,No,No,Medium,Medium,Low (N=10),Low (N=10),Yes (N=19),No,No,No,No,Yes (N=10),No,"Sitting, Walking",Lab,Earbud,Commercial,No,No,"Communication, Data Annotation, Device Control, Device Input, Health, Music Player, Phone Calls","Gesture Elicitation Study, Hands Gesture Recognition, Hearables, IMU, User-Defined Gesture","Hearables are highly functional earphone-type wearables; however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures. In previous research on hearables for hand input, user understanding and gesture recognition systems have been developed. However, in the realm of user understanding, investigation concerning hand input with hearables remains incomplete, and existing recognition systems have not demonstrated proficiency in discerning user-defined gestures. In this study, we conducted a gesture elicitation study (GES) assuming hand input using hearables under six conditions (three interaction areas × two device shapes). Then, we extracted ear-level gestures that the device’s built-in IMU sensor could recognize from the user-defined gestures and investigated the recognition performance. The results of sitting experiments showed that the gesture recognition rate for in-ear devices was 91.0% and that for ear-hook devices was 74.7%.",https://dl.acm.org/doi/10.1145/3676503
107,Shojaeifard et al.,2024,Head Gestures and Pointing,Head,Yaw,"Accelerometer, Gyroscope",Yes,1,Coarse,Yes,Yes,No,No,Medium,High,High (N=6),Medium (N=1),No,No,No,No,No,Yes (N=6),No,Sitting,"Car, Lab",Earbud,Commercial,No,No,"Driving, Safety","Earables, Head Position Tracking, Vehicular Sensing","The Internet of Things is enabling innovations in the automotive industry by expanding the capabilities of vehicles by connecting them with the cloud. One important application domain is traffic safety, which can benefit from monitoring the driver’s condition on how safely they are handling the vehicle. By detecting drowsiness, inattentiveness, and distraction of the driver, it is possible to react before accidents happen. This paper uses accelerometer and gyroscope data collected using an ear-worn sensor to classify the orientation of the driver’s head in a moving vehicle. We show that lightweight machine learning algorithms such as Random Forest and K-Nearest Neighbor can be used to reach accurate classifications even without applying any noise reduction to the signal data. Data cleaning and transformation approaches are studied to see how they give deeper insights into the classification problem. This study paves the way for the development of driver monitoring systems capable of reacting to anomalous driving behaviour before traffic accidents can happen.",https://doi.org/10.1145/3627050.3627067
108,Srivastava et al.,2024,Mouth,Speech Apparatus,"Silent Speech (Sentences), Silent Speech (Words)","Accelerometer, Gyroscope",No,65,Semantic,Yes,Yes,Yes,No,High,High,Medium (N=19),N/A,No,No,No,No,No,Yes (N=19),Yes (N=19),"Noise, Sitting, Walking",Lab,Custom Device,Research Prototype,No,No,"AR/VR, Accessibility, Authentification, Communication, Device Control, Device Input, Music Player, Privacy, Security, Silent Speech","Accessible Interfaces, Earables, GPT, IMU Sensing, Llm, Silent Speech, Transformers","We present Unvoiced, a novel unvoiced user interface that leverages jaw motion to enable users to silently interact with their devices using earables. The core idea is to translate low-frequency jaw motion signals into high-frequency information-rich mel spectrograms. Our proposed cross-modal translation incorporates phonetic, contextual, and syntactic information, while the specialized loss function optimizes for these linguistic features. This ensures that the generated spectrograms capture nuanced speech characteristics. Evaluated for 19 users across four tasks, Unvoiced demonstrates >94% task completion rate and <9% word error rate for over 90% of phrases. Further, Unvoiced maintains >90% task completion rate in noisy conditions.",https://doi.org/10.1145/3666025.3699374
109,Shimon et al.,2024,"Ear and Earable, Hand Gestures and Location",N/A,N/A,N/A,N/A,0,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,Yes (N=18),No,No,No,No,No,No,Sitting,Lab,N/A,N/A,N/A,N/A,Device Control,"Earables, Earbased Interaction, Embodied Interaction, Input Techniques, Touch Surfaces, Uni-Manual Interaction","Small form factor limits physical input space in earable (i.e., ear-mounted wearable) devices. Off-device earable inputs in alternate mid-air and on-skin around-ear interaction spaces using uni-manual gestures can address this input space limitation. Segmenting these alternate interaction spaces to create multiple gesture regions for reusing off-device gestures can expand earable input vocabulary by a large margin. Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration. We collected and analyzed 7560 uni-manual gesture motion data from 18 participants to explore earable gesture reuse by segmentation of on-skin and mid-air spaces around the ear. Our results show that gesture performance degrades significantly beyond 3 mid-air and 5 on-skin around-ear gesture regions for different uni-manual gesture classes (e.g., swipe, pinch, tap). We also present qualitative findings on most and least preferred regions (and associated boundaries) by end-users for different uni-manual gesture shapes across both interaction spaces for earable devices. Our results complement earlier elicitation studies and interaction technologies for earables to help expand the gestural input vocabulary and potentially drive future commercialization of such devices.",https://dl.acm.org/doi/10.1145/3643513
110,Srivastava et al.,2024,Mouth,Speech Apparatus,Silent Speech (Sentences),"Accelerometer, EMG, Gyroscope",No,12,Semantic,Yes,Yes,No,Yes,High,High,High (N=9),N/A,No,No,No,No,No,Yes (N=9),No,Sitting,Lab,"Earbud, Headphone",Research Prototype,Yes,No,"AR/VR, Accessibility, Communication, Device Control, Device Input, Music Player, Phone Calls, Privacy, Video Conference","Accessibility, Exg, IMU Sensing, Silent Speech Recognition","Silent speech recognition has emerged as a promising approach for enabling hands-free and discreet interaction with head-worn devices. In this paper, we present QuietSync, a multimodal system that combines inertial measurement unit (IMU) and contact electrode (ExG) signals to achieve accurate silent speech recognition using of-the-shelf devices. QuietSync utilizes an IMU attached to the lower part of the headphones near the ear and strategically places ExG electrodes on the headphones, glasses (nose and behind the ear), and face (for VR applications) to capture subtle movements and muscle activity associated with silent speech production. We conducted a user study with 9 participants and successfully recognized 12 commands with an accuracy of 94.2%. Our system leverages the complementary nature of IMU and ExG signals to enhance the robustness and reliability of silent speech recognition. The IMU captures subtle movements of the jaw and facial muscles, while the ExG electrodes detect low-amplitude surface muscle activity associated with speech production. We show that our system is not afected by the length and speech mannerisms of the commands, and can be fne-tuned for users of varied native languages with only 5 samples. Our fndings demonstrate the feasibility of using of-the-shelf head-worn devices to enable silent speech recognition, opening up new possibilities for seamless and discreet interaction with devices such as VR/AR headsets and earables. To the best of our knowledge, QuietSync is the frst system to enable silent speech interaction for multiple form factors.",https://doi.org/10.1145/3678957.3685720
111,Sun et al.,2024,Mouth,Speech Apparatus,"Silent Speech (Phonemes), Silent Speech (Words)","Microphone, Speaker",Yes,76,Semantic,Yes,Yes,No,No,High,High,High (N=50),Medium (N=50),No,Yes (N=50),No,No,No,Yes (N=50),No,"Hand Motion, Head Movement, Mask, Music, Noise, Sitting, Walking, Wearing Styles",Lab,Earbud,Research Prototype,No,No,Silent Speech,"Acoustic Sensing, Earphone, Silent Speech Recognition","As the most natural and convenient way to communicate with people, speech is always preferred in HumanComputer Interactions. However, voice-based interaction still has several limitations. It raises privacy concerns in some circumstances and the accuracy severely degrades in noisy environments. To address these limitations, silent speech recognition (SSR) has been proposed, which leverages the inaudible information (e.g., lip movements and throat vibration) to recognize the speech. In this paper, we present EarSSR, an earphone-based silent speech recognition system to enable interaction without a need of vocalization. The key insight is that when people are speaking, their ear canals exhibit unique deformation patterns and the corresponding deformation patterns are related to words/letters even without any vocalization. We utilize the built-in microphone and speaker of an earphone to capture the ear canal deformation. Ultrasound signals are emitted and the reflected signals are analyzed to extract the signal features corresponding to speech-induced ear canal deformation for silent speech recognition. We design a two-channel hierarchical convolutional neural network to achieve fine-grained letter/word recognition. Our extensive experiments show that EarSSR can achieve an accuracy of 82% for single alphabetic letter recognition and an accuracy of 93% for word recognition.",https://ieeexplore.ieee.org/document/10411110
112,Suzuki et al.,2024,Hand Gestures and Location,Hand,"Calling Gesture, Grip (Mid-Air), Squeeze (Mid-Air), Swipe (Mid-Air), Twist (Mid-Air)","Microphone, Speaker",Yes,7,Semantic,No,Yes,Yes,No,Low,Medium,Low (N=20),Low (N=20),Yes (N=11),No,Yes (N=11),No,Yes (N=11),Yes (N=20),No,"Gloves, Music, Sitting, Surplus Person, Walking",Lab,Earbud,Research Prototype,No,No,Device Input,"Acoustic Sensing, Deep Learning, Doppler Efect, Hearables, Mid-Air Gesture Recognition, Sound Leakage","We introduce EarHover, an innovative system that enables mid-air gesture input for hearables. Mid-air gesture input, which eliminates the need to touch the device and thus helps to keep hands and the device clean. However, existing mid-air gesture input methods for hearables have been limited to adding cameras or infrared sensors. By focusing on the sound leakage phenomenon unique to hearables, we have realized mid-air gesture recognition using a speaker and an external microphone that are highly compatible with hearables. The signal leaked to the outside of the device due to sound leakage can be measured by an external microphone, which detects the diferences in refection characteristics caused by the hand’s speed and shape during mid-air gestures. Among 27 types of gestures, we determined the seven suitable gestures for EarHover in terms of signal discrimination and user acceptability. We then evaluated the gesture detection and classifcation performance of two prototype devices (in-ear type/open-ear type) for real-world application scenarios.",https://dl.acm.org/doi/10.1145/3654777.3676367
113,Wang et al.,2024,Hand Gestures and Location,Hand,Slide (Face),Microphone,Yes,5,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,Yes (N=20),No,No,Yes (N=20),No,Yes (N=26),"Make-Up, Music, Noise, Sitting, Sports, Standing, Walking","Car, Living Room, Office, Outdoors",Earbud,Research Prototype,No,No,Authentification,"Biometrics, Earable, Fingerprint, Friction, User Authentication","Ear wearables (earables) are emerging platforms that are broadly adopted in various applications. There is an increasing demand for robust earables authentication because of the growing amount of sensitive information and the IoT devices that the earable could access. Traditional authentication methods become less feasible due to the limited input interface of earables. Nevertheless, the rich head-related sensing capabilities of earables can be exploited to capture human biometrics. In this paper, we propose EarSlide, an earable biometric authentication system utilizing the advanced sensing capacities of earables and the distinctive features of acoustic fingerprints when users slide their fingers on the face. It utilizes the inward-facing microphone of the earables and the face-ear channel of the ear canal to reliably capture the acoustic fingerprint. In particular, we study the theory of friction sound and categorize the characteristics of the acoustic fingerprints into three representative classes, pattern-class, ridge-groove-class, and coupling-class. Different from traditional fingerprint authentication only utilizes 2D patterns, we incorporate the 3D information in acoustic fingerprint and indirectly sense the fingerprint for authentication. We then design representative sliding gestures that carry rich information about the acoustic fingerprint while being easy to perform. It then extracts multi-class acoustic fingerprint features to reflect the inherent acoustic fingerprint characteristic for authentication. We also adopt an adaptable authentication model and a user behavior mitigation strategy to effectively authenticate legit users from adversaries. The key advantages of EarSlide are that it is resistant to spoofing attacks and its wide acceptability. Our evaluation of EarSlide in diverse real-world environments with intervals over one year shows that EarSlide achieves an average balanced accuracy rate of 98.37% with only one sliding gesture.",https://dl.acm.org/doi/10.1145/3643515
114,Wang et al.,2024,Hand Gestures and Location,Hand,"Slide (Face), Tap (Face)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),Sitting,Lab,Earbud,Commercial,No,No,Authentification,"Adversarial Learning, Implicit Authentication, Wearable Computing","The surge in popularity of wireless headphones, particularly wireless earbuds, as smart wearables, has been notable in recent years. These devices, empowered by artificial intelligence (AI), are broadening their utility in areas such as speech recognition, augmented reality, pose recognition, and health care monitoring, thereby enriching user experiences through novel interactive interfaces driven by embedded sensors. However, the widespread adoption of wireless earbuds has spurred concerns regarding security and privacy, necessitating robust bespoke security measures. Despite the miniaturization of mobile chips enabling the integration of sophisticated algorithms into smart wearables, the research and industrial communities have yet to accord adequate attention to earbud security. This article focuses on empowering wireless earbuds to authenticate their legitimate users, tackling the challenges associated with conventional authentication methods. Instead of relying on input interface authentication methods like PIN or lock patterns, this research delves into leveraging inertial measurement unit (IMU) data collected during interactions with devices to extract novel biometric features, presenting an alternative approach that nonetheless confronts challenges related to signal capture and interference. Consequently, we propose and design BudsAuth, an implicit user authentication framework that harnesses built-in IMU sensors in smart earbuds to capture vibration signals induced by onface touching interactions with the earbuds. These vibrations are utilized to deliver continuous and implicit user authentication with high precision and compatibility across various earbud models. Extensive evaluation demonstrates BudsAuth’s capability to achieve an equal error rate (EER) of 0.0003, representing an approximate 99.97% accuracy with seven consecutive samples of interactive gestures for implicit authentication.",https://ieeexplore.ieee.org/abstract/document/10478100
115,Wang et al.,2024,Hand Gestures and Location,Hand,Write (Face),Microphone,Yes,36,Semantic,No,Yes,Yes,Yes,Low,Medium,High (N=10),High,No,Yes (N=20),No,No,No,Yes (N=10),No,"Head Movement, Sitting, Standing, Walking","Car, Living Room, Office, Outdoors",Earbud,Research Prototype,No,No,"Device Input, Privacy","Acoustic Sensing, Earable, Face and Ear Interaction, Gestures Recognition","As earables gain popularity, there emerges a need for intuitive user interfaces that adapt to diverse daily scenarios. Traditional methods like touchscreens and voice control often fall short in environments like movie theatres, where silence and darkness are required, or on busy streets where visual distraction introduces extra risk. We propose an innovative earable-based system utilizing unique acoustic friction generated by fingers for alphanumeric input. Our approach digs into the acoustic friction theory, applying this knowledge to better understand the transformation from 2D handwriting into a 1D acoustic time series. This theoretical foundation guides our system design and feature extraction. Specifically, we have redesigned certain characters to enhance their acoustic distinctiveness without compromising the natural handwriting style of users, ensuring the system userfriendly. Our system combines DenseNet and GRU architectures in a multimodal model, refined through transfer learning to adapt to diverse user behaviors. Tested in real-world scenarios with 10 participants, our system achieves a 95% accuracy in recognizing both letters and numbers.",https://ieeexplore.ieee.org/document/10637602
116,Xie et al.,2024,Head Gestures and Pointing,Head,General Head Movement,"Accelerometer, Gyroscope",No,1,Semantic,Yes,Yes,Yes,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=30),"Cycling, Music, Noise, Sitting, Standing, Walking, Wearing Styles","Lab, Outdoors",Earbud,Commercial,Yes,No,Authentification,"Earphones, Head Motion, Ubiquitous Computing, User Authentication","With the growing reliance on digital systems in today’s mobile Internet era, robust authentication methods are crucial for safeguarding personal data and controlling access to resources. Conventional methods, such as knowledge-based and biometric-based authentication, are widely used but still have some usage limitations and potential security concerns, like wearing protective suits/masks or being imitated by attackers with ulterior motives. In this paper, we propose another earphone-based authentication system, namely EarPass, that leverages users’ unique head motion patterns in response to a very short period of music segment. Here, we employ a Convolutional Neural Network (CNN)-based feature extractor to capture and map distinct head motions into a well-separated latent space, achieving high-dimensional data extraction. We demonstrate the consistency, uniqueness, and robustness of head motion patterns through extensive experiments and reach a 98.2% F1-score, indicating superior performance compared to conventional authentication methods. Additionally, EarPass is user-friendly, secure, and adaptable to various environments, including noisy and movement-oriented scenarios. By integrating the authentication system into Android devices, we showcase its real-world applicability and low energy consumption with minimal latency. The source code of EarPass will be open-source to further research and collaboration within the community.",https://ieeexplore.ieee.org/document/10631065
117,Yang et al.,2024,"Ear and Earable, Hand Gestures and Location",Hand,"Approach (Mid-Air), Click (Mid-Air), Close (Mid-Air), Cover (Face), Open (Mid-Air), Pinch (Ear), Pinch (Face), Press (Face), Scratch (Face), Slide (Mid-Air)","Microphone, Speaker",Yes,10,Semantic,No,Yes,No,No,Low,Medium,High (N=22),Medium (N=1),No,Yes (N=22),Yes (N=22),No,Yes (N=22),Yes (N=22),No,"Distance, Eating, Hydration, Music, Noise, Sitting, Speaking, Sports, Standing, Walking",Lab,Earbud,Research Prototype,No,No,"AR/VR, Device Control, Health, Music Player","Acoustic Sensing, Gesture Detection, Wearable Computing","We present MAF, a novel acoustic sensing approach that leverages the commodity hardware in bone conduction earphones for handto-face gesture interactions. Briefly, by shining audio signals with bone conduction earphones, we observe that these signals not only propagate along the surface of the human face but also dissipate into the air, creating an acoustic field that envelops the individual’s head. We conduct benchmark studies to understand how various handto-face gestures and human factors influence this acoustic field. Building on the insights gained from these initial studies, we then propose a deep neural network combined with signal preprocessing techniques. This combination empowers MAF to effectively detect, segment, and subsequently recognize a variety of hand-to-face gestures, whether in close contact with the face or above it. Our comprehensive evaluation based on 22 participants demonstrates that MAF achieves an average gesture recognition accuracy of 92% across ten different gestures tailored to users’ preferences.",https://dl.acm.org/doi/10.1145/3613904.3642437
118,Yi et al.,2024,Mouth,Speech Apparatus,Silent Speech (Phonemes),EMG,Yes,10,Semantic,Yes,Yes,No,No,High,High,Medium (N=33),Medium (N=3),No,No,No,No,No,Yes (N=33),No,"Sitting, Time",Lab,Custom Device,Research Prototype,No,No,Silent Speech,"EMG Signals, Silent Command Recognition, User Adaptation","Wearable human-computer interactions in daily life are increasingly encouraged by the prevalence of intelligent wearables. It poses a demanding requirement of micro-interaction and minimizing social awkwardness. Our previous work demonstrated the feasibility of recognizing silent commands through around-ear biosensors with the limitation of user adaptation. In this work, we ease the limitation by a personalization framework that integrates spectral factorization of signals, temporal confidence rejection and commonly used transfer learning algorithms. Specifically, we first empirically formulate the user adaptation issue by presenting the accuracies of applying transfer learning algorithms to our previous method. Second, we improve the signal-to-noise ratio by proposing the supervised spectral factorization method that learns the amplitude and phase mappings between around-ear signals and the signals of articulated facial muscles. Third, we leverage the time continuity of commands and introduce the time decay into confidence rejection. Finally, extensive experiments have been conducted to evaluate the feasibility and improvements. The results indicate an average accuracy of 92.38% which is significantly larger than solely using transfer learning algorithms. And a comparable accuracy can be achieved with significantly reduced data of new users. The overall performance shows the framework can significantly improve the accuracy of user adaptations. The work would aid a further step toward commercial products for silent command recognition and inspire the solution to the user adaptation challenge of wearable human-computer interactions.",https://doi.org/10.1109/TNSRE.2023.3342068

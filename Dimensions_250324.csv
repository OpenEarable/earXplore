,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ID,Main Author,Year,Location,Filter,Input Body Part,Gesture,Sensors,No Additional Sensing,Number of Selected Gestures,Resolution,Hands- Free,Eyes- Free,Possible on One Ear,Adaptation of the Interaction Detection Algorithm to the Individual User,Discreetness of Interaction Techniques,Social Acceptability of Interaction Techniques,Accuracy of Interaction Recognition,Robustness of Interaction Detection,Elicitation Study,Usability Evaluations,Cognitive Ease Evaluations,Discreetness of Interactions Evaluations,Social Acceptability of Interactions Evaluations,Accuracy of Interactions Evaluation,Alternative Interaction Validity Evaluations,Evaluation of Different Conditions,Evaluations of Different Settings,Earphone Type,Development Stage,Real-Time Processing,On Device Processing,Motivations (Notes),Motivations,Intended Applications,Keywords,Abstract,DB ID,Citation,Study Link,DONE,Discreetness of Interaction Techniques - MK,Social Acceptability of Interaction Techniques - MK,Elemental Task,Degrees of Freedom,Resolution - Alt,Spatial,No. of proposed Semantic Gestures,Discrete,Continuous,Active Feedback,Theoretical Grounding of Interaction Techniques,Usability,"Device 
Appearance","Cognitive 
Ease",Multi-Gesture Detection,Composite Evaluative Score
[33],Weisenberger et al.,1987,Actuation,Enabling,N/A,Vibration (Actuation),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,N/A,N/A,"In the population of hearing-impaired persons in the United States and other countries are a substantial number who do not receive satisfactory benefit from conventional acoustic amplification.  For these persons, some auditory functions can be  taken over to some degree by a substitute sensory modality (i.e., vision or touch).; The studies described above suggest that the tactile system can be a usef~~pfrocessor of acoustic information. In the present study, the design and preliminary evaluation of a vibrotactile aid designed to fill some of these basic auditory function\ are described.",,"Accessibility, Health",N/A,"A binaural earmold sound-to-tactile aid was constructed by inserting a vibrating element into a Lucite earmold. The earmold could be vibrated at either 80 Hz (when incoming acoustic signals were below 2000 I-Ir), at 300 Hz (when incoming acoustic signals were above 2000 Hz), or both (when incoming acoustic signals were broadband). Subjects were fitted with one of these bimodal vibrating earmolds in each ear. Normal-hearing and hearing-impaired subjects were tested in three tasks: sound localization, errvironmental sound identification, and syllable rhythm and stress. The device provided some benefit to performance, although the amounts of improvement varied across tasks and subjects. Possible modifications in device design, and potential combinations of auditory and tactile input via earmold systems, are discussed.",1,,https://books.google.de/books?hl=de&lr=&id=VxxqiKVFM1sC&oi=fnd&pg=PA51&dq=Development+and+preliminary+evaluation+of+an+earmold+sound-to-tactile+aid++for+the+hearing-impaired&ots=vpNdh7d4NX&sig=SpPe8laxSWuV6u8qdxH05Js6s20,DONE,,,,,,,,,,,,,,,,
[714],Brewster et al.,2003,Head Gestures and Pointing,In,Head,"Roll, Pitch","Accelerometer, Gyroscope, Magnetometer",Yes,1,Coarse,Yes,Yes,No,No,Medium,High,N/A,N/A,No,Yes (N=18),No,No,No,No,No,Sitting,Lab,Custom Device,Commercial,Yes,No,"Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use.; Mobile and wearable computers have been one of the major growth areas of computing in recent years. Compared to desktop systems these devices have restricted input and output capabilities that typically reduces their usability. With often very limited amounts of screen space, their visual displays can easily become cluttered with information and widgets. Input is limited, with small keyboards or simple handwriting recognition the norm.; With the imminent dramatic increase in network bandwidth available to mobile and wearable devices, and the consequent rise in the number of possible services, new interaction techniques are needed to access services whilst on the move.; Our solution to input focuses on multi-dimensional gestural interaction.",,Device Input,"Gestural interaction, wearable computing","Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users’ gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.",2,,https://dl.acm.org/doi/abs/10.1145/642611.642694,DONE,,,"Selection, Orientation, Path",2,coarse (4),relative,1,No,Yes,Coupled,Not Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,22%
[32],Metzger et al.,2004,"Hand Gestures and Location, Head Gestures and Pointing",In,"Hand, Head","Slide (Mid-Air), Hold (Mid-Air), Roll","Proximity Sensor, Accelerometer",Yes,7,"Coarse, Semantic",No,Yes,No,No,Low,Low,High (N=1),N/A,No,No,No,No,No,Yes (N=1),No,Sitting,Lab,Headphone,Research Prototype,Yes,No,"FreeDigiter uses hand gesture recognition to control mobile devices. In contrast to many existing gesture recognition systems, data analysis is simple, requiring the filtering of the binary output of a proximity sensor. This simplicity allows for small and inexpensive hardware with low power consumption and high wearability.",,"Music Player, Phone Calls, Accessibility, Device Control",N/A,"We present FreeDigiter, an interface for mobile devices which enables rapid entry of digits using finger gestures. FreeDigiter is an infrared proximity sensor with a dual axis accelerometer and requires little signal processing. Initial laboratory experiments attain accuracy rates of 99.0%; and the system is tolerant to highly varying lighting conditions. The FreeDigiter system requires little power and could be implemented in a very small form factor appropriate for controlling in–ear hearing aids, small MP3 players, and hands–free mobile phone headsets.",3,,https://ieeexplore.ieee.org/document/1364684,DONE,,,"Selection, Path, Quantification",2,coarse,relative,7,Yes,Yes,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,32%
[40],Buil & Hollemans,2005,Ear and Earable,In,Hand,Press (Earable),Button,Yes,3,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,Assess the users accepted operating force for button pushes at the headphones,Improving Usability and Ergonomics,Music Player,N/A,"The touch headphones are a solution for providing playback and volume controls on in-ear type headphones.  One of the issues with placing controls on earpieces is that applied pressure is transferred to the inner ear,  which potentially creates discomfort. The experiment described in this short paper shows that conventional button switches  are not well accepted. Users preferred to operate a button on an earpiece with a force of around 85 grams.",4,,https://ieeexplore.ieee.org/abstract/document/1550805,DONE,,,"Selection, Position",X,X,X,3,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,41%
[84],Buil et al.,2005,Ear and Earable,In,"Hand, Wearable State","Tap (Earable), Hold (Earable), Remove Earbud, Attach Earbud",Capacitive Sensor,Yes,5,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Research Prototype,Yes,No,Replace remote control in the headphone wire,"Expanding Interaction Methods, Enhancing Functionalities of Current Devices","Music Player, Device Control","MP3, music playback, headphones, capacitive touch control, user system interaction, user interface","The Touch Headphones are meant for portable music players and aim to present an improvement to the conventional remote control in the headphone wire, and a solution for controls on wireless in-ear type headphones. Two capacitive touch sensors per earpiece sense when earpieces are being tapped on, and being put in or out.",5,,https://dl.acm.org/doi/abs/10.1145/1085777.1085877,DONE,,,Selection,X,X,X,5,Yes,No,Coupled,Partly Present,Not Indicated,Not Indicated,Not Indicated,Present,35%
[61],Manabe & Fukumoto,2006,Eye-Tracking,In,Gaze,,EOG,,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,"Music Player, Device Control, AR/VR",,,6,,,,,,"Selection, Position",2,coarse (15),absolute,,Yes,No,,,,,,,
[623],Simpson et al.,2008,Mouth,In,Teeth,Click,Accelerometer,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Accessibility, Device Control",,,7,,,,,,Selection,X,X,relative,,Yes,No,,,,,,,
[184],Tamaki et al.,2009,Hand Gestures and Location,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8,,,,,,,,,,,,,,,,,,,
[612],Grübler & Suzuki,2010,Face,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9,,,,,,,,,,,,,,,,,,,
[624],Simpson et al.,2010,Mouth,In,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10,,,,,,,,,,,,,,,,,,,
[253],Gamper et al.,2011,Head Gestures and Pointing,Enabling,Head,Yaw,Microphone,Yes,1,Fine,Yes,Yes,No,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=3),Sitting,Lab,Headphone,Commercial,No,No,"To produce realistic auditory augmentation, knowledge about the head orientation of the user is often necessary. Conventional head tracking systems use e.g. cameras to track visibly distinct markers or inertial sensors to detect head movements.",,"Video Conference, AR/VR, Motion Tracking",N/A,"A head orientation tracking system using binaural headset microphones is proposed. Unlike previous approaches, the proposed method does not require anchor sources, but relies on speech signals of the wearers of the binaural headsets. From the binaural microphone signals, time of arrival (TOA) and time difference of arrival (TDOA) estimates are obtained. The tracking is performed using a particle filter integrated with a maximum likelihood estimation function. In a case study, the proposed method is used to track the head orientations of three conferees in a meeting scenario. With an accuracy of about 10 degrees, the proposed method is shown to outperform a reference method which achieves an accuracy of about 35 degrees.",11,,https://hannesgamper.com/wp-content/papercite-data/pdf/gamper2011b.pdf,DONE,,,,,,,,,,,,,,,,
[23],Manabe & Fukumoto,2011,Ear and Earable,In,Hand,Tap (Earable),Speaker,Yes,2,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=6),Medium (N=6),No,No,No,No,No,Yes (N=6),No,"Sitting, Walking, Walking Stairs, Head Movement, Jumping",Lab,"Earbud, Headphone",Research Prototype,Yes,No,Headphones from input device to input-and-output device,"Expanding Interaction Methods, Enhancing Functionalities of Current Devices","Music Player, Device Control","Headphones, tap, input device, wearable","A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.",12,,https://doi.org/10.1145/2047196.2047236,DONE,,,Selection,X,X,X,2,Yes,No,Coupled,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Present,54%
[85],Matsumura & Fukumoto,2012,Ear and Earable,Enabling,Wearable State,"Share (Earable), Wear (Earable)","Proximity Sensor, EMG",Yes,2,Semantic,Yes,Yes,No,No,High,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,"Earbud, Headphone",Research Prototype,Yes,Yes,Making traditional interfaces more intelligent,"Expanding Interaction Methods, Enhancing Functionalities of Current Devices",Music Player,"Earphones, implicit interaction, intelligent interface","We present universal earphones that use both a proximity sensor and a skin conductance sensor and we demonstrate several implicit interaction techniques they achieve by automatically detecting the context of use. The universal earphones have two main features. The first involves detecting the left and right sides of ears, which provides audio to either ear, and the second involves detecting the shared use of earphones and this provides mixed stereo sound to both earphones. These features not merely free users from having to check the left and right sides of earphones, but they enable them to enjoy sharing stereo audio with other people.",13,,https://dl.acm.org/doi/abs/10.1145/2166966.2167025,DONE,,,,,,,,,,,,,,,,
[62],Sano et al.,2012,Face,In,Facial Expression,Smile,EMG,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Music Player, Communication, Data Annotation",,,14,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[94],Tessendorf and Derleth,2012,Ear and Earable,In,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=21),Yes (N=21),No,Yes (N=21),No,No,"Walking, Walking Stairs, Head Movement, Jumping",Lab,Custom Device,Research Prototype,Yes,Yes,sensing and annotation device to unobtrusively capture head movements in real life situations; In this context the challenge arises to collect and annotate multimodal reference data to identify hearing situations to be improved and to train the multimodal classifier running within the HI.; unobtrusively capture head movements in real life situations; higher accuracy and saving time for the experimenter,,"Data Annotation, Accessibility",N/A,"In this work we present a newly developed earworn sensing and annotation device to unobtrusively capture head movements in real life situations. It has been designed in the context of developing multimodal hearing instruments (HIs), but is not limited to this application domain. The ear-worn device captures triaxial acceleration, rate of turn and magnetic field and features a one-button-approach for real-time data annotation through the user. The system runtime is over 5 hours at a sampling rate of 128 Hz. In a user study with 21 participants the device was perceived as comfortable and showed a robust hold at the ear. On the example of head acceleration data we perform unsupervised clustering to demonstrate the benefit of head movements for multimodal HIs. We believe the novel technology will help to push the boundaries of HI technology.",15,,https://ieeexplore.ieee.org/abstract/document/6346464,DONE,,,Position,X,X,X,1,Yes,No,X,Not Present,High (N=21),Medium (N=21),High (N=21),Not Indicated,55%
[25],Akiyama et al.,2013,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16,,,,,,,,,,,,,,,,,,,
[20],Manabe et al.,2013,Eye-Tracking,In,Gaze,,EOG,,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Music Player,,,17,,,,,,Selection,1,coarse (3),relative,,Yes,No,,,,,,,
[1],Lissermann et al.,2014,Ear and Earable,In,Hand,Touch (Ear),Capacitive Sensor,Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,Yes (N=27),No,No,No,No,No,No,"Sitting, Music",Lab,Custom Device,Research Prototype,Yes,No,decreasing the visual demand of interfaces,Enabling Hands-Free and/or Eyes-Free Control,"Music Player, Device Control, Gaming","Ear-based interaction, ear-worn, mobile interaction, eyes-free, device augmentation, touch, multi-touch","One of the pervasive challenges in mobile interaction is  decreasing the visual demand of interfaces towards eyes-free  interaction. In this paper, we focus on the unique affordances  of the human ear to support one-handed and eyes-free mobile  interaction. We present EarPut, a novel interface concept and  hardware prototype, which unobtrusively augments a variety  of accessories that are worn behind the ear (e.g. headsets  or glasses) to instrument the human ear as an interactive  surface. The contribution of this paper is three-fold. We  contribute (i) results from a controlled experiment with 27  participants, providing empirical evidence that people are  able to target salient regions on their ear effectively and  precisely, (ii) a first, systematically derived design space  for ear-based interaction and (iii) a set of proof of concept  EarPut applications that leverage on the design space and  embrace mobile media navigation, mobile gaming and smart  home interaction.",18,,https://dl.acm.org/doi/10.1145/2686612.2686655,DONE,,,"Selection, Position",X,coarse (2-6),X,7,No,Yes,Coupled,Partly Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,28%
[90],Looney et al.,2014,Brain,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19,,,,,,,,,,,,,,,,,,,
[93],Sahni et al.,2014,Mouth,In,Speech Apparatus,Silent Speech (Words),"Magnetometer, Proximity Sensor",No,11,Semantic,Yes,Yes,No,No,High,High,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"We address the problem of performing silent speech recognition where vocalized audio is not available (e.g. due to a user’s medical condition) or is highly noisy (e.g. during firefighting or combat).; In contrast, silent speech recognition systems [7] , which utilize parts of the human speech production system (rather than the audio produced), can provide significant benefits for individuals rendered incapable of intelligible speech. Such a system can potentially allow issuing voice commands to computing devices or speech generation through text-to-speech interfaces without audible commands generated by the user.",,"Silent Speech, Accessibility, Device Input","silent speech recognition, wearable computing, mobile interfaces","We address the problem of performing silent speech recognition where vocalized audio is not available (e.g. due to a user’s medical condition) or is highly noisy (e.g. during firefighting or combat). We describe our wearable system to capture tongue and jaw movements during silent speech. The system has two components: the Tongue Magnet Interface (TMI), which utilizes the 3-axis magnetometer aboard Google Glass to measure the movement of a small magnet glued to the user’s tongue, and the Outer Ear Interface (OEI), which measures the deformation in the ear canal caused by jaw movements using proximity sensors embedded in a set of earmolds. We collected a data set of 1901 utterances of 11 distinct phrases silently mouthed by six able-bodied participants. Recognition relies on using hidden Markov modelbased techniques to select one of the 11 phrases. We present encouraging results for user dependent recognition.",20,,https://dl.acm.org/doi/10.1145/2634317.2634322,DONE,,,"Selection, Text",X,X,X,11,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,23%
[7],Bedri et al.,2015,Mouth,Enabling,Jaw,General Jaw Movement,Proximity Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,Yes (N=27),No,No,No,No,No,"Sitting, Speaking, Eating, Walking, Walking Stairs",Lab,Earbud,Research Prototype,No,No,Researchers tried to benefit from the features that headphones and earpieces hold to develop systems that will help monitoring human activities.; The motivation behind OEI was the ability to perform silent speech recognition by classifying motion patterns made by the tongue and lower jaw; Socially acceptable.  2. Comfortable of use for extended periods of time.  3. Non-intrusive (no persing or implantation).  4. Ease of donning and disrobing.,,"Silent Speech, Activity Recognition","Outer ear, jaw motion, proximity sensor, silent speech, food intake, jaw gestures","The human ear seems to be a rigid anatomical part with no apparent activity, yet many facial and body activity can be measured from it. Research apparatuses and commercial products have demonstrated the capability of monitoring hart rate, tongue activities, jaw motion and eye blinking from the ear. In this paper we describe the design and the implementation of the Outer Ear Interface (OEI) which utilizes a set of infrared proximity sensors to measure the deformation in the ear canal caused by the lower jaw movement. OEI has been used in different applications that requires tracking of jaw activity which includes silent speech recognition, jaw gesture detection and food intake monitoring.",21,,https://dl.acm.org/doi/10.1145/2800835.2807933,DONE,,,,,,,,,,,,,,,,
[190],Bleichner et al.,2015,Brain,In,Brain Activity,,EEG,,,,,,,,,,,,,,,,,,,,,,,,,,,BCI Applic.,,,22,,,,,,,,,,,,,,,,,,,
[92],Norton et al.,2015,Brain,In,Brain Activity,,EEG,,,,,,,,,,,,,,,,,,,,,,,,,,,BCI Applic.,,,23,,,,,,,,,,,,,,,,,,,
[188],Wang et al.,2015,Brain,In,Brain Activity,,EEG,,,,,,,,,,,,,,,,,,,,,,,,,,,BCI Applic.,,,24,,,,,,,,,,,,,,,,,,,
[19],Weigel et al.,2015,Ear and Earable,Enabling,Hand,Touch (Earable),"Capacitive Sensor, Resistive Sensor",Yes,1,"Semantic, Coarse",No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Custom Device,Research Prototype,Yes,No,"The human skin is recognized as a promising input surface for interactions with mobile and wearable devices. However, it has been difficult to design and implement touch sensors that can be placed directly on the skin. We present iSkin, an input surface for mobile human-computer interaction on the human body.",Expanding Interaction Methods,"Device Input, Music Player","On-body input, Mobile computing, Wearable computing, Touch input, Stretchable Sensor, flexible sensor, Electronic skin","We propose iSkin, a novel class of skin-worn sensors for touch input on the body. iSkin is a very thin sensor overlay, made of biocompatible materials, and is flexible and stretchable. It can be produced in different shapes and sizes to suit various locations of the body such as the finger, forearm, or ear. Integrating capacitive and resistive touch sensing, the sensor is capable of detecting touch input with two levels of pressure, even when stretched by 30% or when bent with a radius of 0.5 cm. Furthermore, iSkin supports single or multiple touch areas of custom shape and arrangement, as well as more complex widgets, such as sliders and click wheels. Recognizing the social importance of skin, we show visual design patterns to customize functional touch sensors and allow for a visually aesthetic appearance. Taken together, these contributions enable new types of on-body devices. This includes finger-worn devices, extensions to conventional wearable devices, and touch input stickers, all fostering direct, quick, and discreet input for mobile computing.",25,,https://dl.acm.org/doi/10.1145/2702123.2702391,DONE,,,"Selection, Position, Path",2,coarse,"absolute, relative",,Yes,Yes,,Present,,,,,
[63],Ashbrook et al.,2016,Mouth,In,Teeth,Click,Bone Conduction Microphone,,,,Yes,Yes,No,,,,Medium (N=20),,,,,,,,,,,,,No,,,,"Device Control, Device Input, Music Player, Phone Calls, Communication",,,26,,,,,,Selection,X,X,X,,Yes,No,,Not Present,,Medium (Derived),,,
[234],Chen et al.,2020,"Ear and Earable, Hand Gestures and Location",Elicitation,Not Applicable,Not Applicable,Not Applicable,Not Indicated,0,Not Applicable,Not Applicable,Not Applicable,Not Indicated,Not Indicated,Not Applicable,Not Applicable,Not Indicated,Not Indicated,Yes (N=27),Yes (N=27),No,No,Yes (N=27),No,No,Sitting,Lab,Not Applicable,Not Applicable,Not Indicated,Not Indicated,"elicitation study, This paper aims to bring more understandings of gesture design.; To better understand how users’ physical- and social-comfort is impacted when interacting with smart devices via ear-based input, we conducted a user elicitation study from which results we compile a set of user-defined gestures.",,"Device Control, Device Input, Music Player, Phone Calls",Gestures; Ear-based Input; User-defined; Guessability; Think-aloud,"The human ear is highly sensitive and accessible, making it especially suitable for being used as an interface for interacting with smart earpieces or augmented glasses. However, previous works on ear-based input mainly address gesture sensing technology and researcher-designed gestures. This paper aims to bring more understandings of gesture design. Thus, for a user elicitation study, we recruited 28 participants, each of whom designed gestures for 31 smart device-related tasks. This resulted in a total of 868 gestures generated. Upon the basis of these gestures, we compiled a taxonomy and concluded the considerations underlying the participants’ designs that also offer insights into their design rationales and preferences. Thereafter, based on these study results, we propose a set of user-defined gestures and share interesting findings. We hope this work can shed some light on not only sensing technologies of ear-based input, but also the interface design of future wearable interfaces.",55,,https://dl.acm.org/doi/10.1145/3427314,DONE,,,,,,,,,,,,,,,,
[34],Laput et al.,2016,Ear and Earable,In,Wearable State,"Remove Earbud, Attach Earbud","Microphone, Speaker",Yes,2,Semantic,No,Yes,Yes,No,Medium,High,High (N=12),High (N=12),No,No,No,No,No,Yes (N=12),No,Music,"Office, University Building",Earbud,Commercial,Yes,No,We propose utilizing these ubiquitous sensors to bring novel sensing abilities to devices without extra or special hardware,,"Music Player, Phone Calls, Device Control","Acoustic sensing, mobile devices, interaction techniques, novel input","Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration. However, adding extra, special purpose sensors increases size, price and build complexity. Instead, we use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos, and conclude with an evaluation that quantifies their performance and demonstrates high accuracy.",27,,https://dl.acm.org/doi/10.1145/2856767.2856812,DONE,,,Selection,X,X,X,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,54%
[69],Merrill et al.,2016,Brain,In,Brain Activity,,EEG,,,,,,,,,,,,,,,,,,,,,,,,,,,BCI Applic.,,,28,,,,,,,,,,,,,,,,,,,
[43],Nguyen et al.,2016,Eye-Tracking,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,29,,,,,,,,,,,,,,,,,,,
[17],Ando et al.,2017,"Head Gestures and Pointing, Face",In,"Head, Jaw","Facial Gesture (Mouth), Pitch, Roll, Yaw",Pressure Sensor,Yes,11,"Semantic, Coarse",Yes,Yes,Yes (Performance Loss),No,Medium,High,High (N=12),Medium (N=6),No,No,No,No,No,Yes (N=12),No,"Sitting, Music",Lab,Earbud,Research Prototype,No,No,"However, to do this, the user usually needs to use at least one hand and look at the smartphone’s screen to take the smartphone out of the pocket and control its music player.  To address this issue, some commercial earphones have adopted sensors to allow users to operate the connected smartphone",,"Music Player, Device Control","Jaw movement, mouth movement, facial movement, head movement, barometer, hands-free, eyes-free, earphones, outer ear interface, wearable computing","We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four Open Mouth levels.",30,,https://dl.acm.org/doi/10.1145/3126594.3126649,DONE,,,"Selection, Orientation, Path",3,X,relative,11,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,36%
[70],Bleichner & Debener,2017,Eye-Tracking,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31,,,,,,,,,,,,,,,,,,,
[206],Dim & Ren,2017,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32,,,,,,,,,,,,,,,,,,,
[22],Kikuchi et al.,2017,Ear and Earable,In,Hand,Pull (Ear),Proximity Sensor,Yes,9,Semantic,No,Yes,Yes,No,Medium,Medium,Low (N=8),Medium (N=8),No,No,No,No,No,Yes (N=8),No,"Sitting, Walking",Lab,Earbud,Research Prototype,Yes,No,"It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an “eyes-free” interface.; exchange an increasing amount of information through their earphones.; a new type of input method is required.",,"Music Player, Device Control, Device Input","Earphone, Skin Deformation, Photo Reflective Sensor","In this paper, we propose EarTouch, a new sensing technology for ear-based input for controlling applications by slightly pulling the ear and detecting the deformation by an enhanced earphone device. It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an “eyes-free” interface. As for the operation of EarTouch, the shape deformation of the ear is measured by optical sensors. Deformation of the skin caused by touching the ear with the fingers is recognized by attaching optical sensors to the earphone and measuring the distance from the earphone to the skin inside the ear. EarTouch supports recognition of multiple gestures by applying a support vector machine (SVM). EarTouch was validated through a set of user studies.",33,,https://dl.acm.org/doi/10.1145/3098279.3098538,DONE,,,"Selection, Path",2,X,relative,9,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Partly Present,32%
[64],Maag et al.,2017,Mouth,In,Tongue,Press,Pressure Sensor,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Music Player, Device Input",,,34,,,,,,Selection,2,coarse (3),relative,,Yes,No,,,,,,,
[12],Matthies et al.,2017,"Face, Head Gestures and Pointing",In,"Facial Expression, Head","Smile, Facial Gesture (Mouth), Facial Gesture (Eye), Pitch, Yaw","EMG, Capacitive Sensor, Electrical Field Sensing",Yes,15,Semantic,Yes,Visual Attention,No,No,Low,Low,Low (N=1),Medium (N=3),No,No,No,No,No,Yes (N=1),No,Sitting,Lab,"Earbud, Custom Device",Research Prototype,No,No,"EarFieldSensing (EarFS) is a novel input method for mobile and wearable computing using facial expressions.; Therefore, we make use of a facial expression control in the manner of microinteractions »...because they may minimize interruption; that is, they allow for a tiny burst of interaction with a device so that the user can quickly return to the task at hand.",,Device Input,"Electric field sensing, body potential sensing, facial expression control, wearable computing, hands-fee, eyes-free","EarFieldSensing (EarFS) is a novel input method for mobile and wearable computing using facial expressions. Facial muscle movements induce both electric field changes and physical deformations, which are detectable with electrodes placed inside the ear canal. The chosen ear-plug form factor is rather unobtrusive and allows for facial gesture recognition while utilizing the close proximity to the face. We collected 25 facial-related gestures and used them to compare the performance levels of several electric sensing technologies (EMG, CS, EFS, EarFS) with varying electrode setups. Our developed wearable fine-tuned electric field sensing employs differential amplification to effectively cancel out environmental noise while still being sensitive towards small facial-movement-related electric field changes and artifacts from ear canal deformations. By comparing a mobile with a stationary scenario, we found that EarFS continues to perform better in a mobile scenario. Quantitative results show EarFS to be capable of detecting a set of 5 facial gestures with a precision of 90% while sitting and 85.2% while walking. We provide detailed instructions to enable replication of our low-cost sensing device. Applying it to different positions of our body will also allow to sense a variety of other gestures and activities.",35,,https://dl.acm.org/doi/10.1145/3025453.3025692,DONE,,,"Selection, Path, Orientation",2,X,relative,15,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,18%
[111],Wang et al.,2017,"Face, Eye-Tracking",In,"Facial Expression, Gaze",,"EOG, EMG",,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Device Control,,,36,,,,,,Selection,2,coarse (4),relative,,Yes,No,,,,,,,
[117],Ahn et al.,2018,Brain,In,Brain Activity,,EEG,,,,,,,,,,,,,,,,,,,,,,,,,,,BCI Applic.,,,37,,,,,,,,,,,,,,,,,,,
[177],Carioli et al.,2018,Mouth,Enabling,Jaw,General Jaw Movement,Piezoelectric Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=4),Sitting,Lab,Earbud,Research Prototype,No,No,"Direct measurement of the earcanal deformation is particularly challenging due to the non-uniform shape of the earcanal, the complex strains exhibited and the variation of earcanal shape and deformation among individuals.; ",,Motion Tracking,"Bending of curved surfaces, earcanal deformation, piezoelectric sensor","The earcanal shape is unique for each human being and temporarily changes when the jaw moves due to eating, chewing, or speaking. The earcanal deformation can be studied by the geometrical analysis of a distorted earpiece custom-fitted inside the earcanal, but the distortion of the earpiece is complex in nature and complicated to analyze. An earcanal bending sensor consisting of a thin piezoelectric strip attached to a customfitted earpiece is presented in this paper. An analytical approach based on computing the geometrical parameters of distorted and undistorted earpieces is developed to: 1) estimate the average bending moment and the resulting stress applied to the customfitted earpiece while opening the jaw and 2) calculate the sensitivity of the piezoelectric earcanal bending sensor. The theoretical model is experimentally validated. The proposed approach can be applied to measure the bending of any curved body in general, and custom-fitted earpieces in particular. It, therefore, enables the designing of versatile in-ear sensors capable of tracking jaw activity and evaluating the energy capacity of earcanal deformation for in-ear energy harvesting purposes.",38,,https://ieeexplore.ieee.org/document/8194832,DONE,,,,,,,,,,,,,,,,
[91],Favre-Felix et al.,2018,Eye-Tracking,In,Gaze,,EOG,,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Accessibility,,,39,,,,,,Selection,1,coarse (3),absolute,,Yes,No,,,,,,,
[296],Rateau et al.,2022,"Ear and Earable, Hand Gestures and Location",Elicitation,Not Applicable,Not Applicable,Not Applicable,Not Indicated,0,Not Applicable,Not Applicable,Not Applicable,Not Indicated,Not Indicated,Not Applicable,Not Applicable,Not Indicated,Not Indicated,Yes (N=20),No,No,No,Yes (N=50),No,No,Sitting,Remote Setting,Earbud,Commercial,Not Indicated,Not Indicated,"The specific example we explore in this paper combines smart earbuds with a proximal smartwatch to support input specifically geared toward smartwatch and smartwatch-earbud interactions.; it becomes necessary to increase the set of gestures to include control of multiple wearable devices.; interactions with multiple smart devices may result in different gestures and the social acceptability of these gestures may vary.; when users are aware of the fact that multiple personal devices can sense input, it may be advisable to adapt gesture design.",,"Phone Calls, Music Player, Device Control, Device Input, Communication","elicitation study, smartwatch, earbuds, wearables","Due to the proliferation of smart wearables, it is now the case that designers can explore novel ways that devices can be used in combination by end-users. In this paper, we explore the gestural input enabled by the combination of smart earbuds coupled with a proximal smartwatch. We identify a consensus set of gestures and a taxonomy of the types of gestures participants create through an elicitation study. In a follow-on study conducted on Amazon’s Mechanical Turk, we explore the social acceptability of gestures enabled by watch+earbud gesture capture. While elicited gestures continue to be simple, discrete, in-context actions, we find that elicited input is frequently abstract, varies in size and duration, and is split almost equally between on-body, proximal, and more distant actions. Together, our results provide guidelines for on-body, near-ear, and in-air input using earbuds and a smartwatch to support gesture capture.",87,,https://dl.acm.org/doi/10.1145/3567710,DONE,,,,,,,,,,,,,,,,
[31],Huang et al.,2018,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,40,,,,,,,,,,,,,,,,,,,
[18],Lee et al.,2018,Ear and Earable,In,Hand,"Tap (Ear), Lift (Ear), Dwell (Ear), Joystick (Ear), Drag (Ear), Toggle (Ear)",Camera,Yes,6,"Semantic, Coarse",No,Yes,No,No,Medium,Medium,N/A,N/A,Yes (N=20),No,No,Yes (N=32),Yes (N=32),No,No,Sitting,"Café, Lab",Custom Device,Research Prototype,Yes,No,"However, while aspects such as tracking fidelity, display quality and computing power have advanced considerably to produce today’s high-end products, input and interaction technologies are less mature.; Recognizing the need for input systems for AR glasses that leave the hands unencumbered; We argue there is a need to improve our understanding of how users conceive of touches to the face as an input modality [31] to better inform future design and development efforts. Specifically, we argue that a key omission in our current understanding relates to the social acceptability [29] of facial touches - how comfortable users feel performing or observing this type of input in real life situations.",,"Device Input, AR/VR, Device Control","Hand-to-Face Input, Social Acceptability, User Elicitation, Augmented Reality, Head Mounted Display","Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable handto-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",41,,https://dl.acm.org/doi/10.1145/3242587.3242642,DONE,,,"Selection, Position",1,"coarse (6), coarse (3), coarse (5)",absolute,~33,Yes,Yes,Coupled,Present (N=20),Not Indicated,Low (Derived),Not Indicated,Not Indicated,28%
[610],Min et al. ,2018,Head Gestures and Pointing,In,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,No,No,Medium,Medium,Low(N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"In this paper, we explore audio and kinetic sensing on earable devices with the commercial on-the-shelf form factor.",,"Health, Communication, Data Annotation, Emotion Recognition, Activity Recognition","Earable, Earbud, Audio sensing, Kinetic sensing","In this paper, we explore audio and kinetic sensing on earable devices with the commercial on-the-shelf form factor. For the study, we prototyped earbud devices with a 6-axis inertial measurement unit and a microphone. We systematically investigate the differential characteristics of the audio and inertial signals to assess their feasibility in human activity recognition. Our results demonstrate that earable devices have a superior signal-to-noise ratio under the influence of motion artefacts and are less susceptible to acoustic environment noise. We then present a set of activity primitives and corresponding signal processing pipelines to showcase the capabilities of earbud devices in converting accelerometer, gyroscope, and audio signals into the targeted human activities with a mean accuracy reaching up to 88% in varying environmental conditions.",42,,https://dl.acm.org/doi/10.1145/3211960.3211970,DONE,,,"Selection, Path, Orientation",2,X,relative,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,22%
[65],Nguyen et al.,2018,Mouth,In,"Tongue, Teeth","Press, Grit","Capacitive Sensor, EEG, EMG",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Authentification, Accessibility, Privacy, Device Control",,,43,,,,,,Selection,3,coarse (11),absolute,,Yes,No,,,,,,,
[60],Taniguchi et al.,2018,Mouth,In,Tongue,Press,Proximity Sensor,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Music Player,,,44,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[8],Amesaka et al.,2019,"Face, Head Gestures and Pointing, Mouth",In,"Facial Expression, Head","Facial Gesture (Mouth), Roll","Microphone, Speaker",Yes,5,Semantic,Yes,Yes,No,No,Medium,High,Medium (N=11),N/A,No,No,No,No,No,Yes (N=11),No,Sitting,Lab,Earbud,Research Prototype,No,No,"propose a new input method for mobile and wearable computing using facial expressions.; An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes.; Considering wearable computing environments, an advanced handsfree input method is required because our hands may be occupied in some situations such as cooking, walking with luggage, among other things. Speech recognition systems can be used as a handsfree input method; however, speaking in quiet public places is difficult, and audible sounds can be easily contaminated by environmental noise.",,"Device Control, Device Input, Music Player","facial expression recognition, ultrasound, ear canal transfer function, hearables","In this study, we propose a new input method for mobile and wearable computing using facial expressions. Facial muscle movements induce physical deformation in the ear canal. Our system utilizes such characteristics and estimates facial expressions using the ear canal transfer function (ECTF). Herein, a user puts on earphones with an equipped microphone that can record an internal sound of the ear canal. The system transmits ultrasonic band-limited swept sine signals and acquires the ECTF by analyzing the response. An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes. We investigated the performance of our proposed method for 21 facial expressions with 11 participants. Moreover, we proposed a signal correction method that reduces positional errors caused by attaching/detaching the device. The evaluation results confirmed that the f-score was 40.2% for the uncorrected signal method and 62.5% for the corrected signal method. We also investigated the practical performance of six facial expressions and confirmed that the f-score was 74.4% for the uncorrected signal method and 90.0% for the corrected signal method. We found the ECTF can be used for recognizing facial expressions with high accuracy equivalent to other related work.",45,,https://dl.acm.org/doi/10.1145/3341163.3347747,DONE,,,"Selection, Orientation, Path",2,X,relative,20,Yes,No,X,Present (N=11),Not Indicated,Medium (Derived),Not Indicated,Present,31%
[80],Ferlini et al.,2019,Head Gestures and Pointing,Enabling,Head,Yaw,"Accelerometer, Gyroscope, Magnetometer",Yes,1,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=10),"Eating, Speaking, Standing",Lab,Earbud,Commercial,Yes,No,"Earables, like the eSense platform we evaluate in this paper, have an enormous and mostly unexploited potential. For instance, if adopted as hearing-aids, earables could be used both as sensors and actuators.; In this work, we focus on the evaluation of the eSense platform [7, 12] in tracking the head movements of a user concentrating on a specific spatial point.; By tracking instantaneous head movements as a proxy to track visual attention, our study shows how a system, that relies only on accelerometer and gyroscope, can still provide useful insights on where a person is facing.; his paper lays the foundations of a line of work aiming to sense and characterize human attention through earables, wearables that are neither socially-awkward, nor cumbersome, unusable or unrealistic (e.g. combining an hearing-aid with a pair of eye-tracking glasses). Lastly, it sheds light on how a magnetometer would behave if placed in an earable.",,"Accessibility, Motion Tracking","Earables, Head Motion Tracking, Visual Attention","Head tracking is a fundamental component in visual attention detection, which, in turn, can improve the state of the art of hearing aid devices. A multitude of wearable devices for the ear (so called earables) exist. Current devices lack a magnetometer which, as we will show, represents a big challenge when one tries to use them for accurate head tracking. In this work we evaluate the performance of eSense, a representative earable device, to track head rotations. By leveraging two different streams (one per earbud) of inertial data (from the accelerometer and the gyroscope), we achieve an accuracy up to a few degrees. We further investigate the interference generated by a magnetometer in an earable to understand the barriers to its use in these types of devices.",46,,https://dl.acm.org/doi/10.1145/3345615.3361131,DONE,,,,,,,,,,,,,,,,
[77],Hoelzemann et al.,2019,Ear and Earable,In,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=7),No,No,No,No,No,"Sitting, Walking, Walking Stairs, Sports","Lab, Sports Site",Earbud,Commercial,Yes,No,"Wearable activity recognition research needs benchmark data, which rely heavily on synchronizing and annotating the inertial sensor data, in order to validate the activity classifiers. Such validation studies become challenging when recording outside the lab, over longer stretches of time.; It would allow the users to annotate their data without much effort in a socially comfortable way, which also enables ’in the wild’ experiments as study volunteers annotate activities in their daily lives.",,Data Annotation,N/A,"Wearable activity recognition research needs benchmark data, which rely heavily on synchronizing and annotating the inertial sensor data, in order to validate the activity classifiers. Such validation studies become challenging when recording outside the lab, over longer stretches of time. This paper presents a method that uses an inconspicuous, earworn device that allows the wearer to annotate his or her activities as the recording takes place. Since the ear-worn device has integrated inertial sensors, we use cross-correlation over all wearable inertial signals to propagate the annotations over all sensor streams. In a feasibility study with 7 participants performing 6 different physical activities, we show that our algorithm is able to synchronize signals between sensors worn on the body using cross-correlation, typically within a second. A comfort rating scale study has shown that attachment is critical. Button presses can thus define markers in synchronized activity data, resulting in a fast, comfortable, and reliable annotation method.",47,,https://dl.acm.org/doi/10.1145/3345615.3361136,DONE,,,Position,X,X,X,1,Yes,No,X,Not Present,Medium (N=7),High (Derived),Not Indicated,Not Indicated,31%
[106],Lee et al.,2019,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,48,,,,,,,,,,,,,,,,,,,
[275],Shimon et al.,2024,"Ear and Earable, Hand Gestures and Location",Elicitation,Not Applicable,Not Applicable,Not Applicable,Not Indicated,0,Not Applicable,Not Applicable,Not Applicable,Not Indicated,Not Indicated,Not Applicable,Not Applicable,Not Indicated,Not Indicated,Yes (N=18),No,No,No,No,No,No,Sitting,Lab,Not Applicable,Not Applicable,Not Indicated,Not Indicated,"Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration.; However, the small form factor and input area constraint in wireless earbuds restrict physical touch interaction to simple on-device taps and swipes, thus limiting their input space.; Although prior literature explored off-device uni-manual gesture reuse across different segmented regions for outer-ear touches to support gesture reusability, similar exploration is lacking for around-ear on-skin space (i.e., face, head, and neck) and above-ear mid-air spaces for supporting similar gestures.; We address this research gap by studying the reuse of different gesture classes at different mid-air and onskin gesture regions and the effect of increasing the number of gesture regions on quantitative and qualitative gesture performance metrics for mid-air and on-skin interaction spaces.; Understanding of end-user preference shift from one interaction space to another for increasing gesture reuse; Analysis of the effects of different factors (i.e., the number of gesture regions and associated boundaries) on interaction space segmentation for uni-manual, off-device earable gestures.",,Device Control,"Embodied Interaction, Input Techniques, Uni-manual Interaction, Touch Surfaces, Earbased Interaction, Earables","Small form factor limits physical input space in earable (i.e., ear-mounted wearable) devices. Off-device earable inputs in alternate mid-air and on-skin around-ear interaction spaces using uni-manual gestures can address this input space limitation. Segmenting these alternate interaction spaces to create multiple gesture regions for reusing off-device gestures can expand earable input vocabulary by a large margin. Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration. We collected and analyzed 7560 uni-manual gesture motion data from 18 participants to explore earable gesture reuse by segmentation of on-skin and mid-air spaces around the ear. Our results show that gesture performance degrades significantly beyond 3 mid-air and 5 on-skin around-ear gesture regions for different uni-manual gesture classes (e.g., swipe, pinch, tap). We also present qualitative findings on most and least preferred regions (and associated boundaries) by end-users for different uni-manual gesture shapes across both interaction spaces for earable devices. Our results complement earlier elicitation studies and interaction technologies for earables to help expand the gestural input vocabulary and potentially drive future commercialization of such devices.",112,,https://dl.acm.org/doi/10.1145/3643513,DONE,,,,,,,,,,,,,,,,
[14],Lee et al.,2019,Face,In,Facial Expression,"Smile, Facial Gesture (Other)","Accelerometer, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Music Player, Phone Calls",,,49,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[2],Nasser et al.,2019,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,50,,,,,,,,,,,,,,,,,,,
[78],Odoemelem et al.,2019,Head Gestures and Pointing,In,Head,"Roll, Pitch","Accelerometer, Gyroscope",Yes,2,Fine,Yes,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Commercial,Yes,No,"We present an especially minimal and low-cost solution that uses the eSense [1] ear-worn prototype as a small head-worn controller, enabling direct control of an inexpensive robot arm in the environment.; In contrast, we present here a human-robot interface that aims at being minimal in terms of size and costs, and intend to investigate the trade-offs that are caused by this minimalism in terms of accuracy and speed.",,"Device Control, Accessibility, Motion Tracking",N/A,"Head motion-based interfaces for controlling robot arms in real time have been presented in both medical-oriented research as well as human-robot interaction. We present an especially minimal and low-cost solution that uses the eSense [1] ear-worn prototype as a small head-worn controller, enabling direct control of an inexpensive robot arm in the environment. We report on the hardware and software setup, as well as the experiment design and early results.",51,,https://dl.acm.org/doi/10.1145/3345615.3361138,DONE,,,"Path, Orientation, Selection",2,fine,absolute,2,No,Yes,Coupled,Not Present,Not Indicated,High (Derived),Not Indicated,Present,38%
[15],Shirota et al.,2019,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,52,,,,,,,,,,,,,,,,,,,
[620],Vega Gálvez et al.,2019,Mouth,In,Teeth,Click,"Accelerometer, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Device Control,,,53,,,,,,"Selection, Position",X,X,absolute,,Yes,No,,,,,,,
[27],Yan et al.,2019,Hand Gestures and Location,In,Hand,Cover (Face),Microphone,Yes,1,Semantic,No,Yes,No,No,Medium,Medium,High (N=12),Medium (N=12),No,Yes (N=12),Yes (N=12),Yes (N=17),Yes (N=17),Yes (N=12),No,Sitting,"Lab, Office",Earbud,Commercial,Yes,No,"introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking.;  However, there are two major challenges with voice input [32]. First, users worry about the privacy risks of disclosing their personal information while speaking. Second, they suffer the inconvenience of repeatedly speaking the wake-up word or pressing a button during multiple rounds of voice input. With PrivateTalk we can address these two issues simultane ously. With PrivateTalk, a user can perform voice input by covering the mouth on one side with a hand while speaking (the Hand-On-Mouth gesture) as shown in Figure 1.",,"AR/VR, Privacy, Device Input","Voice input, hand gesture","We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is per formed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the differ ence of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.",54,,https://dl.acm.org/doi/10.1145/3332165.3347950,DONE,,,Position,X,X,X,1,Yes,No,Additional,Present,High (N=12),High (Derived),High (N=12),Not Indicated,65%
[223],Cao et al.,2020,Ear and Earable,Enabling,Wearable State,Wear (Earable),"Microphone, Speaker",Partly,1,Fine,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,Yes (N=5),"Distance, Obstacles, Walking",Lab,Earbud,Commercial,Yes,No,new modality of acoustic motion tracking using earphones; We believe this is an important step towards “earable” sensing. include earphones into the ecosystem of acoustic motion tracking.,,"Motion Tracking, AR/VR, Device Control, Device Input","Earable sensing, Earphone-based acoustic sensing, Motion tracking","Acoustic motion tracking is an exciting new research area with promising progress in the last few years. Due to the inherent low propagation speed in the air, acoustic signals have the unique advantage of fine sensing granularity compared to RF signals. Speakers and microphones nowadays are pervasively available in devices surrounding us, such as smartphones and voice-controlled smart speakers. Though promising, one fundamental issue hindering the adoption of acoustic-based motion tracking is that the positions of microphones and speakers inside a device are fixed, which greatly limits the flexibility of acoustic motion tracking. In this work, we propose a new modality of acoustic motion tracking using earphones. Earphone-based tracking mitigates the constraints associated with traditional smartphone-based tracking. With novel designs and comprehensive experiments, we show earphone-based motion tracking can achieve a great flexibility and a high accuracy at the same time. We believe this is an important step towards “earable” sensing.",55,,https://dl.acm.org/doi/10.1145/3384419.3430730,DONE,,,,,,,,,,,,,,,,
[212],Chen et al.,2020,Face,In,Facial Expression,"Smile, Facial Gesture (Eye), Facial Gesture (Mouth), Facial Gesture (Other)",Camera,,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,"Silent Speech, Emotion Recognition, Communication, Feedback System, Customer Analytics",,,56,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[21],Choi & Kim,2020,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,57,,,,,,,,,,,,,,,,,,,
[149],Kaveh et al.,2020,Eye-Tracking,In,Brain Activity,,EEG,,,,,,,,,,,,,,,,,,,,,,,,,,,BCI Applic.,,,58,,,,,,Selection,,,,,,,,,,,,,
[249],Pfreundtner et al.,2020,Head Gestures and Pointing,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,59,,,,,,,,,,,,,,,,,,,
[16],Prakash et al.,2020,Mouth,In,Teeth,"Click, Slide",Speaker,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Device Input, Accessibility, Authentification, Device Control, Health",,,60,,,,,,Selection,2,coarse (2x3),absolute,,Yes,No,,,,,,,
[26],Xu et al.,2020,"Ear and Earable, Hand Gestures and Location",In,Hand,"Tap (Face), Tap (Ear), Slide (Face), Slide (Ear)",Microphone,Yes,8,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=18),Medium (N=18),Yes (N=16),Yes (N=12),Yes (N=16),No,Yes (N=16),Yes (N=18),No,"Noise, Sitting",Lab,Earbud,Commercial,Yes,No,"This observation gives rise to EarBuddy, a novel eyes-free input system that detects gestures performed along users’ faces using wireless earbuds.; We propose EarBuddy, a novel eyes-free input technique supported by wireless earbuds without the need for hardware modification; gesture set for EarBuddy that is optimized for user preference and microphone detectability.",,"Music Player, Phone Calls, Communication, Device Input, AR/VR","Wireless earbuds, face and ear interaction, gesture recognition","Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.",61,,https://dl.acm.org/doi/10.1145/3313831.3376836,DONE,,,"Selection, Position, Path",1,X,relative,27,Yes,No,X,Present (N=16),High (N=12),High (Derived),High (N=16),Present,83%
[611],Yang et al.,2020,Head Gestures and Pointing,In,Head,"Roll, Pitch, Yaw","Accelerometer, Gyroscope",No,1,Fine,Yes,Visual Attention,No,No,Medium,High,Medium (N=7),N/A,No,No,No,No,No,Yes (N=7),Yes (N=7),"Standing, Walking",University Building,Headphone,Commercial,Yes,No,This paper aims to use modern earphones as a platform for acoustic augmented reality (AAR).,,"AR/VR, Activity Recognition","Augmented Reality, Acoustics, Smart Earphones, Wearable Computing, Indoor Localization, Inertial Measurement Unit (IMU), Dead Reckoning, Motion Tracking, Sensor Fusion, Head Related Transfer Function (HRTF), Spatial Audio","This paper aims to use modern earphones as a platform for acoustic augmented reality (AAR). We intend to play 3D audio-annotations in the user’s ears as she moves and looks at AAR objects in the environment. While companies like Bose and Microsoft are beginning to release such capabilities, they are intended for outdoor environments. Our system aims to explore the challenges indoors, without requiring any infrastructure deployment. Our core idea is two-fold. (1) We jointly use the inertial sensors (IMUs) in earphones and smartphones to estimate a user’s indoor location and gazing orientation. (2) We play 3D sounds in the earphones and exploit the human’s responses to (re)calibrate errors in location and orientation. We believe this fusion of IMU and acoustics is novel, and could be an important step towards indoor AAR. Our system, Ear-AR, is tested on 7 volunteers invited to an AAR exhibition – like a museum – that we set up in our building’s lobby and lab. Across 60 different test sessions, the volunteers browsed different subsets of 24 annotated objects as they walked around. Results show that Ear-AR plays the correct audio-annotations with good accuracy. The user-feedback is encouraging and points to further areas of research and applications.",62,,https://dl.acm.org/doi/10.1145/3372224.3419213,DONE,,,"Selection, Orientation",3,fine,absolute,1,No,Yes,Coupled,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Not Indicated,25%
[229],Fan et al.,2021,Ear and Earable,In,"Hand, Wearable State","Slide (Earable), Tap (Earable)",Pressure Sensor,Yes,2,Semantic,No,Yes,No,No,Medium,High,High (N=1),Medium (N=1),No,No,No,No,No,Yes (N=1),No,"Music, Sitting",Lab,Custom Device,Research Prototype,No,No,bringing intelligence to headphones.; We envision HeadFi can serve as a vital supplementary solution to existing smart headphone design by directly transforming large amounts of existing “dumb” headphones into intelligent ones.,,"Music Player, Device Control","Earable Computing, Wearable Devices, User Identification, Heartrate Monitoring, Touch Gesture Control, Voice Communication","Headphones continue to become more intelligent as new functions (e.g., touch-based gesture control) appear. These functions usually rely on auxiliary sensors (e.g., accelerometer and gyroscope) that are available in smart headphones. However, for those headphones that do not have such sensors, supporting these functions becomes a daunting task. This paper presents HeadFi, a new design paradigm for bringing intelligence to headphones. Instead of adding auxiliary sensors into headphones, HeadFi turns the pair of drivers that are readily available inside all headphones into a versatile sensor to enable new applications spanning across mobile health, user-interface, and context-awareness. HeadFi works as a plug-in peripheral connecting the headphones and the pairing device (e.g., a smartphone). The simplicity (can be as simple as only two resistors) and small form factor of this design lend itself to be embedded into the pairing device as an integrated circuit. We envision HeadFi can serve as a vital supplementary solution to existing smart headphone design by directly transforming large amounts of existing “dumb” headphones into intelligent ones. We prototype HeadFi on PCB and conduct extensive experiments with 53 volunteers using 54 pairs of non-smart headphones under the institutional review board (IRB) protocols. The results show that HeadFi can achieve 97.2%–99.5% accuracy on user identification, 96.8%–99.2% accuracy on heart rate monitoring, and 97.7%–99.3% accuracy on gesture recognition.",63,,https://dl.acm.org/doi/10.1145/3447993.3448624,DONE,,,"Selection, Path",1,X,relative,2,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,38%
[247],Ferlini et al.,2021,Head Gestures and Pointing,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64,,,,,,,,,,,,,,,,,,,
[258],Gashi et al.,2021,"Head Gestures and Pointing, Face",In,"Head, Facial Expression","Pitch, Yaw, Smile, Facial Gesture (Mouth)","Accelerometer, Gyroscope",Yes,4,Semantic,Yes,Yes,Yes,No,Low,Medium,Low (N=21),N/A,No,No,No,No,No,Yes (N=21),No,Sitting,Lab,Earbud,Commercial,No,No,"Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions.",,"Social Interaction, Feedback System, Activity Recognition, Video Conference","Earable Computing, Transfer learning, Head Gestures Detection, Facial Expressions Recognition, Hierarchical Classification","Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions. Several researchers have thus tackled the problem of automatically recognizing head gestures and facial expressions, mainly leveraging video data. In this paper, we instead consider inertial signals collected from unobtrusive, earmounted devices. We focus on typical activities performed during social interactions – head shaking, nodding, smiling, talking and yawning – and propose a hierarchical classification approach to discriminate them from each other. Further, we investigate whether the transfer of knowledge learned from publicly available datasets leads to further performance improvements. Our results show that the combined use of our hierarchical approach and transfer learning allows the classifier to discriminate head and mouth activities with an F1 score of 84.79, smile, talk and yawn with an F1 score of 45.42, and nodding and head shaking with an F1 score of 88.24, outperforming shallow classifiers by 2-9 percentage points.",65,,https://dl.acm.org/doi/10.1145/3462244.3479921,DONE,,,"Path, Orientation, Selection",2,X,relative,4,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,23%
[217],Hashem et al.,2021,Head Gestures and Pointing,In,Head,Yaw,Bluetooth,No,1,Coarse,Yes,Visual Attention,No,No,Medium,High,High (N=1),Low (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Standing",Lab,Earbud,Commercial,Yes,No,"By harnessing this information, it is possible to lock on that device in a multi-device environment and allow only it to respond to user gestures without ambiguity.; Head orientation detection in a multi-device environment can help indicate which device a user is interacting with, and thus provide a seamless way of mapping user gestures to the intended device without riddling other devices with unintended gestures.; In this paper, we present Look&Lock: a ubiquitous and accurate head orientation tracking system for multi-device environments. Look&Lock leverages Bluetooth Low Energy (BLE) capabilities of earables and their interaction with other BLE-enabled devices to track head orientation and map commands to the intended device.; With the rate of technological advancements, we anticipate that earables will be more compact and present future opportunities to be leveraged beyond audio use.",,Device Control,"IoT, BLE, smart earphones, wearables, earables, earable computing, head orientation","We present Look&Lock: a novel ubiquitous system that utilizes BLE communication between smart appliances in the environment and earables worn by the user to track head orientation and determine which device she is looking at. By harnessing this information, it is possible to lock on that device in a multi-device environment and allow only it to respond to user gestures without ambiguity. Our system leverages commercial off-the-shelf earables to provide accurate training-free head orientation tracking that is robust in different room settings. We implement a prototype using Android phones and eSense, a commercially available multi-sensory personal earable device. Our evaluation shows that Look&Lock can correctly identify devices inside a user’s field of view with accuracy up to 100% and is robust to different configurations and room settings.",66,,https://dl.acm.org/doi/10.1145/3446382.3448653,DONE,,,"Selection, Orientation",1,coarse (4),absolute,1,No,Yes,Coupled,Partly Present,Not Indicated,High (Derived),Not Indicated,Not Indicated,31%
[622],Islam et al.,2021,Head Gestures and Pointing,In,Head,"Pitch, Yaw","Accelerometer, Gyroscope",No,2,Semantic,Yes,Yes,N/A,No,Medium,Medium,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Earbud,Commercial,No,No,"Detecting head- and mouth-related human activities of elderly people are very important for nurse care centers. They need to track different types of activities of elderly people like swallowing, eating, etc., to measure the health status of elderly people. In this regard, earable devices open up interesting possibilities for monitoring personal-scale behavioral activities.; The objectives of this paper are as follows: collecting accelerometer and gyroscope data from eSense device through Bluetooth and mobile application; detecting headand mouth-related human activities alongside some other regular activities; using traditional machine learning and deep learning classifiers to detect activities and compare performances",,"Accessibility, Health, Activity Recognition","Health care, Earables, Wearable; Activity recognition, eSense","Detecting head- and mouth-related human activities of elderly people are very important for nurse care centers. They need to track different types of activities of elderly people like swallowing, eating, etc., to measure the health status of elderly people. In this regard, earable devices open up interesting possibilities for monitoring personal-scale behavioral activities. Here, we introduce activity recognition based on an earable device called ‘eSense’. It has multiple sensors that can be used for human activity recognition. ‘eSense’ has a 6-axis inertial measurement unit with a microphone and Bluetooth. In this paper, we propose an activity recognition framework using eSense device. We collect accelerometer and gyroscope sensor data from eSense device to detect head- and mouth-related activities along with other normal human activities. We evaluated the classification performance of the classifier using both accelerometer and gyroscope data. For this work, we develop a smartphone application for data collection from the eSense. Several statistical features are exploited to recognize head- and mouth-related activities (e.g., head nodding, head shaking, eating, and speaking), and regular activities (e.g., stay, walk, and speaking while walking). We explored different types of machine learning approaches like Convolutional Neural Network (CNN), Random Forest (RnF), K-Nearest Neighbor (KNN), Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), etc., for classifying activities. We have achieved satisfactory results. Our results show that using both accelerometer and gyroscope sensors can improve performance. We achieve accuracy of 80.45% by LDA, 93.34% by SVM, 91.92% by RnF, 91.64% by KNN, and 93.76% by CNN while we exploit both accelerometer and gyroscope sensor data together. The results demonstrate the prospect of eSense device for detecting human activities in various healthcare monitoring system.",67,,https://link.springer.com/chapter/10.1007/978-981-15-8944-7_11,DONE,,,"Selection, Path, Orientation",2,X,relative,2,Yes,No,X,Partly Present,Not Indicated,High (Derived),Not Indicated,Present,26%
[226],Jin et al.,2021,Hand Gestures and Location,In,Hand,"Sign Language (Words), Sign Language (Sentences), Close (Mid-Air), Tap (Mid-Air)","Microphone, Speaker",Yes,74,Semantic,No,Yes,No,No,Low,High,Medium (N=8),High (N=2),No,No,No,No,No,Yes (N=8),No,"Standing, Distance, Orientation","Office, University Building, Outdoors",Headphone,Research Prototype,Yes,No,"We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures.; Many Deaf people live successful and productive lives using sign languages. However, it is still quite challenging for the Deaf group to communicate with the hearing population conveniently.",,Accessibility,"Acoustic sensing, sign language gesture recognition, earphones","We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians.",68,,https://dl.acm.org/doi/10.1145/3463519,DONE,,,"Selection, Path, Text, Orientation",6,X,relative,74,Yes,No,Coupled,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Present,47%
[618],Kakaraparthi et al.,2021,Hand Gestures and Location,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,69,,,,,,,,,,,,,,,,,,,
[213],Khanna et al.,2021,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),Accelerometer,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Privacy,,,70,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[224],Laporte et al.,2021,Head Gestures and Pointing,In,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,Yes,No,Medium,Medium,Low(N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Verbal and non-verbal activities convey insightful information about people’s affect, empathy, and engagement during social interactions. In this paper, we investigate the usage of inertial sensors to recognize verbal (e.g., speaking), non-verbal (e.g., head nodding, shaking) and other activities (e.g., eating, no movement).; Our motivation for detecting verbal and non-verbal activities is rooted in our work on building human memory augmentation systems. Using earable computing, we attempt to recognize different types of human activities, in particular head gestures, with the purpose of detecting when a social interaction is taking place. This is because the presence of others and our interactions with them play important roles in our memories, both during the formation of memory [32] and at retrieval time [6]: moments of social interactions might be easier to remember (formation time), and remembering a particular interaction might also help to remind us of particular details (retrieval time)",,"Social Interaction, Data Annotation","Datasets, Earable Computing, Head Gestures Recognition, Memory Recall","Verbal and non-verbal activities convey insightful information about people’s affect, empathy, and engagement during social interactions. In this paper, we investigate the usage of inertial sensors to recognize verbal (e.g., speaking), non-verbal (e.g., head nodding, shaking) and other activities (e.g., eating, no movement). We implement an end-to-end deep neural network to distinguish among these activities. We then explore the generalizability of the approach in three scenarios: (1) using new data to detect a known activity from a known user, (2) detecting a novel activity of a known user and (3) detecting the activity of an unknown user. Results show that using accelerometer and gyroscope sensors, the model achieves a balanced accuracy of 55% when tested on data from a new user, 41% on a new activity of an existing user, and 80% on new data of a known activity from an existing user. The results are between 7-47 percentage points higher than baseline classifiers.",71,,https://dl.acm.org/doi/10.1145/3460418.3479322,DONE,,,"Path, Selection, Orientation",2,X,relative,2,Yes,No,X,Partly Present,Not Indicated,High (Derived),Not Indicated,Present,26%
[626],Ma et al.,2021,Hand Gestures and Location,In,Hand,Tap (Face),Microphone,Yes,5,Semantic,No,Yes,No,Yes,Medium,Medium,High (N=29),High (N=4),No,No,No,No,No,Yes (N=29),No,"Sitting, Music","Lab, Outdoors",Earbud,Research Prototype,Yes,No,"However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions.; Thus, in the interest of form factor and cost, we explore other alternatives for human motion sensing on earables. To achieve reliable detection of both intense and light human motions, we present OESense, a novel acoustic-based in-ear human motion sensing system.; With this work, we aim at developing a general earable sensing system for human motion detection.",,"Health, Activity Recognition, Motion Tracking",N/A,"Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3% step counting recall, 98.3% recognition recall for 5 activities, and 97.0% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds’ fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.",72,,https://dl.acm.org/doi/10.1145/3458864.3467680,DONE,,,"Selection, Position",X,X,X,12,Yes,No,X,Partly Present (N=29),Not Indicated,Low (Derived),Not Indicated,Present,52%
[259],Nasser et al.,2021,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,73,,,,,,,,,,,,,,,,,,,
[628],Peng,2021,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,74,,,,,,,,,,,,,,,,,,,
[227],Röddiger et al.,2021,Ear and Earable,In,Tensor Tympani,Contract,Pressure Sensor,Yes,3,Semantic,Yes,Yes,Yes,No,High,High,High (N=16),Medium (N=16),No,Yes (N=8),Yes (N=16),No,No,Yes (N=16),No,"Sitting, Music",Lab,Earbud,Research Prototype,Yes,No,Input techniques using “subtle” or “motionless” input gestures are desirable in mobile contexts because they take into consideration the social context of mobile device usage; Input techniques with little to no movement avoid the inconvenience of techniques requiring large physical efort and are more socially acceptable to spectators,,"Music Player, Phone Calls","tensor tympani muscle, discreet interaction, subtle gestures, earables, hearables, in-ear barometry","We explore how discreet input can be provided using the tensor tympani - a small muscle in the middle ear that some people can voluntarily contract to induce a dull rumbling sound. We investigate the prevalence and ability to control the muscle through an online questionnaire (N=192) in which 43.2% of respondents reported the ability to “ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can be used to detect voluntary tensor tympani contraction in the sealed ear canal. This data was used to train a classifer based on three simple ear rumble “gestures” which achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction, grounded in three manual, dual-task application scenarios (N=8). This highlights the applicability of EarRumble as a low-efort and discreet eyes- and hands-free interaction technique that users found “magical” and “almost telepathic”.",75,,https://dl.acm.org/doi/10.1145/3411764.3445205,DONE,,,Selection,X,X,X,4,Yes,No,X,Present (N=83),High (N=8),Low (Derived),High (N=99),Present,73%
[221],Sun et al.,2021,Mouth,In,Teeth,Click,"Accelerometer, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Music Player,,,76,,,,,,Selection,3,coarse (13),absolute,,Yes,No,,,,,,,
[231],Verma et al.,2021,Face,In,Facial Expression,"Facial Gesture (Eye), Facial Gesture (Nose), Facial Gesture (Mouth), Facial Gesture (Other)","Accelerometer, Gyroscope",,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,"AR/VR, Feedback System, Emotion Recognition, Video Conference",,,77,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[616],Wu et al.,2021,Face,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,78,,,,,,,,,,,,,,,,,,,
[608],Alkiek et al.,2022,Hand Gestures and Location,In,Hand,"Approach (Mid-Air), Slide (Mid-Air), Recede (Mid-Air)",Bluetooth,Yes,8,Semantic,No,Yes,Yes,No,Low,Low,High (N=1),High (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Orientation, Sitting, Standing","Office, Living Room",Earbud,Research Prototype,Yes,No,"However, the limited interface of these devices, usually being a single button or force-sensor, inhibits their potential. Earables can provide a much richer experience by extending the ways in which users can interact with them. In this paper, we present EarGest, a novel earable-based hand gesture recognition system that does not require calibration or training, and works with commercially available BLE-enabled earphones and devices.; Our proposed system is unique in its ability to leverage Bluetooth to detect hand motion near the ear to recognize gestures. By harnessing information from BLE connections between the wireless earphones and a host device, we accurately detect and classify different hand gestures performed by users, while also determining discrete levels of hand speed.",,"Device Control, Music Player, Phone Calls","Earables, gesture recognition, HCI; sensing","Earables have been increasingly gaining attention from consumers and manufacturers alike due to their small footprint, ease of use, and the added accessibility they bring. However, the limited interface of these devices, usually being a single button or force-sensor, inhibits their potential. Earables can provide a much richer experience by extending the ways in which users can interact with them. In this paper, we present EarGest, a novel earable-based hand gesture recognition system that does not require calibration or training, and works with commercially available BLE-enabled earphones and devices. Our proposed system is unique in its ability to leverage Bluetooth to detect hand motion near the ear to recognize gestures. By harnessing information from BLE connections between the wireless earphones and a host device, we accurately detect and classify different hand gestures performed by users, while also determining discrete levels of hand speed. EarGest operates without interfering with the regular functionality of the earphones and introduces minimal energy overhead on the host device. We implement a prototype of the system using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and settings. Results show that our system can detect and classify seven near-ear hand gestures with an accuracy up to 98.5%, as well as identify hand motion speed with 96% accuracy.",79,,https://ieeexplore.ieee.org/abstract/document/9918622,DONE,,,"Selection, Path",3,X,relative,8,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,48%
[614],Bi & Liu,2022,Head Gestures and Pointing,In,Head,"Pitch, Roll, Yaw",Accelerometer,Yes,12,Semantic,Yes,Yes,No,Yes,Medium,High,Medium (N=30,High,No,No,No,No,No,Yes (N=30),No,"Walking, Slope, Sitting, Ground Material, Speed",Lab,Earbud,Commercial,No,No,"However, head immobility for a long time has led to the emergence of “phubbers” and “office workers,” which sounds an alarm for people’s health, such as neck pain [1]. Immersing yourself in interaction with mobile phones during walking is also an important cause of traffic accidents [2]. Fortunately, the development of the Internet of Healthcare Things (IoHT) has brought new hope",,"Health, Safety","Earphones, head gesture, Internet of Healthcare Things (IoHT), metalearning","With the popularity of personal computing devices, people often keep long-term head immobility in front of screens, resulting in the emergence of “phubbers” and “office workers.” The early warning solutions in the Internet of Healthcare Things (IoHT) have brought hope to protect users’ health and safety. However, most existing works cannot recognize the different head gestures during walking, which is also a common cause of text neck and traffic accidents. In addition, they also need a large amount of data to update the model to adapt to the new environment, which reduces the practicality of the model. To solve these problems, we propose a system, CSEar, based on builtin accelerometers of off-the-shelf wireless earphones, which can recognize 12 kinds of head gestures both in resting and walking states. First, an innovative algorithm is designed to detect head gesture signals, especially for the signals mixed with gait. Then, we propose the MetaSensing, a head gesture recognition model that can improve the recognition ability with few samples compared with the existing metalearning algorithms. Finally, the experimental results prove the effectiveness and robustness of the CSEar.",80,,https://ieeexplore.ieee.org/document/9815053,DONE,,,"Selection, Path, Orientation",3,X,relative,12,Yes,No,X,Partly Present,Not Indicated,High (Derived),Not Indicated,Present,43%
[452],Bi et al.,2022,Ear and Earable,In,Hand,Tap (Earable),Accelerometer,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),"Sitting, Standing","Lab, Train",Earbud,Commercial,No,No,"However, some of the earphone-based authentication solutions need to customize special earphones, which are not universal.; To solve this problem, a new authentication solution based on the existing commercial earphones is proposed to authenticate a user by tapping on the earphone rhythmically.; Therefore, the security and privacy problem in ICWSN is becoming increasingly serious",,Authentification,"Accelerometer, earphone, tap gesture, user authentication","The rapid development of the information-centric wireless sensor network (ICWSN) has solved the challenges of information transmission and processing caused by the accelerated growth of wearable devices and the wide deployment of the Internet of Things (IoT) recently. The privacy security is also a growing problem. The existing works use earphones, covert, and user-friendly wearable devices, for user authentication. However, some of the earphone-based authentication solutions need to customize special earphones, which are not universal. Other solutions use microphones and speakers of earphones for authentication, which are susceptible to changes in the auricle’s internal environment, resulting in a decline in performance. To solve this problem, a new authentication solution based on the existing commercial earphones is proposed to authenticate a user by tapping on the earphone rhythmically. This rhythmic tap behavior causes a change of the signal waveform of the built-in accelerometer in the earphone. Based on this, we design a pipeline to authenticate the user’s identity. We first design an event detection algorithm to segment the tap signal accurately. Then, we use the global features calculated based on the event detection algorithm and local features extracted from the convolutional neural network (CNN) for building an authentication model using the Naive Bayes (NB) classifier. Finally, 20 users are recruited to evaluate the experiment and the recognition accuracy reaches 98%. Moreover, we extend the experiment to prove that it has a good performance against the different attacks and is robust in different scenarios.",81,,https://ieeexplore.ieee.org/document/9367286,DONE,,,X,X,X,X,1,Yes,No,X,Present (N=3),Not Indicated,High (Derived),Not Indicated,Not Indicated,25%
[345],Choi et al.,2022,Face,In,"Facial Expression, Head",Ekman 7,"Accelerometer, PPG",,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,Customer Analytics,,,82,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[627],Futami et al,2022,Face,In,Facial Expression,"Facial Gesture (Mouth), Facial Gesture (Eye)",Proximity Sensor,,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,Device Input,,,83,,,,,,Selection,X,X,relative,,Yes,No,,,,,,,
[315],Jin et al.,2022,Mouth,In,Speech Apparatus,"Silent Speech (Words), Silent Speech (Sentences)","Microphone, Speaker, Accelerometer, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Device Control, Silent Speech",,,84,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[278],Li et al.,2022,Face,In,Facial Expression,"Facial Gesture (Mouth), Facial Gesture (Eye), Smile","Microphone, Speaker",,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,Data Annotation,,,85,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[512],Song et al.,2022,Face,In,Facial Expression,Ekman 7,"Microphone, Speaker",,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,"AR/VR, Device Control, Customer Analytics",,,86,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[307],Srivastava et al.,2022,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),"Accelerometer, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Silent Speech, AR/VR, Privacy, Device Control",,,87,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[309],Wang et al.,2022,Head Gestures and Pointing,In,Head,"Pitch, Yaw",Microphone,No,2,Fine,Yes,Visual Attention,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=12),Sitting,Lab,Headphone,Research Prototype,Yes,No,"Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones.; allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.; Enabling a device to detect this intention precisely and naturally can simplify user interface fow, enable hands-free interaction, and adapt interfaces to the context of use. For example, smartphones can detect users’ proximity and face orientation toward the device to turn on the screen and allow them to read their notifcations in a hand-free manner. Additionally, a system that can provide an accurate classifcation of whether a user’s head is oriented toward a specifc device. This device-specifc binary attention detector can be used to drive context-aware experiences.",,"Device Input, Activity Recognition, Device Control, Motion Tracking","Acoustic ranging, head orientation, earphone, head pose estimation","Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7◦ in yaw, and 5.8◦ in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.",88,,https://dl.acm.org/doi/10.1145/3491102.3517698,DONE,,,"Selection, Orientation, Path",2,fine,absolute,2,No,Yes,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Present,28%
[306],Wang et al.,2022,Mouth,In,Teeth,"Click, Slide",Microphone,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Authentification,,,89,,,,,,"Selection, Position, Path",X,X,absolute,,Yes,No,,,,,,,
[388],Yang et al.,2022,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,90,,,,,,,,,,,,,,,,,,,
[284],Alkiek et al.,2023,Ear and Earable,In,Hand,"Tap (Ear), Slide (Ear), Pull (Ear), Press (Ear), Pinch (Ear)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,Medium,High (N=4),N/A,No,No,No,No,No,Yes (N=4),No,Sitting,Lab,Earbud,Commercial,Yes,No,"However, modern-day earables usually have a limited interface, inhibiting their potential as an accessible medium of input.; Augmenting earables with a hand-to-ear gesture interface would allow them to overcome this setback and provide a more convenient and richer experience for users.","Expanding Interaction Methods, Enhancing Functionalities of Current Devices, Reducing Interaction Effort, Improving Usability and Ergonomics","Music Player, Phone Calls, Device Control, Device Input","Earables, gesture recognition, inertial sensors, natural interface, on-body interaction","Earables have been gaining popularity over the past few years for their ease of use and convenience over wired earphones. However, modern-day earables usually have a limited interface, inhibiting their potential as an accessible medium of input. To this end, we present EarBender: an ear-based real-time system that bridges the gap between earables and on-body interaction, providing a more diverse and natural form of interaction with devices. EarBender enables touch-based hand-to-ear gestures on mobile devices by leveraging inertial sensors in commercially available earable devices. Our proposed system detects the slight deformation in a user’s ear resulting from different ear-based actions including swiping and tapping and classifies the action performed. EarBender is designed to be energy-efficient, easy to deploy and robust to different users, requiring little to no calibration. We implement a prototype of EarBender using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and parameter settings. Results show that the system can detect the occurrence of gestures with a 96.8% accuracy and classify seven different hand-to-ear gestures with an accuracy up to 97.4% maintained across four subjects.",91,,https://dl.acm.org/doi/10.1145/3594739.3610671,DONE,,,"Selection, Path, Position",1,X,relative,7,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,42%
[344],Chugh et al.,2023,"Head Gestures and Pointing, Face",Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,92,,,,,,,,,,,,,,,,,,,
[342],Jin et al.,2023,Mouth,In,"Head, Facial Expression",Silent Speech (Morphemes),"Accelerometer, Gyroscope",,,,Partly,Visual Attention,,,,,,,,,,,,,,,,,,,,,,Accessibility,,,93,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[376],Li et al.,2023,"Ear and Earable, Hand Gestures and Location",In,Hand,"Pinch (Ear), Thinking Gesture, Hold (Face), Cover (Face), Calling Gesture, Cover (Ear), Support (Face)",Microphone,No,8,Semantic,No,Yes,Yes (Performance Loss),No,Low,Medium,High (N=10),N/A,Yes (N=10),No,Yes (N=25),No,Yes (N=25),Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"We targeted on hand-to-face gestures because such gestures relate closely to speech and yield signifcant acoustic features (e.g., impeding voice propagation).; Thus, researchers have been seeking supplementary input methods as parallel input channels to assist voice interaction; In this paper, we investigated the feasibility of using voiceaccompanying hand-to-face (VAHF) gestures as parallel channels to improve the traditional voice interaction fow.; Specifcally, we aim at designing VAHF gestures and recognizing them with an acoustic-based cross-device sensing method. We targeted hand-toface gestures as the instantiation of voice-accompanying gestures because they have been proven to be natural, expressive (e.g., various landmarks on the face to yield a large gesture space), and more related to the speech by existing researches; Moreover, hand-to-face gestures yielded signifcant features in voice propagation, which is benefcial for acoustic sensing; outline new opportunities for VAHF gestures to beneft voice interaction.; Our quantitative analysis sheds light on the recognition capability of the diferent sensor combinations over VAHF gestures with diferent characteristics. We conducted a comprehensive study to elicit the gesture space of VAHF gestures and proposed a gesture set with better usability, better social acceptance, less fatigue, and less ambiguity.",,"Device Control, Device Input","hand gestures, acoustic sensing, sensor fusion","Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield signifcant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we frst gathered candidate gestures and then applied a structural analysis to them in diferent dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3% for recognizing 3 gestures and 91.5% for recognizing 8 gestures (excluding the ""empty"" gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.",94,,https://dl.acm.org/doi/10.1145/3544548.3581008,DONE,,,"Selection, Position",X,X,X,15,Yes,No,X,Present (N=10),High (N=25),Low (Derived),Not Indicated,Present,44%
[308],Panda et al.,2023,"Ear and Earable, Head Gestures and Pointing, Hand Gestures and Location",In,"Hand, Head","Lift (Earable), Cup (Mid-Air), Cover (Face), Pitch, Roll, Press (Earable), Yaw","Accelerometer, Gyroscope, Magnetometer, LiDAR Sensor, Button",Yes,11,"Semantic, Fine, Coarse",Partly,Visual Attention,No,No,Medium,High,N/A,N/A,Yes,No,No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,Yes,Yes,"the presence of microphones and even inertial motion sensors on some units (typically for spatial audio support [5, 10]) hints at richer possibilities for headphone-situated input, interaction, and wearable sensing. In this way sensor-enhanced headphones ofer designers an opportunity to (re-)consider notions of the user’s context, activity, and proximal hand gestures–enabling rich interactions beyond the status-quo, button-pushing type of interactions with headphones.; Further, by augmenting existing user actions, such an approach reduces the number of explicit gestures that the user has to learn, control, enact, and remember.",,Video Conference,"headphones, wearables, sensing, research through design, design space","Via Research through Design (RtD), we explore the potential of headphones as a general-purpose input device for both foreground motion-gestures as well as background sensing of user activity. As a familiar wearable device, headphones ofer a compelling site for head-situated interaction and sensing. Using emerging sensing modalities such as inertial motion, capacitive touch sensing, and depth cameras, our implemented prototypes explore sensing and interaction techniques that ofer a range of compelling capabilities. User scenarios include context-aware privacy, gestural audiovisual control, and co-opting natural body language as context to drive animated avatars for ""camera-of"" scenarios in remote workor to co-opt (oft-subconscious) head movements such as dodging attacks in video games to enhance the gameplay experience. Drawing from literature and other frameworks, we situate our prototypes and related techniques in a design space across the dual dimensions of (1) type of input (touch, mid-air, or head orientation); and (2) the context of user action (application, body, or environment). In particular, interactions that combine multiple inputs and contexts at the same time ofer a rich design space of headphonesituated wearable interactions and sensing techniques.",95,,https://dl.acm.org/doi/10.1145/3563657.3596022,DONE,,,"Selection, Position, Orientation",4,fine,"absolute, relative",11,Yes,Yes,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,35%
[443],Paul et al.,2023,"Brain, Eye-Tracking",Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,96,,,,,,,,,,,,,,,,,,,
[277],Stanke et al.,2023,Actuation,Enabling,N/A,,N/A,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,97,,,,,,,,,,,,,,,,,,,
[461],Yi et al.,2023,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),EMG,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Silent Speech, Phone Calls",,,98,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[619],Zhang et al.,2023,Ear and Earable,In,Hand,Touch (Ear),"Speaker, Microphone",Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,No,This paper presents an approach using a speaker/microphone pair to perform active acoustic sensing in the inaudible frequency range to achieve continuous finger positioning on the ear.,,"Music Player, Phone Calls","acoustic sensing, bone conduction, positioning, human interface","The advancement of semiconductor and battery technologies popularized tiny acoustic wearable devices such as bone conduction wireless headsets. However, this small form factor poses inconvenience when controlling these devices, as they cannot equip large footprint intuitive interfaces such as volume sliders and touch screens. This paper presents a technique using acoustic responses measured by a bone conduction speaker and a microphone to utilize the ear as a touch input interface. We discovered that a finger placed on different parts of the ear affects the acoustic radiation characteristic of the ear, modulating the leaked sound, and by leveraging this effect, the touch position can be estimated. Experimental results show that five distinct frequency responses with five different finger positions can be obtained, which indicates that our method could allow bone conduction headsets to capture continuous finger positions without additional hardware.",99,,https://dl.acm.org/doi/10.1145/3560905.3568075,DONE,,,"Selection, Position",X,coarse (5),X,1,No,Yes,X,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Not Indicated,20%
[292],Zhang et al.,2023,Face,In,Facial Expression,Ekman 7,Microphone,,,,Yes,Visual Attention,,,,,,,,,,,,,,,,,,,,,,"AR/VR, Accessibility, Emotion Recognition",,,100,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[713],Zhang et al.,2023,Mouth,In,Speech Apparatus,Silent Speech (Words),"Speaker, Microphone",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Silent Speech, Music Player",,,101,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[516],Zhu et al.,2023,Head Gestures and Pointing,In,Head,"Roll, Pitch, Yaw","Accelerometer, Gyroscope",Yes,12,Semantic,Yes,Yes,Yes,Yes,Medium,High,High (N=15),N/A,No,No,No,No,No,Yes (N=15),No,"Walking Stairs, Standing, Walking, Sports",Lab,Earbud,Research Prototype,Yes,No,"The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations.; In spite of good recognition performance, these works share a common shortcoming, that is, only considering singular activities and outputting a single activity type.",,"Device Control, Music Player, Phone Calls, Activity Recognition","Composite activity recognition, Earable device, Multi-task learning","The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations. We implement a real-time prototype and conduct extensive experiments to evaluate it. The results show that CHAR can recognize 60 kinds of composite activities (12 head gestures and 5 body movements) with high accuracies of 97.0% and 89.7% in user- dependent and independent cases, respectively.",102,,https://ieeexplore.ieee.org/abstract/document/10916516,DONE,,,"Selection, Path, Orientation",3,X,relative,12,Yes,No,X,Present,Not Indicated,Medium (Derived),Not Indicated,Present,52%
[346],Dong et al.,2024,Mouth,In,Speech Apparatus,Silent Speech (Words),Microphone,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Silent Speech, Device Control, Accessibility",,,103,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[498],Ge at al.,2024,Head Gestures and Pointing,In,Head,Yaw,Microphone,No,1,Fine,Yes,Visual Attention,No,No,Medium,High,High (N=6),High (N=6),No,No,No,No,No,Yes (N=6),Yes,"Standing, Distance, Noise","University Building, Outdoors",Earbud,Research Prototype,No,No,"Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakers generates a periodically fluctuating sound field, which the user’s two earphones detect.; Consequently, head tracking plays a crucial role in human–computer interaction (HCI) applications, such as tracking users’ attention during webpage browsing for content customization or tracking user attention for exhibit introduction in museums. An accurate, user-friendly, and widely applicable head tracking solution compatible with commercial off-the-shelf (COTS) devices is desired for daily use.",,"Customer Analytics, Activity Recognition","Acoustic signal processing, human computer interaction, signal processing, systems, user interfaces","Head tracking is a technique that allows for the measurement and analysis of human focus and attention, thus enhancing the experience of human–computer interaction (HCI). Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakers generates a periodically fluctuating sound field, which the user’s two earphones detect. By assessing the distance and angle alterations between the earphones and speakers, we propose a model to determine the user’s head movement and orientation. Our evaluation results indicate a high degree of accuracy in both head movement tracking, with an average tracking error of 2.98 cm,  and head orientation tracking, with an average error of 1.83◦. Furthermore, in a deployed exhibition scenario, we attained an accuracy of 89.2% in estimating the user’s focus direction.",104,,https://ieeexplore.ieee.org/document/10192901,DONE,,,"Selection, Orientation",1,fine,absolute,1,No,Yes,X,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Not Indicated,30%
[482],Hu et al.,2024,Head Gestures and Pointing,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,105,,,,,,,,,,,,,,,,,,,
[509],Hu et al.,2024,Head Gestures and Pointing,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,106,,,,,,,,,,,,,,,,,,,
[625],Lepold et al.,2024,"Brain, Eye-Tracking",Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,107,,,,,,,,,,,,,,,,,,,
[449],Ronco et al.,2024,Hand Gestures and Location,In,Hand,"Pinch (Mid-Air), Tilt (Mid-Air), Slide (Mid-Air), Push (Mid-Air), Pull (Mid-Air), Rub (Mid-Air), Circle (Mid-Air), Hold (Mid-Air)",mmWaveRadar,Yes,11,Semantic,No,Yes,Yes,No,Low,Medium,High (N=20),N/A,No,No,No,No,No,Yes (N=20),No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,"Hand Gesture Recognition (HGR) has been proposed as an intuitive Human-Machine Interface, potentially suitable for controlling several classes of devices in the context of the Internet of Things. This paper proposes a low-power in-ear HGR system based on mm-wave radars, efficient spatial and temporal Convolutional Neural Networks and an energy-optimized hardware design.; However, their limitations are becoming more evident when it comes to adapting to the evolving landscape of ubiquitous, and perhaps simple devices such as wearable systems, where the integration of traditional interfaces is not a viable option for physical constraints, strict energy requirements, or purely aesthetic reasons. The need for more intuitive and adaptive solutions has become increasingly evident, prompting extensive research into alternative technologies.",,Device Control,"mm-wave, radar, gesture recognition, low-power, embedded, sensor","Smart Internet of Things (IoT) devices are on the rise in popularity, with innovative use cases and applications emerging every year. Including intelligence in these novel systems presents the challenge of integrating interaction and communication in scenarios where traditional interfaces are not viable. Hand Gesture Recognition (HGR) has been proposed as an intuitive Human-Machine Interface, potentially suitable for controlling several classes of devices in the context of the Internet of Things. This paper proposes a low-power in-ear HGR system based on mm-wave radars, efficient spatial and temporal Convolutional Neural Networks and an energy-optimized hardware design. The design is suitable for battery-operated devices, with stringent size and energy constraints, enabling user interaction with wearable devices, but also suitable for home appliances and industrial applications. The proposed machine learning model is characterized thoroughly for robustness and generalization capabilities, achieving 94.9% (single subject) and 86.1% (LeaveOne-Out Cross-validation) accuracy on a set of 11+1 gestures with a model size of only 36 KiB and inference latency of 32.4 ms on a 64 MHz Cortex-M33 microcontroller, making it compatible with real-time applications. The system is demonstrated in a fully integrated, miniaturized in-ear device with a full-system average power consumption of 18.4 mW, a more than 6x improvement on the current state of the art.",108,,https://ieeexplore.ieee.org/document/10562162,DONE,,,"Selection, Path, Orientation",3,X,relative,11,Yes,No,X,Present,Not Indicated,High (Derived),Not Indicated,Present,54%
[635],Sato et al.,2024,Ear and Earable,In,Hand,"Pull (Ear), Swipe (Ear), Pinch (Ear), Fold (Ear)","Accelerometer, Gyroscope, Magnetometer",Yes,15,Semantic,No,Yes,No,No,Medium,Medium,Low(N=10),Low (N=10),Yes (N=19),No,No,No,No,Yes (N=10),No,"Sitting, Walking",Lab,Earbud,Commercial,No,No,"however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures.; Consequently, a compelling demand exists for a device operation method that uses only hearables.; Consequently, there is a need to systematically identify and classify gesture sets that align with various input technologies, particularly by exploring user-defined gestures under defined interaction area constraints.; Consequently, it becomes important to investigate user-defined gestures that are suitable for scenarios where the user is only wearing the hearables.",,"Device Control, Music Player, Phone Calls, Communication, Health, Data Annotation, Device Input","Hearables, Hands Gesture Recognition, User-Defined Gesture, Gesture Elicitation Study, IMU","Hearables are highly functional earphone-type wearables; however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures. In previous research on hearables for hand input, user understanding and gesture recognition systems have been developed. However, in the realm of user understanding, investigation concerning hand input with hearables remains incomplete, and existing recognition systems have not demonstrated proficiency in discerning user-defined gestures. In this study, we conducted a gesture elicitation study (GES) assuming hand input using hearables under six conditions (three interaction areas × two device shapes). Then, we extracted ear-level gestures that the device’s built-in IMU sensor could recognize from the user-defined gestures and investigated the recognition performance. The results of sitting experiments showed that the gesture recognition rate for in-ear devices was 91.0% and that for ear-hook devices was 74.7%.",109,,https://dl.acm.org/doi/10.1145/3676503,DONE,,,"Selection, Position, Path",3,X,relative,47,Yes,No,X,Present (N=19),Not Indicated,High (Derived),Not Indicated,Partly Present,23%
[291],Shojaeifard et al.,2024,Head Gestures and Pointing,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,110,,,,,,,,,,,,,,,,,,,
[640],Srivastava et al.,2024,Mouth,In,Speech Apparatus,"Silent Speech (Words), Silent Speech (Sentences)","Accelerometer, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"Silent Speech, Device Input, Privacy, Security, Authentification, Communication, AR/VR, Accessibility",,,111,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[654],Srivastava et al.,2024,Mouth,In,Speech Apparatus,Silent Speech (Sentences),"Accelerometer, EMG, Gyroscope",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,"AR/VR, Privacy, Accessibility, Communication, Device Control, Phone Calls, Device Input, Video Conference, Music Player",,,112,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[506],Sun et al.,2024,Mouth,In,Speech Apparatus,"Silent Speech (Phonemes), Silent Speech (Words)","Microphone, Speaker",,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Silent Speech,,,113,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[639],Suzuki et al.,2024,Hand Gestures and Location,In,Hand,"Swipe (Mid-Air), Twist (Mid-Air), Grip (Mid-Air), Squeeze (Mid-Air), Calling Gesture","Speaker, Microphone",Yes,7,Semantic,No,Yes,Yes,No,Low,Medium,Low (N=20),Low (N=20),Yes (N=11),No,Yes (N=11),No,Yes (N=11),Yes (N=20),No,"Sitting, Gloves, Walking, Surplus Person, Music",Lab,Earbud,Research Prototype,No,No,"We introduce EarHover, an innovative system that enables mid-air gesture input for hearables. Mid-air gesture input, which eliminates the need to touch the device and thus helps to keep hands and the device clean.; We propose EarHover (Figure 1), a system that minimizes the cost of additional sensors and enables mid-air gesture recognition.",,Device Input,"Hearables, mid-air gesture recognition, Doppler efect, sound leakage, deep learning, acoustic sensing","We introduce EarHover, an innovative system that enables mid-air gesture input for hearables. Mid-air gesture input, which eliminates the need to touch the device and thus helps to keep hands and the device clean. However, existing mid-air gesture input methods for hearables have been limited to adding cameras or infrared sensors. By focusing on the sound leakage phenomenon unique to hearables, we have realized mid-air gesture recognition using a speaker and an external microphone that are highly compatible with hearables. The signal leaked to the outside of the device due to sound leakage can be measured by an external microphone, which detects the diferences in refection characteristics caused by the hand’s speed and shape during mid-air gestures. Among 27 types of gestures, we determined the seven suitable gestures for EarHover in terms of signal discrimination and user acceptability. We then evaluated the gesture detection and classifcation performance of two prototype devices (in-ear type/open-ear type) for real-world application scenarios.",114,,https://dl.acm.org/doi/10.1145/3654777.3676367,DONE,,,"Selection, Path, Orientation",4,X,relative,27,Yes,No,X,Present (N=11),Not Indicated,Low (Derived),High (N=11),Present,37%
[286],Wang et al.,2024,Hand Gestures and Location,In,Hand,Slide (Face),Microphone,Yes,5,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,Yes (N=20),No,No,Yes (N=20),No,Yes (N=26),"Noise, Music, Sitting, Standing, Sports, Walking, Make-Up","Living Room, Office, Outdoors, Car",Earbud,Research Prototype,No,No,"There is an increasing demand for robust earables authentication because of the growing amount of sensitive information and the IoT devices that the earable could access.; In particular, we repurpose the face and earables as a natural scanner to indirectly sense the fingerprint biometrics.",,Authentification,"Biometrics, Fingerprint, Friction, User Authentication, Earable","Ear wearables (earables) are emerging platforms that are broadly adopted in various applications. There is an increasing demand for robust earables authentication because of the growing amount of sensitive information and the IoT devices that the earable could access. Traditional authentication methods become less feasible due to the limited input interface of earables. Nevertheless, the rich head-related sensing capabilities of earables can be exploited to capture human biometrics. In this paper, we propose EarSlide, an earable biometric authentication system utilizing the advanced sensing capacities of earables and the distinctive features of acoustic fingerprints when users slide their fingers on the face. It utilizes the inward-facing microphone of the earables and the face-ear channel of the ear canal to reliably capture the acoustic fingerprint. In particular, we study the theory of friction sound and categorize the characteristics of the acoustic fingerprints into three representative classes, pattern-class, ridge-groove-class, and coupling-class. Different from traditional fingerprint authentication only utilizes 2D patterns, we incorporate the 3D information in acoustic fingerprint and indirectly sense the fingerprint for authentication. We then design representative sliding gestures that carry rich information about the acoustic fingerprint while being easy to perform. It then extracts multi-class acoustic fingerprint features to reflect the inherent acoustic fingerprint characteristic for authentication. We also adopt an adaptable authentication model and a user behavior mitigation strategy to effectively authenticate legit users from adversaries. The key advantages of EarSlide are that it is resistant to spoofing attacks and its wide acceptability. Our evaluation of EarSlide in diverse real-world environments with intervals over one year shows that EarSlide achieves an average balanced accuracy rate of 98.37% with only one sliding gesture.",115,,https://dl.acm.org/doi/10.1145/3643515,DONE,,,"Position, Path",2,X,relative,5,Yes,No,X,Partly Present,High (N=20),Medium (Derived),Not Indicated,Not Indicated,38%
[532],Wang et al.,2024,Hand Gestures and Location,In,Hand,"Slide (Face), Tap (Face)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),Sitting,Lab,Earbud,Commercial,No,No,"However, the widespread adoption of wireless earbuds has spurred concerns regarding security and privacy, necessitating robust bespoke security measures. Despite the miniaturization of mobile chips enabling the integration of sophisticated algorithms into smart wearables, the research and industrial communities have yet to accord adequate attention to earbud security.; This article focuses on empowering wireless earbuds to authenticate their legitimate users, tackling the challenges associated with conventional authentication methods. Instead of relying on input interface authentication methods like PIN or lock patterns, this research delves into leveraging inertial measurement unit (IMU) data collected during interactions with devices to extract novel biometric features, presenting an alternative approach that nonetheless confronts challenges related to signal capture and interference.; Especially as the demand for interactive functions on smart earbuds grows, necessitating increased user permissions and information access, highlighting the importance of privacy and security measures.",,Authentification,"Adversarial learning, implicit authentication, wearable computing","The surge in popularity of wireless headphones, particularly wireless earbuds, as smart wearables, has been notable in recent years. These devices, empowered by artificial intelligence (AI), are broadening their utility in areas such as speech recognition, augmented reality, pose recognition, and health care monitoring, thereby enriching user experiences through novel interactive interfaces driven by embedded sensors. However, the widespread adoption of wireless earbuds has spurred concerns regarding security and privacy, necessitating robust bespoke security measures. Despite the miniaturization of mobile chips enabling the integration of sophisticated algorithms into smart wearables, the research and industrial communities have yet to accord adequate attention to earbud security. This article focuses on empowering wireless earbuds to authenticate their legitimate users, tackling the challenges associated with conventional authentication methods. Instead of relying on input interface authentication methods like PIN or lock patterns, this research delves into leveraging inertial measurement unit (IMU) data collected during interactions with devices to extract novel biometric features, presenting an alternative approach that nonetheless confronts challenges related to signal capture and interference. Consequently, we propose and design BudsAuth, an implicit user authentication framework that harnesses built-in IMU sensors in smart earbuds to capture vibration signals induced by onface touching interactions with the earbuds. These vibrations are utilized to deliver continuous and implicit user authentication with high precision and compatibility across various earbud models. Extensive evaluation demonstrates BudsAuth’s capability to achieve an equal error rate (EER) of 0.0003, representing an approximate 99.97% accuracy with seven consecutive samples of interactive gestures for implicit authentication.",116,,https://ieeexplore.ieee.org/abstract/document/10478100,DONE,,,"Selection, Position, Path",2,X,relative,7,Yes,No,X,Not Present,Not Indicated,Not Indicated,Not Indicated,Present,25%
[676],Wang et al.,2024,Hand Gestures and Location,In,Hand,Write (Face),Microphone,Yes,36,Semantic,No,Yes,Yes,Yes,Low,Medium,High (N=10),High,No,Yes (N=20),No,No,No,Yes (N=10),No,"Sitting, Standing, Head Movement, Walking","Office, Living Room, Car, Outdoors",Earbud,Research Prototype,No,No,"As earables gain popularity, there emerges a need for intuitive user interfaces that adapt to diverse daily scenarios. Traditional methods like touchscreens and voice control often fall short in environments like movie theatres, where silence and darkness are required, or on busy streets where visual distraction introduces extra risk. We propose an innovative earable-based system utilizing unique acoustic friction generated by fingers for alphanumeric input.; This paper introduces FaceTyping, an interaction system that enables alphanumeric handwriting on the face using passive acoustic sensing in earables.",,"Device Input, Privacy","Earable, Face and Ear Interaction, Gestures Recognition, Acoustic Sensing","As earables gain popularity, there emerges a need for intuitive user interfaces that adapt to diverse daily scenarios. Traditional methods like touchscreens and voice control often fall short in environments like movie theatres, where silence and darkness are required, or on busy streets where visual distraction introduces extra risk. We propose an innovative earable-based system utilizing unique acoustic friction generated by fingers for alphanumeric input. Our approach digs into the acoustic friction theory, applying this knowledge to better understand the transformation from 2D handwriting into a 1D acoustic time series. This theoretical foundation guides our system design and feature extraction. Specifically, we have redesigned certain characters to enhance their acoustic distinctiveness without compromising the natural handwriting style of users, ensuring the system userfriendly. Our system combines DenseNet and GRU architectures in a multimodal model, refined through transfer learning to adapt to diverse user behaviors. Tested in real-world scenarios with 10 participants, our system achieves a 95% accuracy in recognizing both letters and numbers.",117,,https://ieeexplore.ieee.org/document/10637602,DONE,,,"Selection, Path, Quantification, Text",2,X,relative,36,Yes,No,X,Present,High (N=20),High (Derived),Not Indicated,Present,62%
[675],Xie et al.,2024,Head Gestures and Pointing,Enabling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,118,,,,,,,,,,,,,,,,,,,
[354],Yang et al.,2024,"Ear and Earable, Hand Gestures and Location",In,Hand,"Press (Face), Pinch (Face), Pinch (Ear), Cover (Face), Scratch (Face), Close (Mid-Air), Open (Mid-Air), Slide (Mid-Air), Click (Mid-Air), Approach (Mid-Air)","Speaker, Microphone",Yes,10,Semantic,No,Yes,No,No,Low,Medium,High (N=22),Medium (N=1),No,Yes (N=22),Yes (N=22),No,Yes (N=22),Yes (N=22),No,"Sitting, Noise, Distance, Hydration, Standing, Walking, Sports, Eating, Speaking, Music",Lab,Earbud,Research Prototype,No,No,"However, these systems face limitations in capturing contactless gestures (i.e., those gestures performed over the face) because such gestures do not generate signals that are detectable by the aforementioned sensors.; can we design a system that is able to detect hand-to-face gestures, whether in contact with the face or above it, using widely accessible mobile devices?; Firstly, its wearable nature guarantees that users can move freely without any inconvenience or hindrance, always interacting with the device seamlessly.; Secondly, MAF does not depend on specialized sensors or require any modifications to standard bone conduction earphones.; We then craft a plethora of user studies to i) examine the impact of various human factors on the mobile acoustic field; and ii) assess the social acceptance of this mobile acoustic field-based gesture interaction by interviewing 22 participants.",,"Music Player, AR/VR, Health, Device Control","Wearable Computing, Gesture Detection, Acoustic Sensing","We present MAF, a novel acoustic sensing approach that leverages the commodity hardware in bone conduction earphones for handto-face gesture interactions. Briefly, by shining audio signals with bone conduction earphones, we observe that these signals not only propagate along the surface of the human face but also dissipate into the air, creating an acoustic field that envelops the individual’s head. We conduct benchmark studies to understand how various handto-face gestures and human factors influence this acoustic field. Building on the insights gained from these initial studies, we then propose a deep neural network combined with signal preprocessing techniques. This combination empowers MAF to effectively detect, segment, and subsequently recognize a variety of hand-to-face gestures, whether in close contact with the face or above it. Our comprehensive evaluation based on 22 participants demonstrates that MAF achieves an average gesture recognition accuracy of 92% across ten different gestures tailored to users’ preferences.",119,,https://dl.acm.org/doi/10.1145/3613904.3642437,DONE,,,Selection,"1, X",X,"absolute, relative",12,Yes,No,X,Partly Present,Medium (N=22),Low (Derived),Medium (N=22),Present,43%
[86],Bedri et al.,2015,Mouth,Out,Jaw,General Jaw Movement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19,,,,,,,,,,,,,,,,,,,
[527],Yi et al.,2024,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),EMG,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Silent Speech,,,120,,,,,,Selection,X,X,X,,Yes,No,,,,,,,
[712],Manabe et al.,2012,Hand Gestures and Location,Duplicate of [23],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[617],Hossain et al.,2019,,Duplicate of [622],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[621],Wang et al.,2021,,Duplicate of [306],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[629],Futami et al,2022,,Duplicate of [627],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[281],Ma et al.,2022,,Duplicate of [626],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[335],Choi et al.,2023,,Duplicate of [345],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[299],Wang et al.,2023,Mouth,Duplicate of [306],Teeth,"Click, Slide",Microphone,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Authentification,,,-,,,,,,"Position, Selection, Path",X,X,absolute,,Yes,No,,,,,,,
[32],Metzger et al.,2004,Hand Gestures and Location,Singled Loc,Hand,"Slide (Mid-Air), Hold (Mid-Air)",Proximity Sensor,Present,6,,No,Yes,Not Present,Not Present,Low (Derived),Low (Derived),High (N=1),Not Indicated,,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Accessibility, Device Control",,,-,,,Done,,,"Selection, Path, Quantification",1,coarse,relative,6,Yes,Yes,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,32%
[32],Metzger et al.,2004,Head Gestures and Pointing,Singled Loc,Head,Roll,Accelerometer,Present,1,,No,Yes,Not Present,Not Present,Medium (Derived),High (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Accessibility, Device Control",,,-,,,Done,,,"Path, Orientation",1,X,relative,1,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,22%
[17],Ando et al.,2017,Face,Singled Loc,Jaw,Facial Gesture (Mouth),Pressure Sensor,Present,6,,Yes,Yes,Partly Present,Not Present,Medium (Derived),Medium (Derived),High (N=12),Medium (N=6),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, Device Control",,,-,,,Done,,,Selection,2,X,relative,6,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,34%
[17],Ando et al.,2017,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Roll, Yaw",Pressure Sensor,Present,5,,Yes,Yes,Partly Present,Not Present,Medium (Derived),High (Derived),Medium (N=12),Medium (N=6),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, Device Control",,,-,,,Done,,,"Selection, Orientation, Path",3,X,relative,5,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,31%
[12],Matthies et al.,2017,Face,Singled Loc,Facial Expression,"Smile, Facial Gesture (Mouth), Facial Gesture (Eye)","EMG, Capacitive Sensor, Electrical Field Sensing",Present,11,,Yes,Visual Attention,Not Present,Not Present,Low (Derived),Low (Derived),Low (N=1),Medium (N=3),,,,,,,,,,,,Not Present,Not Present,,,Device Input,,,-,,,Done,,,"Selection, Orientation",2,X,relative,11,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,18%
[12],Matthies et al.,2017,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Yaw","EMG, Capacitive Sensor, Electrical Field Sensing",Present,4,,Yes,Yes,Not Present,Not Present,Medium (Derived),High (Derived),Low (N=1),Medium (N=3),,,,,,,,,,,,Not Present,Not Present,,,Device Input,,,-,,,Done,,,"Selection, Path, Orientation",2,X,relative,4,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,23%
[111],Wang et al.,2017,Eye-Tracking,Singled Loc,"Facial Expression, Gaze",,"EOG, EMG",,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Device Control,,,-,,,,,,Selection,2,coarse (4),relative,,Yes,No,,,,,,,
[111],Wang et al.,2017,Face,Singled Loc,"Facial Expression, Gaze",,"EOG, EMG",,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Device Control,,,-,,,,,,Selection,2,coarse (4),relative,,Yes,No,,,,,,,
[8],Amesaka et al.,2019,Face,Singled Loc,Facial Expression,Facial Gesture (Mouth),"Microphone, Speaker",Present,3,,Yes,Yes,Not Present,Not Present,Medium (Derived),Medium (Derived),Medium (N=11),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input, Music Player",,,-,,,Done,,,"Selection, Orientation, Path",1,X,relative,12,Yes,No,X,Present (N=11),Not Indicated,Medium (Derived),Not Indicated,Present,29%
[8],Amesaka et al.,2019,Head Gestures and Pointing,Singled Loc,Head,Roll,"Microphone, Speaker",Present,2,,Yes,Yes,Not Present,Not Present,Medium (Derived),High (Derived),Medium (N=11),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input, Music Player",,,-,,,Done,,,"Selection, Path, Orientation",1,X,relative,6,Yes,No,X,Present (N=11),Not Indicated,Medium (Derived),Not Indicated,Present,31%
[234],Chen et al.,2020,Ear and Earable,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[234],Chen et al.,2020,Hand Gestures and Location,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[26],Xu et al.,2020,Ear and Earable,Singled Loc,Hand,"Tap (Ear), Slide (Ear)",Microphone,Present,3,,No,Yes,Present,Present,Medium (Derived),High (N=16),Medium (N=18),Medium (N=18),,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Communication, Device Input, AR/VR",,,-,,,Done,,,"Selection, Position, Path",1,X,relative,8,Yes,No,X,Present (N=16),High (N=12),High (Derived),Medium (N=16),Present,73%
[26],Xu et al.,2020,Hand Gestures and Location,Singled Loc,Hand,"Tap (Face), Slide (Face)",Microphone,Present,5,,No,Yes,Present,Present,Medium (Derived),High (N=16),High (N=18),Medium (N=18),,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Communication, Device Input, AR/VR",,,-,,,Done,,,"Selection, Position, Path",1,X,relative,19,Yes,No,X,Present (N=16),High (N=12),High (Derived),High (N=16),Present,83%
[258],Gashi et al.,2021,Face,Singled Loc,Facial Expression,"Smile, Facial Gesture (Mouth)","Accelerometer, Gyroscope",Present,2,,Yes,Yes,Present,Not Present,Low (Derived),Low (Derived),Low (N=21),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Social Interaction, Feedback System",,,-,,,Done,,,"Path, Selection",X,X,X,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,21%
[258],Gashi et al.,2021,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Present,2,,Yes,Yes,Present,Not Present,Medium (Derived),Medium (Derived),Low (N=21),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Social Interaction, Feedback System",,,-,,,Done,,,"Path, Orientation, Selection",2,X,relative,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,24%
[296],Rateau et al.,2022,Ear and Earable,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[296],Rateau et al.,2022,Hand Gestures and Location,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[344],Chugh et al.,2023,Head Gestures and Pointing,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[344],Chugh et al.,2023,Face,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[376],Li et al.,2023,Ear and Earable,Singled Loc,Hand,"Pinch (Ear), Cover (Ear)",Microphone,Not Present,2,,No,Yes,Partly Present,Not Present,Medium (Derived),High (N=25),High (N=10),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input",,,-,,,Done,,,"Selection, Position",X,X,X,5,Yes,No,X,Present (N=10),High (N=25),Low (Derived),Not Indicated,Present,44%
[376],Li et al.,2023,Hand Gestures and Location,Singled Loc,Hand,"Thinking Gesture, Hold (Face), Cover (Face), Calling Gesture, Support (Face)",Microphone,Not Present,6,,No,Yes,Partly Present,Not Present,Low (Derived),High (N=25),High (N=10),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input",,,-,,,Done,,,"Selection, Position",X,X,X,10,Yes,No,X,Present (N=10),High (N=25),Low (Derived),Not Indicated,Present,44%
[308],Panda et al.,2023,Ear and Earable,Singled Loc,Hand,"Lift (Earable), Press (Earable)","Accelerometer, Gyroscope, Magnetometer, Button",Present,2,,No,Yes,Not Present,Not Present,Medium (Derived),Medium (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Present,,,Video Conference,,,-,,,Done,,,"Selection, Position",1,X,relative,2,Yes,No,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,32%
[308],Panda et al.,2023,Hand Gestures and Location,Singled Loc,Hand,"Cup (Mid-Air), Cover (Face)",LiDAR Sensor,Present,2,,No,Yes,Not Present,Not Present,Low (Derived),Medium (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Present,,,Video Conference,,,-,,,Done,,,"Selection, Position",X,X,X,2,Yes,No,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,31%
[308],Panda et al.,2023,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Roll, Yaw","Accelerometer, Gyroscope, Magnetometer",Present,8,,Yes,Visual Attention,Not Present,Not Present,Medium (Derived),High (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Present,,,Video Conference,,,-,,,Done,,,"Selection, Position, Orientation",3,fine,"absolute, relative",2,Yes,Yes,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,35%
[275],Shimon et al.,2024,Ear and Earable,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[275],Shimon et al.,2024,Hand Gestures and Location,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[354],Yang et al.,2024,Ear and Earable,Singled Loc,Hand,Pinch (Ear),"Speaker, Microphone",Present,1,,No,Yes,Not Present,Not Present,Medium (Derived),High (Derived),High (N=22),Medium (N=1),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, AR/VR, Health, Device Control",,,-,,,Done,,,Selection,X,X,X,1,Yes,No,X,Partly Present,Medium (N=22),Low (Derived),Medium (N=22),Present,46%
[354],Yang et al.,2024,Hand Gestures and Location,Singled Loc,Hand,"Press (Face), Cover (Face), Close (Mid-Air), Open (Mid-Air), Slide (Mid-Air), Click (Mid-Air), Approach (Mid-Air)","Speaker, Microphone",Present,9,,No,Yes,Not Present,Not Present,Low (Derived),Medium (Derived),High (N=22),Medium (N=1),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, AR/VR, Health, Device Control",,,-,,,Done,,,"Selection, Path",2,X,relative,11,Yes,No,X,Partly Present,Medium (N=22),Low (Derived),Medium (N=22),Present,43%
[9],Pham et al.,2020,Eye-Tracking,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[244],Cao et al.,2021,Mouth,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
[57],Salzar et al.,2008,Head Gestures and Pointing,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,,
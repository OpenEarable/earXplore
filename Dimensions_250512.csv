,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ID,Main Author,Year,Location,Filter,Input Body Part,Gesture,Sensors,No Additional Sensing,Number of Selected Gestures,Resolution,Hands- Free,Eyes- Free,Possible on One Ear,Adaptation of the Interaction Detection Algorithm to the Individual User,Discreetness of Interaction Techniques,Social Acceptability of Interaction Techniques,Accuracy of Interaction Recognition,Robustness of Interaction Detection,Elicitation Study,Usability Evaluations,Cognitive Ease Evaluations,Discreetness of Interactions Evaluations,Social Acceptability of Interactions Evaluations,Accuracy of Interactions Evaluation,Alternative Interaction Validity Evaluations,Evaluation of Different Conditions,Evaluations of Different Settings,Earphone Type,Development Stage,Real-Time Processing,On Device Processing,Motivations (Notes),Motivations,Intended Applications,Keywords,Abstract,DB ID,Study Link,DONE,Discreetness of Interaction Techniques - MK,Social Acceptability of Interaction Techniques - MK,Elemental Task,Degrees of Freedom,Resolution - Alt,Spatial,No. of proposed Semantic Gestures,Discrete,Continuous,Active Feedback,Theoretical Grounding of Interaction Techniques,Usability,"Device 
Appearance","Cognitive 
Ease",Multi-Gesture Detection,Composite Evaluative Score
[33],Weisenberger et al.,1987,Actuation,Enabling,N/A,Vibration (Actuation),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,N/A,N/A,"In the population of hearing-impaired persons in the United States and other countries are a substantial number who do not receive satisfactory benefit from conventional acoustic amplification.  For these persons, some auditory functions can be  taken over to some degree by a substitute sensory modality (i.e., vision or touch).; The studies described above suggest that the tactile system can be a usef~~pfrocessor of acoustic information. In the present study, the design and preliminary evaluation of a vibrotactile aid designed to fill some of these basic auditory function\ are described.",,"Accessibility, Health",N/A,"A binaural earmold sound-to-tactile aid was constructed by inserting a vibrating element into a Lucite earmold. The earmold could be vibrated at either 80 Hz (when incoming acoustic signals were below 2000 I-Ir), at 300 Hz (when incoming acoustic signals were above 2000 Hz), or both (when incoming acoustic signals were broadband). Subjects were fitted with one of these bimodal vibrating earmolds in each ear. Normal-hearing and hearing-impaired subjects were tested in three tasks: sound localization, errvironmental sound identification, and syllable rhythm and stress. The device provided some benefit to performance, although the amounts of improvement varied across tasks and subjects. Possible modifications in device design, and potential combinations of auditory and tactile input via earmold systems, are discussed.",1,https://www.researchgate.net/publication/19575713_Development_and_preliminary_evaluation_of_an_earmold_sound-to-tactile_aid_for_the_hearing-impaired,DONE,,,,,,,,,,,,,,,,
[714],Brewster et al.,2003,Head Gestures and Pointing,In,Head,"Roll, Pitch","Accelerometer, Gyroscope, Magnetometer",Yes,1,Coarse,Yes,Yes,No,No,Medium,High,N/A,N/A,No,Yes (N=18),No,No,No,No,No,Sitting,Lab,Custom Device,Commercial,Yes,No,"Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use.; Mobile and wearable computers have been one of the major growth areas of computing in recent years. Compared to desktop systems these devices have restricted input and output capabilities that typically reduces their usability. With often very limited amounts of screen space, their visual displays can easily become cluttered with information and widgets. Input is limited, with small keyboards or simple handwriting recognition the norm.; With the imminent dramatic increase in network bandwidth available to mobile and wearable devices, and the consequent rise in the number of possible services, new interaction techniques are needed to access services whilst on the move.; Our solution to input focuses on multi-dimensional gestural interaction.",,Device Input,"Gestural interaction, wearable computing","Mobile and wearable computers present input/output problems due to limited screen space and interaction techniques. When mobile, users typically focus their visual attention on navigating their environment - making visually demanding interface designs hard to operate. This paper presents two multimodal interaction techniques designed to overcome these problems and allow truly mobile, ‘eyes-free’ device use. The first is a 3D audio radial pie menu that uses head gestures for selecting items. An evaluation of a range of different audio designs showed that egocentric sounds reduced task completion time, perceived annoyance, and allowed users to walk closer to their preferred walking speed. The second is a sonically enhanced 2D gesture recognition system for use on a belt-mounted PDA. An evaluation of the system with and without audio feedback showed users’ gestures were more accurate when dynamically guided by audio-feedback. These novel interaction techniques demonstrate effective alternatives to visual-centric interface designs on mobile devices.",2,https://dl.acm.org/doi/abs/10.1145/642611.642694,DONE,,,"Selection, Orientation, Path",2,coarse (4),relative,1,No,Yes,Coupled,Not Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,22%
[32],Metzger et al.,2004,"Hand Gestures and Location, Head Gestures and Pointing",In,"Hand, Head","Slide (Mid-Air), Hold (Mid-Air), Roll","Proximity Sensor, Accelerometer",Yes,7,"Coarse, Semantic",No,Yes,No,No,Low,Low,High (N=1),N/A,No,No,No,No,No,Yes (N=1),No,Sitting,Lab,Headphone,Research Prototype,Yes,No,"FreeDigiter uses hand gesture recognition to control mobile devices. In contrast to many existing gesture recognition systems, data analysis is simple, requiring the filtering of the binary output of a proximity sensor. This simplicity allows for small and inexpensive hardware with low power consumption and high wearability.",,"Music Player, Phone Calls, Accessibility, Device Control",N/A,"We present FreeDigiter, an interface for mobile devices which enables rapid entry of digits using finger gestures. FreeDigiter is an infrared proximity sensor with a dual axis accelerometer and requires little signal processing. Initial laboratory experiments attain accuracy rates of 99.0%; and the system is tolerant to highly varying lighting conditions. The FreeDigiter system requires little power and could be implemented in a very small form factor appropriate for controlling in–ear hearing aids, small MP3 players, and hands–free mobile phone headsets.",3,https://ieeexplore.ieee.org/document/1364684,DONE,,,"Selection, Path, Quantification",2,coarse,relative,7,Yes,Yes,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,32%
[40],Buil & Hollemans,2005,Ear and Earable,In,Hand,Press (Earable),Button,Yes,3,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,Assess the users accepted operating force for button pushes at the headphones,Improving Usability and Ergonomics,Music Player,N/A,"The touch headphones are a solution for providing playback and volume controls on in-ear type headphones.  One of the issues with placing controls on earpieces is that applied pressure is transferred to the inner ear,  which potentially creates discomfort. The experiment described in this short paper shows that conventional button switches  are not well accepted. Users preferred to operate a button on an earpiece with a force of around 85 grams.",4,https://ieeexplore.ieee.org/abstract/document/1550805,DONE,,,"Selection, Position",X,X,X,3,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,41%
[84],Buil et al.,2005,Ear and Earable,In,"Hand, Wearable State","Tap (Earable), Hold (Earable), Remove Earbud, Attach Earbud",Capacitive Sensor,Yes,5,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Research Prototype,Yes,No,Replace remote control in the headphone wire,"Expanding Interaction Methods, Enhancing Functionalities of Current Devices","Music Player, Device Control","MP3, Music Playback, headphones, capacitive touch control, user system interaction, user interface","The Touch Headphones are meant for portable music players and aim to present an improvement to the conventional remote control in the headphone wire, and a solution for controls on wireless in-ear type headphones. Two capacitive touch sensors per earpiece sense when earpieces are being tapped on, and being put in or out.",5,https://dl.acm.org/doi/abs/10.1145/1085777.1085877,DONE,,,Selection,X,X,X,5,Yes,No,Coupled,Partly Present,Not Indicated,Not Indicated,Not Indicated,Present,35%
[61],Manabe & Fukumoto,2006,Eye-Tracking,In,Eye,"Horizontal Gaze, Vertical Gaze",EOG,Yes,2,Coarse,Yes,No,No,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=6),Sitting,Lab,Headphone,Research Prototype,No,No,"A headphone-type gaze detector for a full-time  wearable interface is proposed.; Our goal is a full-time wearable gaze detector that does  not obscure the user’s view, has wide measurement  range, and of course is cosmetically acceptable. Our  idea is a headphone-type EOG-based gaze detector.",,"Music Player, Device Control, AR/VR","Gaze interface, EOG, wearable, headphone, Kalman  filter","A headphone-type gaze detector for a full-time  wearable interface is proposed. It uses a Kalman filter  to analyze multiple channels of EOG signals measured  at the locations of headphone cushions to estimate  gaze direction. Evaluations show that the average  estimation error is 4.4° (horizontal) and 8.3° (vertical),  and that the drift is suppressed to the same level as in  ordinary EOG. The method is especially robust against  signal anomalies. Selecting a real object from among  many surrounding ones is one possible application of  this headphone gaze detector.",6,https://dl.acm.org/doi/abs/10.1145/1125451.1125655,DONE,,,"Selection, Position",2,coarse (15),absolute,,Yes,No,,,,,,,
[623],Simpson et al.,2008,Mouth,In,Teeth,Click,Accelerometer,Yes,1,Semantic,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"Here, we tested a new method in which small tooth-clicks were detected by an accelerometer contacting the side of the head. The resulting signals were paired with head tracking technology to provide combined cursor and button control.; The goal of this paper was to determine to what extent small tooth-clicks that could be produced by persons with severe upper extremity paralysis could be used as control signals for enabling computer access.",,"Accessibility, Device Control","Assistive technology, computer interfaces, spinal cord injury, tooth-click","People with severe upper limb paralysis use devices that monitor head movements to control computer cursors. The three most common methods for producing mouse button clicks are dwell-time, sip-and-puff control, and voice-recognition. Here, we tested a new method in which small tooth-clicks were detected by an accelerometer contacting the side of the head. The resulting signals were paired with head tracking technology to provide combined cursor and button control. This system was compared with sip-andpuff control and dwell-time selection. A group of 17 people with disabilities and ten people without disabilities tested each system by producing mouse clicks as inputs to two software programs. Tooth-click/head-mouse control was much faster than dwell-time control and not quite as fast as sip-and-puff control, but it was more reliable and less cumbersome than the latter.",7,https://ieeexplore.ieee.org/document/4473368,DONE,,,Selection,X,X,relative,,Yes,No,,,,,,,
[184],Tamaki et al.,2009,Hand Gestures and Location,Enabling,Hand,"Hold (Mid-Air), Tilt (Mid-Air), Close (Mid-Air), Open (Mid-Air), Point (Mid-Air)","Camera, Laser, Projector",Yes,4,"Semantic, Fine",No,Visual Attention,Yes,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=1),Standing,Lab,Earbud,Research Prototype,Yes,No,"Existing wearable hand gesture interaction devices are very bulky and cannot be worn in everyday life, because of the presence of a large visual feedback device.;  Although wearable hand gesture interaction devices have to be not only lightweight but also compact and inconspicuous, existing wearable hand gesture interaction devices are very bulky and cannot be worn in everyday life, because of the presence of visual feedback devices such as a head-mounted display.; We propose a small device; Brainy Hand which avoids the use of a display.",,"Music Player, Phone Calls, Device Input, Device Control","interaction device, input device, wearable, hand gesture, laser, audio feedback, projector","Existing wearable hand gesture interaction devices are very bulky and cannot be worn in everyday life, because of the presence of a large visual feedback device. In particular, an eyeglass-type head-mounted display is very large for constant usage. To solve this problem, we propose Brainy Hand, which is a simple wearable device that adopts laser line, or more specifically, a mini-projector as a visual feedback device. Brainy Hand consists of a color camera, an earphone, and a laser line or mini-projector. This device uses a camera to detect 3D hand gestures. The earphone is used for receiving audio feedback. In this study, we introduce several user interfaces using Brainy Hand. (e.g., music player, phone)",8,https://dl.acm.org/doi/abs/10.1145/1520340.1520649?casa_token=6gSF3eNSnpoAAAAA:CQWgiFfxP5gooWAqsVc4zg96VRrgY5i6-nh85Ta7eNyBjDuutYUhqRo5eGOiDyhDejo3WqiH2xrvHw,DONE,,,,,,,,,,,,,,,,
[624],Simpson et al.,2010,Mouth,In,Teeth,Click,Accelerometer,Yes,1,Semantic,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,The field of study aimed at providing computer access for individuals with disabilities is as old as the personal computer itself.1,,"Accessibility, Device Control","assistive technology, computer access, spinal cord injury, tetraplegia","Background. Computer access can play an important role in employment and leisure activities following spinal cord injury. The authors’ prior work has shown that a tooth-click detecting device, when paired with an optical head mouse, may be used by people with tetraplegia for controlling cursor movement and mouse button clicks. Objective. To compare the efficacy of tooth clicks to speech recognition and that of an optical head mouse to a gyrometer head mouse for cursor and mouse button control of a computer. Methods. Six able-bodied and 3 tetraplegic subjects used the devices listed above to produce cursor movements and mouse clicks in response to a series of prompts displayed on a computer.The time taken to move to and click on each target was recorded. Results. The use of tooth clicks in combination with either an optical head mouse or a gyrometer head mouse can provide hands-free cursor movement and mouse button control at a speed of up to 22% of that of a standard mouse.Tooth clicks were significantly faster at generating mouse button clicks than speech recognition when paired with either type of head mouse device. Conclusions. Tooth-click detection performed better than speech recognition when paired with both the optical head mouse and the gyrometer head mouse. Such a system may improve computer access for people with tetraplegia.",9,https://journals.sagepub.com/doi/10.1177/1545968309341647,DONE,,,,,,,,,,,,,,,,
[253],Gamper et al.,2011,Head Gestures and Pointing,Enabling,Head,Yaw,Microphone,Yes,1,Fine,Yes,Yes,No,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=3),Sitting,Lab,Headphone,Commercial,No,No,"To produce realistic auditory augmentation, knowledge about the head orientation of the user is often necessary. Conventional head tracking systems use e.g. cameras to track visibly distinct markers or inertial sensors to detect head movements.",,"Video Conference, AR/VR, Motion Tracking",N/A,"A head orientation tracking system using binaural headset microphones is proposed. Unlike previous approaches, the proposed method does not require anchor sources, but relies on speech signals of the wearers of the binaural headsets. From the binaural microphone signals, time of arrival (TOA) and time difference of arrival (TDOA) estimates are obtained. The tracking is performed using a particle filter integrated with a maximum likelihood estimation function. In a case study, the proposed method is used to track the head orientations of three conferees in a meeting scenario. With an accuracy of about 10 degrees, the proposed method is shown to outperform a reference method which achieves an accuracy of about 35 degrees.",10,https://hannesgamper.com/wp-content/papercite-data/pdf/gamper2011b.pdf,DONE,,,,,,,,,,,,,,,,
[23],Manabe & Fukumoto,2011,Ear and Earable,In,Hand,Tap (Earable),Speaker,Yes,2,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=6),Medium (N=6),No,No,No,No,No,Yes (N=6),No,"Sitting, Walking, Walking Stairs, Head Movement, Jumping",Lab,"Earbud, Headphone",Research Prototype,Yes,No,Headphones from input device to input-and-output device,"Expanding Interaction Methods, Enhancing Functionalities of Current Devices","Music Player, Device Control","Headphones, tap, input device, wearable","A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.",11,https://doi.org/10.1145/2047196.2047236,DONE,,,Selection,X,X,X,2,Yes,No,Coupled,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Present,54%
[85],Matsumura & Fukumoto,2012,Ear and Earable,Enabling,Wearable State,"Share (Earable), Wear (Earable)","Proximity Sensor, EMG",Yes,2,Semantic,Yes,Yes,No,No,High,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,"Earbud, Headphone",Research Prototype,Yes,Yes,Making traditional interfaces more intelligent,"Expanding Interaction Methods, Enhancing Functionalities of Current Devices",Music Player,"Earphones, implicit interaction, intelligent interface","We present universal earphones that use both a proximity sensor and a skin conductance sensor and we demonstrate several implicit interaction techniques they achieve by automatically detecting the context of use. The universal earphones have two main features. The first involves detecting the left and right sides of ears, which provides audio to either ear, and the second involves detecting the shared use of earphones and this provides mixed stereo sound to both earphones. These features not merely free users from having to check the left and right sides of earphones, but they enable them to enjoy sharing stereo audio with other people.",12,https://dl.acm.org/doi/abs/10.1145/2166966.2167025,DONE,,,,,,,,,,,,,,,,
[62],Sano et al.,2010,Face,In,Facial Expression,Smile,EMG,Yes,1,Semantic,Yes,Yes,Yes,No,Low,Medium,Low (N=3),N/A,No,No,No,No,No,Yes (N=3),No,Sitting,Lab,Earbud,Research Prototype,No,No,"We propose novel ways of interactions using biosignals. We developed a prototype earphone with three kinds of biosignal sensors: pulse wave, electromyogram (EMG) and acceleration sensors. Using this sensor system, we implemented three new applications",,"Music Player, Communication, Data Annotation","wearable sensor, elecromyogram, accelometer, pulse sensor, photoplethysmographm","We propose novel ways of interactions using biosignals. We developed a prototype earphone with three kinds of biosignal sensors: pulse wave, electromyogram (EMG) and acceleration sensors. Using this sensor system, we implemented three new applications - automatic music selection, tactile and visual communication and automatic metadata annotation. We also present results of preliminary subjective and objective evaluations.",13,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=194115d845f69c4ad23430dacc278f99e0cd5dcd,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[94],Tessendorf and Derleth,2012,Ear and Earable,In,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=21),Yes (N=21),No,Yes (N=21),No,No,"Walking, Walking Stairs, Head Movement, Jumping",Lab,Custom Device,Research Prototype,Yes,Yes,sensing and annotation device to unobtrusively capture head movements in real life situations; In this context the challenge arises to collect and annotate multimodal reference data to identify hearing situations to be improved and to train the multimodal classifier running within the HI.; unobtrusively capture head movements in real life situations; higher accuracy and saving time for the experimenter,,"Data Annotation, Accessibility",N/A,"In this work we present a newly developed earworn sensing and annotation device to unobtrusively capture head movements in real life situations. It has been designed in the context of developing multimodal hearing instruments (HIs), but is not limited to this application domain. The ear-worn device captures triaxial acceleration, rate of turn and magnetic field and features a one-button-approach for real-time data annotation through the user. The system runtime is over 5 hours at a sampling rate of 128 Hz. In a user study with 21 participants the device was perceived as comfortable and showed a robust hold at the ear. On the example of head acceleration data we perform unsupervised clustering to demonstrate the benefit of head movements for multimodal HIs. We believe the novel technology will help to push the boundaries of HI technology.",14,https://ieeexplore.ieee.org/abstract/document/6346464,DONE,,,Position,X,X,X,1,Yes,No,X,Not Present,High (N=21),Medium (N=21),High (N=21),Not Indicated,55%
[25],Akiyama et al.,2013,Actuation,Enabling,N/A,Thermal (Actuaction),N/A,N/A,1,Fine,Yes,Yes,No,N/A,High,High,N/A,N/A,No,Yes (N=8),No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,N/A,N/A,"Thermal sense plays a significant role in the human recognition of environments and influences human emotions. By employing thermal sense in the music experience, which also greatly affects human emotions, we have successfully created a new medium with an unprecedented emotional experience.; For these reasons, we believe that thermal sense possesses great potential to enhance emotions.",,"Music Player, AR/VR","Thermal display, Musical interface, Cross-modal interface","This report proposes a thermal media system, ThermOn, which enables users to feel dynamic hot and cold sensations on their body corresponding to the sound of music. Thermal sense plays a significant role in the human recognition of environments and influences human emotions. By employing thermal sense in the music experience, which also greatly affects human emotions, we have successfully created a new medium with an unprecedented emotional experience. With ThermOn, a user feels enhanced excitement and comfort, among other responses. For the initial prototype, headphone-type interfaces were implemented using a Peltier device, which allows users to feel thermal stimuli on their ears. Along with the hardware, a thermal-stimulation model that takes into consideration the characteristics of human thermal perception was designed. The prototype device was verified using two methods: the psychophysical method, which measures the skin potential response and the psychometric method using a Likert-scale questionnaire and open-ended interviews. The experimental results suggest that ThermOn (a) changes the impression of music, (b) provides comfortable feelings, and (c) alters the listener’s ability to concentrate on music in the case of a rock song. Moreover, these effects were shown to change based on the methods with which thermal stimuli were added to music (such as temporal correspondence) and on the type of stimuli (warming or cooling). From these results, we have concluded that the ThermOn system has the potential to enhance the emotional experience when listening to music.",15,https://dl.acm.org/doi/abs/10.1145/2493988.2494326,DONE,,,,,,,,,,,,,,,,
[20],Manabe et al.,2013,Eye-Tracking,In,Eye,Vertical Gaze,EOG,Yes,2,Semantic,Yes,No,No,No,Medium,Medium,N/A,N/A,No,Yes (N=6),No,No,No,No,Yes (N=6),Sitting,Lab,Earbud,Research Prototype,No,No,"In this paper, we focus on an earphone-based eye gesture input interface and propose conductive rubber eartip electrodes suitable for daily-use. Experiments with several prototypes were conducted to compare their characteristics.",,"Music Player, Accessibility","EOG, electrode, conductive rubber, earphone, eye gesture","An eartip made of conductive rubber that also realizes biopotential electrodes is proposed for a daily-use earphonebased eye gesture input interface. Several prototypes, each with three electrodes to capture Electrooculogram (EOG), are implemented on earphones and examined. Experiments with one subject over a 10 day period reveal that all prototypes capture EOG similarly but they differ as regards stability of the baseline and the presence of motion artifacts. Another experiment conducted on a simple eye-controlled application with six subjects shows that the proposed prototype minimizes motion artifacts and offers good performance. We conclude that conductive rubber with mixed Ag filler is the most suitable setup for daily-use.",16,http://dx.doi.org/10.1145/2493988.2494329,DONE,,,Selection,1,coarse (3),relative,,Yes,No,,,,,,,
[1],Lissermann et al.,2014,Ear and Earable,In,Hand,Touch (Ear),Capacitive Sensor,Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,Yes (N=27),No,No,No,No,No,No,"Sitting, Music",Lab,Custom Device,Research Prototype,Yes,No,decreasing the visual demand of interfaces,Enabling Hands-Free and/or Eyes-Free Control,"Music Player, Device Control, Gaming","Ear-based interaction, ear-worn, mobile interaction, eyes-free, device augmentation, touch, multi-touch","One of the pervasive challenges in mobile interaction is  decreasing the visual demand of interfaces towards eyes-free  interaction. In this paper, we focus on the unique affordances  of the human ear to support one-handed and eyes-free mobile  interaction. We present EarPut, a novel interface concept and  hardware prototype, which unobtrusively augments a variety  of accessories that are worn behind the ear (e.g. headsets  or glasses) to instrument the human ear as an interactive  surface. The contribution of this paper is three-fold. We  contribute (i) results from a controlled experiment with 27  participants, providing empirical evidence that people are  able to target salient regions on their ear effectively and  precisely, (ii) a first, systematically derived design space  for ear-based interaction and (iii) a set of proof of concept  EarPut applications that leverage on the design space and  embrace mobile media navigation, mobile gaming and smart  home interaction.",17,https://dl.acm.org/doi/10.1145/2686612.2686655,DONE,,,"Selection, Position",X,coarse (2-6),X,7,No,Yes,Coupled,Partly Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,28%
[93],Sahni et al.,2014,Mouth,In,Speech Apparatus,Silent Speech (Words),"Magnetometer, Proximity Sensor",No,11,Semantic,Yes,Yes,No,No,High,High,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"We address the problem of performing silent speech recognition where vocalized audio is not available (e.g. due to a user’s medical condition) or is highly noisy (e.g. during firefighting or combat).; In contrast, silent speech recognition systems [7] , which utilize parts of the human speech production system (rather than the audio produced), can provide significant benefits for individuals rendered incapable of intelligible speech. Such a system can potentially allow issuing voice commands to computing devices or speech generation through text-to-speech interfaces without audible commands generated by the user.",,"Silent Speech, Accessibility, Device Input","silent speech recognition, wearable computing, mobile interfaces","We address the problem of performing silent speech recognition where vocalized audio is not available (e.g. due to a user’s medical condition) or is highly noisy (e.g. during firefighting or combat). We describe our wearable system to capture tongue and jaw movements during silent speech. The system has two components: the Tongue Magnet Interface (TMI), which utilizes the 3-axis magnetometer aboard Google Glass to measure the movement of a small magnet glued to the user’s tongue, and the Outer Ear Interface (OEI), which measures the deformation in the ear canal caused by jaw movements using proximity sensors embedded in a set of earmolds. We collected a data set of 1901 utterances of 11 distinct phrases silently mouthed by six able-bodied participants. Recognition relies on using hidden Markov modelbased techniques to select one of the 11 phrases. We present encouraging results for user dependent recognition.",18,https://dl.acm.org/doi/10.1145/2634317.2634322,DONE,,,"Selection, Text",X,X,X,11,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,23%
[7],Bedri et al.,2015,Mouth,Enabling,Jaw,General Jaw Movement,Proximity Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,Yes (N=27),No,No,No,No,No,"Sitting, Speaking, Eating, Walking, Walking Stairs",Lab,Earbud,Research Prototype,No,No,Researchers tried to benefit from the features that headphones and earpieces hold to develop systems that will help monitoring human activities.; The motivation behind OEI was the ability to perform silent speech recognition by classifying motion patterns made by the tongue and lower jaw; Socially acceptable.  2. Comfortable of use for extended periods of time.  3. Non-intrusive (no persing or implantation).  4. Ease of donning and disrobing.,,"Silent Speech, Activity Recognition","Outer ear, jaw motion, proximity sensor, silent speech, food intake, jaw gestures","The human ear seems to be a rigid anatomical part with no apparent activity, yet many facial and body activity can be measured from it. Research apparatuses and commercial products have demonstrated the capability of monitoring hart rate, tongue activities, jaw motion and eye blinking from the ear. In this paper we describe the design and the implementation of the Outer Ear Interface (OEI) which utilizes a set of infrared proximity sensors to measure the deformation in the ear canal caused by the lower jaw movement. OEI has been used in different applications that requires tracking of jaw activity which includes silent speech recognition, jaw gesture detection and food intake monitoring.",19,https://dl.acm.org/doi/10.1145/2800835.2807933,DONE,,,,,,,,,,,,,,,,
[190],Bleichner et al.,2015,"Brain, Eye-Tracking",In,Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,Yes,No,Medium,Medium,High (N=12),N/A,No,No,No,No,No,Yes (N=12),No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"This study evaluates the signal quality obtained with an unobtrusive solution for EEG monitoring through the integration of miniaturized EEG ton-electrodes into both a discreet baseball cap and an individualized ear piece. We show that such mini electrodes located at scalp and ear locations can reliably record event related potentials in a P300 brain–computer–interface application.; Aiming toward concealed EEG recordings, we integrated identical miniaturized EEG electrodes into a baseball cap, into individualized silicone earpieces (Fig. 1) and placed additional electrodes behind and above the ear.; Our main goal was to investigate whether recordings taken from locations in and around the ear can be used for ERP recording and BCI.",,BCI-Application,"Ear EEG, miniaturized, P300 speller, ton-electrodes","Electroencephalography (EEG) allows the study of the brain–behavior relationship in humans. Most of what we have learned with EEG was through observing the brain–behavior relationship under well-controlled laboratory conditions. However, by reducing “normal” behavior to a minimum the ecological validity of the results can be limited. Recent developments toward mobile EEG solutions allow to study the brain–behavior relationship outside the laboratory in more natural situations. Besides mobility and robustness with respect to motion, mobile EEG systems should also interfere as little as possible with the participant’s behavior. For example, natural interaction with other people could be hindered when it is obvious that a participant wears an EEG cap. This study evaluates the signal quality obtained with an unobtrusive solution for EEG monitoring through the integration of miniaturized EEG ton-electrodes into both a discreet baseball cap and an individualized ear piece. We show that such mini electrodes located at scalp and ear locations can reliably record event related potentials in a P300 brain–computer–interface application.",20,https://physoc.onlinelibrary.wiley.com/doi/full/10.14814/phy2.12362,DONE,,,,,,,,,,,,,,,,
[92],Norton et al.,2015,"Brain, Eye-Tracking",In,Eye,"Vertical Gaze, Horizontal Gaze",EEG,Yes,2,Coarse,Yes,No,Yes,No,Medium,Medium,High (N=3),N/A,No,No,No,No,No,Yes (N=3),No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"Here we introduce a soft, foldable collection of electrodes in open, fractal mesh geometries that can mount directly and chronically on the complex surface topology of the auricle and the mastoid, to provide highfidelity and long-term capture of electroencephalograms in ways that avoid any significant thermal, electrical, or mechanical loading of the skin.; In this paper, we explore the surfaces of the outer ear (the auricle) and adjacent regions (the mastoid) as mounting locations for a type of ultrathin, foldable neural electrode platform that is capable of longterm, high-fidelity EEG recording of signals commonly used in BCI. The combined area of the auricle and mastoid represents a uni",,BCI-Application,"soft electronics, auricle integration, brain–computer interface, text speller","Recent advances in electrodes for noninvasive recording of electroencephalograms expand opportunities collecting such data for diagnosis of neurological disorders and brain–computer interfaces. Existing technologies, however, cannot be used effectively in continuous, uninterrupted modes for more than a few days due to irritation and irreversible degradation in the electrical and mechanical properties of the skin interface. Here we introduce a soft, foldable collection of electrodes in open, fractal mesh geometries that can mount directly and chronically on the complex surface topology of the auricle and the mastoid, to provide highfidelity and long-term capture of electroencephalograms in ways that avoid any significant thermal, electrical, or mechanical loading of the skin. Experimental and computational studies establish the fundamental aspects of the bending and stretching mechanics that enable this type of intimate integration on the highly irregular and textured surfaces of the auricle. Cell level tests and thermal imaging studies establish the biocompatibility and wearability of such systems, with examples of high-quality measurements over periods of 2 wk with devices that remain mounted throughout daily activities including vigorous exercise, swimming, sleeping, and bathing. Demonstrations include a text speller with a steadystate visually evoked potential-based brain–computer interface and elicitation of an event-related potential (P300 wave).",21,https://www.pnas.org/doi/full/10.1073/pnas.1424875112,DONE,,,,,,,,,,,,,,,,
[188],Wang et al.,2015,"Brain, Eye-Tracking",In,Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,No,No,Medium,Medium,Medium (N=4),N/A,No,No,No,No,No,Yes (N=4),No,Sitting,Lab,Earbud,Research Prototype,No,No,"The purpose of this study is to demonstrate an online steady-state visual evoked potential (SSVEP)-based BCI system using EarEEG. EarEEG is a novel recording concept where electrodes are embedded on the surface of earpieces customized to the individual anatomical shape of users’ ear. It has been shown that the EarEEG can be used to record SSVEPs in previous studies. However, a long distance between the visual cortex and the ear makes the signal-to-noise ratio (SNR) of SSVEPs acquired by the EarEEG relatively low.; This study aims to explore the feasibility of an online SSVEP-based BCI using EarEEG recordings.",,"BCI-Application, Device Control",N/A,"The purpose of this study is to demonstrate an online steady-state visual evoked potential (SSVEP)-based BCI system using EarEEG. EarEEG is a novel recording concept where electrodes are embedded on the surface of earpieces customized to the individual anatomical shape of users’ ear. It has been shown that the EarEEG can be used to record SSVEPs in previous studies. However, a long distance between the visual cortex and the ear makes the signal-to-noise ratio (SNR) of SSVEPs acquired by the EarEEG relatively low. Recently, filter bank- and training data-based canonical correlation analysis algorithms have shown significant performance improvement in terms of accuracy of target detection and information transfer rate (ITR). This study implemented an online four-class SSVEPbased BCI system using EarEEG. Four subjects participated in offline and online BCI experiments. For the offline classification, an average accuracy of 82.71±11.83 % was obtained using 4 seclong SSVEPs acquired from earpieces. In the online experiment, all subjects successfully completed the tasks with an average accuracy of 87.92±12.10 %, leading to an average ITR of 16.60±6.55 bits/min. The results suggest that EarEEG can be used to perform practical BCI applications. The EarEEG has the potential to be used as a portable EEG recordings platform, that could enable real-world BCI applications.",22,https://ieeexplore.ieee.org/abstract/document/7318845,DONE,,,,,,,,,,,,,,,,
[19],Weigel et al.,2015,Ear and Earable,Enabling,Hand,Touch (Earable),"Capacitive Sensor, Resistive Sensor",Yes,1,"Semantic, Coarse",No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Custom Device,Research Prototype,Yes,No,"The human skin is recognized as a promising input surface for interactions with mobile and wearable devices. However, it has been difficult to design and implement touch sensors that can be placed directly on the skin. We present iSkin, an input surface for mobile human-computer interaction on the human body.",Expanding Interaction Methods,"Device Input, Music Player","On-body input, Mobile computing, Wearable computing, Touch input, Stretchable Sensor, flexible sensor, Electronic skin","We propose iSkin, a novel class of skin-worn sensors for touch input on the body. iSkin is a very thin sensor overlay, made of biocompatible materials, and is flexible and stretchable. It can be produced in different shapes and sizes to suit various locations of the body such as the finger, forearm, or ear. Integrating capacitive and resistive touch sensing, the sensor is capable of detecting touch input with two levels of pressure, even when stretched by 30% or when bent with a radius of 0.5 cm. Furthermore, iSkin supports single or multiple touch areas of custom shape and arrangement, as well as more complex widgets, such as sliders and click wheels. Recognizing the social importance of skin, we show visual design patterns to customize functional touch sensors and allow for a visually aesthetic appearance. Taken together, these contributions enable new types of on-body devices. This includes finger-worn devices, extensions to conventional wearable devices, and touch input stickers, all fostering direct, quick, and discreet input for mobile computing.",23,https://dl.acm.org/doi/10.1145/2702123.2702391,DONE,,,"Selection, Position, Path",2,coarse,"absolute, relative",,Yes,Yes,,Present,,,,,
[63],Ashbrook et al.,2016,Mouth,In,Teeth,Click,Bone Conduction Microphone,Yes,5,Semantic,Yes,Yes,No,No,High,High,Low (N=20),Medium (N=20),No,No,No,No,No,Yes (N=20),No,"Sitting, Head Movement, Speaking",Lab,Custom Device,Commercial,Yes,No,"In this paper, we present Bitey, a system that detects and classifies tooth clicking sounds to allow hands-free input to an interface. Bitey allows a user to operate an interface instantly: to initiate a process such as a phone call, to respond to a notification, or to control an ongoing process such as music playback. Although other tooth click-based interfaces have been proposed previously (see Related Work), all detect only whether any pair of teeth was clicked; in contrast, Bitey greatly expands the interaction capabilities by classifying which pair of teeth was clicked.",,"Device Control, Device Input, Music Player, Phone Calls, Communication","Bio-acoustics, tooth input, gestures, wearable computing, subtle interfaces, audio interfaces","We present Bitey, a subtle, wearable device for enabling input via tooth clicks. Based on a bone-conduction microphone worn just above the ears, Bitey recognizes the click sounds from up to five different pairs of teeth, allowing fully handsfree interface control. We explore the space of tooth input and show that Bitey allows for a high degree of accuracy in distinguishing between different tooth clicks, with up to 94% accuracy under laboratory conditions for five different tooth pairs. Finally, we illustrate Bitey’s potential through two demonstration applications: a list navigation and selection interface and a keyboard input method.",24,http://dx.doi.org/10.1145/2935334.2935389,DONE,,,Selection,X,X,X,,Yes,No,,Not Present,,Medium (Derived),,,
[34],Laput et al.,2016,Ear and Earable,In,Wearable State,"Remove Earbud, Attach Earbud","Microphone, Speaker",Yes,2,Semantic,No,Yes,Yes,No,Medium,High,High (N=12),High (N=12),No,No,No,No,No,Yes (N=12),No,Music,"Office, University Building",Earbud,Commercial,Yes,No,We propose utilizing these ubiquitous sensors to bring novel sensing abilities to devices without extra or special hardware,,"Music Player, Phone Calls, Device Control","Acoustic sensing, mobile devices, interaction techniques, novel input","Devices can be made more intelligent if they have the ability to sense their surroundings and physical configuration. However, adding extra, special purpose sensors increases size, price and build complexity. Instead, we use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos, and conclude with an evaluation that quantifies their performance and demonstrates high accuracy.",25,https://dl.acm.org/doi/10.1145/2856767.2856812,DONE,,,Selection,X,X,X,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,54%
[69],Merrill et al.,2016,Brain,In,Brain Activity,Mental Gesture,EEG,Yes,5,Semantic,Yes,Yes,Yes,No,High,High,N/A,N/A,No,Yes (N=9),Yes (N=9),No,No,No,Yes (N=12),Sitting,Lab,Custom Device,Research Prototype,No,No,"In this paper, we investigate how EEG signals collected from the ear could be used for “gestural” control of a braincomputer interface (BCI). Specifically, we investigate the efficacy of a support vector classifier (SVC) in distinguishing between mental tasks, or gestures, recorded by a modified, consumer headset.",,"Device Control, BCI-Application",N/A,"While brain-computer interfaces (BCI) based on electroencephalography (EEG) have improved dramatically over the past five years, their inconvenient, headworn form factor has challenged their wider adoption. In this paper, we investigate how EEG signals collected from the ear could be used for “gestural” control of a braincomputer interface (BCI). Specifically, we investigate the efficacy of a support vector classifier (SVC) in distinguishing between mental tasks, or gestures, recorded by a modified, consumer headset. We find that an SVC reaches acceptable BCI accuracy for nine of the subjects in our pool (n=12), and distinguishes at least one pair of gestures better than chance for all subjects. User surveys highlight the need for longer-term research on user attitudes toward in-ear EEG devices, for discreet, non-invasive BCIs.",26,https://ieeexplore.ieee.org/abstract/document/7516246,DONE,,,,,,,,,,,,,,,,
[43],Nguyen et al.,2016,Eye-Tracking,Enabling,Eye,"Horizontal Gaze, Vertical Gaze",EOG,Yes,4,Semantic,Yes,No,No,No,Medium,Medium,N/A,N/A,No,Yes (N=8),No,No,No,No,No,Lying,Lab,Custom Device,Research Prototype,No,No,"This paper introduces LIBS, a light-weight and inexpensive wearable sensing system, that can capture electrical activities of human brain, eyes, and facial muscles with two pairs of custom-built flexible electrodes each of which is embedded on an off-the-shelf foam earplug.",,"Health, Accessibility, Device Control","LIBS, In-ear Wearables, Biosignals, Health Care, Sleep Staging","This paper introduces LIBS, a light-weight and inexpensive wearable sensing system, that can capture electrical activities of human brain, eyes, and facial muscles with two pairs of custom-built flexible electrodes each of which is embedded on an off-the-shelf foam earplug. A supervised nonnegative matrix factorization algorithm to adaptively analyze and extract these bioelectrical signals from a single mixed in-ear channel collected by the sensor is also proposed. While LIBS can enable a wide class of low-cost selfcare, human computer interaction, and health monitoring applications, we demonstrate its medical potential by developing an autonomous whole-night sleep staging system utilizing LIBS’s outputs. We constructed a hardware prototype from off-the-shelf electronic components and used it to conduct 38 hours of sleep studies on 8 participants over a period of 30 days. Our evaluation results show that LIBS can monitor biosignals representing brain activities, eye movements, and muscle contractions with excellent fidelity such that it can be used for sleep stage classification with an average of more than 95% accuracy.",27,http://dx.doi.org/10.1145/2994551.2994562,DONE,,,,,,,,,,,,,,,,
[17],Ando et al.,2017,"Head Gestures and Pointing, Face",In,"Head, Jaw","Facial Gesture (Mouth), Pitch, Roll, Yaw",Pressure Sensor,Yes,11,"Semantic, Coarse",Yes,Yes,Yes (Performance Loss),No,Medium,High,High (N=12),Medium (N=6),No,No,No,No,No,Yes (N=12),No,"Sitting, Music",Lab,Earbud,Research Prototype,No,No,"However, to do this, the user usually needs to use at least one hand and look at the smartphone’s screen to take the smartphone out of the pocket and control its music player.  To address this issue, some commercial earphones have adopted sensors to allow users to operate the connected smartphone",,"Music Player, Device Control","Jaw movement, mouth movement, facial movement, head movement, barometer, hands-free, eyes-free, earphones, outer ear interface, wearable computing","We present a jaw, face, or head movement (face-related movement) recognition system called CanalSense. It recognizes face-related movements using barometers embedded in earphones. We find that face-related movements change air pressure inside the ear canals, which shows characteristic changes depending on the type and degree of the movement. We also find that such characteristic changes can be used to recognize face-related movements. We conduct an experiment to measure the accuracy of recognition. As a result, random forest shows per-user recognition accuracies of 87.6% for eleven face-related movements and 87.5% for four Open Mouth levels.",28,https://dl.acm.org/doi/10.1145/3126594.3126649,DONE,,,"Selection, Orientation, Path",3,X,relative,11,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,36%
[206],Dim & Ren,2017,Actuation,Enabling,N/A,Vibration (Actuation),N/A,N/A,1,Coarse,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,Yes (N=43),No,No,No,No,No,"Walking, Standing","Lab, Outdoors",Custom Device,Research Prototype,N/A,N/A,"Despite the potential benefits, suitable body parts for wearable vibration devices have not been defined or evaluated until now. We conducted three experiments to identify suitable body parts in terms of perceivability, wearability and user body location preferences for vibration devices.; Thus, it is important to find the most suitable vibration positions to convey directional information when walking; these positions should be practical and comfortable to wear in real mobile contexts.",,"Navigation, Accessibility, Military","Navigation, Wearable vibration","Many studies have demonstrated the benefits of wearable vibration devices for walking navigation (Tsukada and Yasumura, 2004). Despite the potential benefits, suitable body parts for wearable vibration devices have not been defined or evaluated until now. We conducted three experiments to identify suitable body parts in terms of perceivability, wearability and user body location preferences for vibration devices. We tested vibration feedback on nine body parts (the ear, neck, chest, waist, wrist, hand, finger, ankle and foot). Experiment 1 and Experiment 2 were conducted in the lab and in real-world walking settings in order to find suitable body parts. Our results indicate that the finger, wrist, ear, neck and feet had the highest perceivability and user preferences. Experiment 3 was conducted to understand the practical usability of those vibration positions in walking navigation. Our study results suggested that the feet are not suitable locations for vibration feedback in walking navigation. Based on the study results, we present design implications and guidelines for wearable vibration devices.",29,http://dx.doi.org/10.1016/j.ijhcs.2016.08.002,DONE,,,,,,,,,,,,,,,,
[22],Kikuchi et al.,2017,Ear and Earable,In,Hand,Pull (Ear),Proximity Sensor,Yes,9,Semantic,No,Yes,Yes,No,Medium,Medium,Low (N=8),Medium (N=8),No,No,No,No,No,Yes (N=8),No,"Sitting, Walking",Lab,Earbud,Research Prototype,Yes,No,"It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an “eyes-free” interface.; exchange an increasing amount of information through their earphones.; a new type of input method is required.",,"Music Player, Device Control, Device Input","Earphone, Skin Deformation, Photo Reflective Sensor","In this paper, we propose EarTouch, a new sensing technology for ear-based input for controlling applications by slightly pulling the ear and detecting the deformation by an enhanced earphone device. It is envisioned that EarTouch will enable control of applications such as music players, navigation systems, and calendars as an “eyes-free” interface. As for the operation of EarTouch, the shape deformation of the ear is measured by optical sensors. Deformation of the skin caused by touching the ear with the fingers is recognized by attaching optical sensors to the earphone and measuring the distance from the earphone to the skin inside the ear. EarTouch supports recognition of multiple gestures by applying a support vector machine (SVM). EarTouch was validated through a set of user studies.",30,https://dl.acm.org/doi/10.1145/3098279.3098538,DONE,,,"Selection, Path",2,X,relative,9,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Partly Present,32%
[64],Maag et al.,2017,Mouth,In,Tongue,Press,Pressure Sensor,Yes,3,Semantic,Yes,Yes,No,No,High,High,Medium (N=1),N/A,No,No,No,No,No,Yes (N=1),No,"Sitting, Music, Head Movement",Lab,Earbud,Research Prototype,No,No,"In this work, we propose BARTON, a robust and low-power in-ear tongue movement sensing system using commercial offthe-shelf (COTS) barometers. BARTON directly measures the subtle air pressure fluctuations in the ear canal induced by facial muscle movements attached to the tongue. Due to the low power consumption (< 5 μA at 1 Hz sampling rate) and low sampling frequencies (≤ 50 Hz) of the barometers BARTON is able to accurately detect tongue movements with lower computational and power efforts compared to microphone based systems. Since a typical barometer is more sensitive to low frequencies, it is naturally resilient to various audio interference such as music and speech. In addition, due to their small form factors, barometers can be discreetly integrated into headphones and earpieces for complete invisibility.",,"Music Player, Device Input","Human computer interaction, Ubiquitous computing, Pressure sensors","Sensing tongue movements enables various applications in hands-free interaction and alternative communication. We propose BARTON, a BARometer based low-power and robust TONgue movement sensing system. Using a low sampling rate of below 50 Hz, and only extracting simple temporal features from in-ear pressure signals, we demonstrate that it is plausible to distinguish important tongue gestures (left, right, forward) at low power consumption. We prototype BARTON with commodity earpieces integrated with COTS barometers for in-ear pressure sensing and an ARM micro-controller for signal processing. Evaluations show that BARTON yields 94% classification accuracy and 8.4 mW power consumption, which achieves comparable accuracy, but consumes 44 times lower energy than the stateof-the-art microphone-based solutions. BARTON is also robust to head movements and operates with music played directly from earphones.",31,https://ieeexplore.ieee.org/document/8368342,DONE,,,Selection,2,coarse (3),relative,,Yes,No,,,,,,,
[12],Matthies et al.,2017,"Face, Head Gestures and Pointing",In,"Facial Expression, Head","Smile, Facial Gesture (Mouth), Facial Gesture (Eye), Pitch, Yaw","EMG, Capacitive Sensor, Electrical Field Sensing",Yes,15,Semantic,Yes,Visual Attention,No,No,Low,Low,Low (N=1),Medium (N=3),No,No,No,No,No,Yes (N=1),No,Sitting,Lab,"Earbud, Custom Device",Research Prototype,No,No,"EarFieldSensing (EarFS) is a novel input method for mobile and wearable computing using facial expressions.; Therefore, we make use of a facial expression control in the manner of microinteractions »...because they may minimize interruption; that is, they allow for a tiny burst of interaction with a device so that the user can quickly return to the task at hand.",,Device Input,"Electric field sensing, body potential sensing, facial expression control, wearable computing, hands-fee, eyes-free","EarFieldSensing (EarFS) is a novel input method for mobile and wearable computing using facial expressions. Facial muscle movements induce both electric field changes and physical deformations, which are detectable with electrodes placed inside the ear canal. The chosen ear-plug form factor is rather unobtrusive and allows for facial gesture recognition while utilizing the close proximity to the face. We collected 25 facial-related gestures and used them to compare the performance levels of several electric sensing technologies (EMG, CS, EFS, EarFS) with varying electrode setups. Our developed wearable fine-tuned electric field sensing employs differential amplification to effectively cancel out environmental noise while still being sensitive towards small facial-movement-related electric field changes and artifacts from ear canal deformations. By comparing a mobile with a stationary scenario, we found that EarFS continues to perform better in a mobile scenario. Quantitative results show EarFS to be capable of detecting a set of 5 facial gestures with a precision of 90% while sitting and 85.2% while walking. We provide detailed instructions to enable replication of our low-cost sensing device. Applying it to different positions of our body will also allow to sense a variety of other gestures and activities.",32,https://dl.acm.org/doi/10.1145/3025453.3025692,DONE,,,"Selection, Path, Orientation",2,X,relative,15,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,18%
[111],Wang et al.,2017,Eye-Tracking,In,Eye,"Vertical Gaze, Blink","EOG, EMG",Yes,3,Semantic,Yes,No,No,No,Medium,Medium,High (N=20),N/A,No,No,No,No,No,Yes (N=20),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"With the developed machine learning algorithms, we can achieve over 97% accuracy of the classified various human eye-facial gestures, which allows us to build up intuitive Human-Machine Interaction (HMI) strategies to let disable people operate exoskeletons or wheelchairs easily and naturally.; The ultimate goal of this research is to seamlessly merge human and machine all together by just using a simple wearable device.; To solve the above problems, we propose to use physiological signals associated with eye saccades, blinks and other facial expressions to replace a BCI interface.",,"Device Control, Accessibility",N/A,"This paper proposed a method using eye saccade and facial expression physiological signals to interpret human intentions as a mean to operate wearable robots. Our approach used only two electrodes placed on top of the left and right ears to identify high fidelity physiological signals. With the developed machine learning algorithms, we can achieve over 97% accuracy of the classified various human eye-facial gestures, which allows us to build up intuitive Human-Machine Interaction (HMI) strategies to let disable people operate exoskeletons or wheelchairs easily and naturally. This method also enables the design of earbud-like wearable device, which can be worn comfortably for long hours to provide ubiquitous controllability to operate external smart devices. The ultimate goal of this research is to seamlessly merge human and machine all together by just using a simple wearable device.",33,https://ieeexplore.ieee.org/abstract/document/8383845,DONE,,,Selection,2,coarse (4),relative,,Yes,No,,,,,,,
[117],Ahn et al.,2018,"Brain, Eye-Tracking",In,Eye,"Horizontal Gaze, Vertical Gaze",EEG,Yes,2,Coarse,Yes,No,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=6),Sitting,Lab,Custom Device,Research Prototype,No,No,"To utilise BCIs in daily life, however, these are critical issues to address. Hence, a wearable EEG device for in-ear SSVEP detection is proposed.",,"Device Control, BCI-Application",N/A,"Steady-state visual evoked potential (SSVEP) has been widely used in electroencephalogram (EEG)-based brain–computer interfaces (BCIs) due to its high information transfer rate (ITR) and short training time. Current methods usually measure SSVEP from electrodes on the scalp, which is an uncomfortable and time-consuming method. Furthermore, most research relies on expensive and non-portable EEG devices. To utilise BCIs in daily life, however, these are critical issues to address. Hence, a wearable EEG device for in-ear SSVEP detection is proposed. The system is 40 × 21 × 10.5 mm3 and weighs 14.2 g, thus being light weight and wearable. Moreover, the system has a noise level of 0.11 μVrms, which is comparable with commercial EEG systems. Six subjects participated in an offline BCI experiment that consisted of six visual targets using the developed in-the-ear EEG system. The results showed a highest ITR of 11.03 ± 4.18 bits/min with an accuracy of 79.9 ± 13.1%, and the experiments demonstrated that the system can be utilised for unobtrusive monitoring of SSVEP in BCI applications.",34,https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/el.2017.3970,DONE,,,,,,,,,,,,,,,,
[177],Carioli et al.,2018,Mouth,Enabling,Jaw,General Jaw Movement,Piezoelectric Sensor,Yes,1,Fine,Yes,Yes,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=4),Sitting,Lab,Earbud,Research Prototype,No,No,"Direct measurement of the earcanal deformation is particularly challenging due to the non-uniform shape of the earcanal, the complex strains exhibited and the variation of earcanal shape and deformation among individuals.; ",,Motion Tracking,"Bending of curved surfaces, earcanal deformation, piezoelectric sensor","The earcanal shape is unique for each human being and temporarily changes when the jaw moves due to eating, chewing, or speaking. The earcanal deformation can be studied by the geometrical analysis of a distorted earpiece custom-fitted inside the earcanal, but the distortion of the earpiece is complex in nature and complicated to analyze. An earcanal bending sensor consisting of a thin piezoelectric strip attached to a customfitted earpiece is presented in this paper. An analytical approach based on computing the geometrical parameters of distorted and undistorted earpieces is developed to: 1) estimate the average bending moment and the resulting stress applied to the customfitted earpiece while opening the jaw and 2) calculate the sensitivity of the piezoelectric earcanal bending sensor. The theoretical model is experimentally validated. The proposed approach can be applied to measure the bending of any curved body in general, and custom-fitted earpieces in particular. It, therefore, enables the designing of versatile in-ear sensors capable of tracking jaw activity and evaluating the energy capacity of earcanal deformation for in-ear energy harvesting purposes.",35,https://ieeexplore.ieee.org/document/8194832,DONE,,,,,,,,,,,,,,,,
[91],Favre-Felix et al.,2018,Eye-Tracking,In,Eye,Vertical Gaze,EOG,Yes,1,Coarse,Yes,No,No,No,Medium,Medium,Low (N=7),N/A,No,No,No,No,No,Yes (N=7),No,Sitting,Lab,Custom Device,Commercial,Yes,No,"Previous studies have shown that eye gaze can be measured through electrooculography (EOG). To explore the precision and real-time feasibility of the methodology, seven hearing-impaired persons were tested, seated with their head fixed in front of three targets positioned at 30 , 0 , and þ30 azimuth.; The idea with the current experiment was to investigate whether eye-gaze steering via an EOG gaze-estimation algorithm would work under relatively easy dynamic conditions, given that the EOG gazeestimation algorithm might produce errors and thus classify wrongly eye-gaze directions. To achieve relatively easy dynamic conditions, the setup was using a target switching time of several seconds to assure that the test person had a stable gaze at the target, in a multitarget environment with several competing talkers.",,Accessibility,"eye tracking, electrooculography, hearing device, sound perception","The behavior of a person during a conversation typically involves both auditory and visual attention. Visual attention implies that the person directs his or her eye gaze toward the sound target of interest, and hence, detection of the gaze may provide a steering signal for future hearing aids. The steering could utilize a beamformer or the selection of a specific audio stream from a set of remote microphones. Previous studies have shown that eye gaze can be measured through electrooculography (EOG). To explore the precision and real-time feasibility of the methodology, seven hearing-impaired persons were tested, seated with their head fixed in front of three targets positioned at 30 , 0 , and þ30 azimuth. Each target presented speech from the Danish DAT material, which was available for direct input to the hearing aid using head-related transfer functions. Speech intelligibility was measured in three conditions: a reference condition without any steering, a condition where eye gaze was estimated from EOG measures to select the desired audio stream, and an ideal condition with steering based on an eye-tracking camera. The ‘‘EOG-steering’’ improved the sentence correct score compared with the ‘‘no-steering’’ condition, although the performance was still significantly lower than the ideal condition with the eye-tracking camera. In conclusion, eye-gaze steering increases speech intelligibility, although real-time EOG-steering still requires improvements of the signal processing before it is feasible for implementation in a hearing aid.",36,https://journals.sagepub.com/doi/10.1177/2331216518814388,DONE,,,Selection,1,coarse (3),absolute,,Yes,No,,,,,,,
[31],Huang et al.,2018,Actuation,Enabling,N/A,Deformation (Actuation),N/A,N/A,4,Semantic,Yes,Yes,Yes,N/A,Medium,Medium,N/A,N/A,Yes (N=10),Yes (N=20),No,No,Yes (N=20),No,No,Sitting,Café,Custom Device,Research Prototype,N/A,N/A,"In this paper, we propose using the auricle – the visible part of the ear – as a means of expressive output to extend body language to convey emotional states.; In this paper, we investigate the possibility of extending the vocabulary of human body language via the ear, an unused part of the body with limited mobility, but whose posture and movement has shown expressive meanings in other animals",,"Communication, Accessibility, Social Interaction","Actuating human body, wearable earpiece, auricle, body language, emotion","In this paper, we propose using the auricle – the visible part of the ear – as a means of expressive output to extend body language to convey emotional states. With an initial exploratory study, we provide an initial set of dynamic and static auricular postures. Using these results, we examined the relationship between emotions and auricular postures, noting that dynamic postures involving stretching the top helix in fast (e.g., 2Hz) and slow speeds (1Hz) conveyed intense and mild pleasantness while static postures involving bending the side or top helix towards the center of the ear were associated with intense and mild unpleasantness. Based on the results, we developed a prototype (called Orrechio) with miniature motors, custommade robotic arms and other electronic components. A preliminary user evaluation showed that participants feel more comfortable using expressive auricular postures with people they are familiar with, and that it is a welcome addition to the vocabulary of human body language.",37,https://doi.org/10.1145/3242587.3242629,DONE,,,,,,,,,,,,,,,,
[18],Lee et al.,2018,Ear and Earable,In,Hand,"Tap (Ear), Lift (Ear), Dwell (Ear), Joystick (Ear), Drag (Ear), Toggle (Ear)",Camera,Yes,6,"Semantic, Coarse",No,Yes,No,No,Medium,Medium,N/A,N/A,Yes (N=20),No,No,Yes (N=32),Yes (N=32),No,No,Sitting,"Café, Lab",Custom Device,Research Prototype,Yes,No,"However, while aspects such as tracking fidelity, display quality and computing power have advanced considerably to produce today’s high-end products, input and interaction technologies are less mature.; Recognizing the need for input systems for AR glasses that leave the hands unencumbered; We argue there is a need to improve our understanding of how users conceive of touches to the face as an input modality [31] to better inform future design and development efforts. Specifically, we argue that a key omission in our current understanding relates to the social acceptability [29] of facial touches - how comfortable users feel performing or observing this type of input in real life situations.",,"Device Input, AR/VR, Device Control","Hand-to-Face Input, Social Acceptability, User Elicitation, Augmented Reality, Head Mounted Display","Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable handto-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",38,https://dl.acm.org/doi/10.1145/3242587.3242642,DONE,,,"Selection, Position",1,"coarse (6), coarse (3), coarse (5)",absolute,~33,Yes,Yes,Coupled,Present (N=20),Not Indicated,Low (Derived),Not Indicated,Not Indicated,28%
[610],Min et al. ,2018,Head Gestures and Pointing,In,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,No,No,Medium,Medium,Low (N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"In this paper, we explore audio and kinetic sensing on earable devices with the commercial on-the-shelf form factor.",,"Health, Communication, Data Annotation, Emotion Recognition, Activity Recognition","Earable, Earbud, Audio sensing, Kinetic sensing","In this paper, we explore audio and kinetic sensing on earable devices with the commercial on-the-shelf form factor. For the study, we prototyped earbud devices with a 6-axis inertial measurement unit and a microphone. We systematically investigate the differential characteristics of the audio and inertial signals to assess their feasibility in human activity recognition. Our results demonstrate that earable devices have a superior signal-to-noise ratio under the influence of motion artefacts and are less susceptible to acoustic environment noise. We then present a set of activity primitives and corresponding signal processing pipelines to showcase the capabilities of earbud devices in converting accelerometer, gyroscope, and audio signals into the targeted human activities with a mean accuracy reaching up to 88% in varying environmental conditions.",39,https://dl.acm.org/doi/10.1145/3211960.3211970,DONE,,,"Selection, Path, Orientation",2,X,relative,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,22%
[65],Nguyen et al.,2018,Mouth,In,"Tongue, Teeth","Press, Grit","Capacitive Sensor, EEG, EMG",Yes,10,Semantic,Yes,Yes,No,No,High,High,Medium (N=15),N/A,No,Yes (N=9),No,No,No,Yes (N=15),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"We aim to develop a hands-free computer interface in which a user can privately interact with machines through a wearable device that he or she can comfortably wear and use in everyday life. To that end, we take the first step to build a tongue-on-teeth typing system, called TYTH (Typing on Your TeetH)1, a head-mounted device that can be worn from behind the user’s ears to capture the relative location and interaction between user’s tongue and teeth. This form of interaction is analogous to using a finger to type on a keypad except that the tongue substitutes for the finger and the teeth is for the keypad.; This work explores a new wearable system to capture the interaction between tongue and teeth from behind the user’s ears. We answer the following key questions in order to realize such a system: what kind of bio-signals can we capture from behind the ear that are valuable for sensing the tongue-teeth relationship? what are the best locations behind the ears to reliably sense those biosignals? how can we derive a technique to detect when a tongue taps on teeth? and how can we locate the area on the teeth that the tongue taps on?",,"Authentification, Accessibility, Privacy, Device Control, Driving, AR/VR","Human Computer Interaction, Tongue-Teeth Typing, Wearable Devices, Brain-Muscles Sensing, Skin Deformation Sensing","This paper explores a new wearable system, called TYTH, that enables a novel form of human computer interaction based on the relative location and interaction between the user’s tongue and teeth. TYTH allows its user to interact with a computing system by tapping on their teeth. This form of interaction is analogous to using a finger to type on a keypad except that the tongue substitutes for the finger and the teeth for the keyboard. We study the neurological and anatomical structures of the tongue to design TYTH so that the obtrusiveness and social awkwardness caused by the wearable is minimized while maximizing its accuracy and sensing sensitivity. From behind the user’s ears, TYTH senses the brain signals and muscle signals that control tongue movement sent from the brain and captures the miniature skin surface deformation caused by tongue movement. We model the relationship between tongue movement and the signals recorded, from which a tongue localization technique and tongue-teeth tapping detection technique are derived. Through a prototyping implementation and an evaluation with 15 subjects, we show that TYTH can be used as a form of hands-free human computer interaction with 88.61% detection rate and promising adoption rate by users.",40,https://doi.org/10.1145/3210240.3210322,DONE,,,Selection,3,coarse (11),absolute,,Yes,No,,,,,,,
[60],Taniguchi et al.,2018,Mouth,In,Tongue,Press,Proximity Sensor,Yes,1,Semantic,Yes,Yes,Yes,No,High,High,High (N=5),Medium (N=5),No,No,No,No,No,Yes (N=5),No,"Sitting, Chewing Gum, Walking",Lab,Earbud,Research Prototype,Yes,No,"In this study, our previously developed method for hands-free operation of a wearable device is applied to the operation of a PAP. Herein, we discuss the concept and working of the earphone-type interface named “earable TEMPO” that can be controlled via the movement of the tongue to start and stop the media on a PAP. In the evaluation experiment, the performance was evaluated while resting, gum chewing (mastication), and walking.",,Music Player,"ear canal, tongue, non-invasive, optical measurement, hand-free controller","In this study, an earphone-type interface named “earable TEMPO” was developed for hands-free operation, wherein the user can control the device by simply pushing the tongue against the roof of the mouth for about one second. This interface can be used to start and stop the music from a portable audio player. The earable TEMPO uses an earphone-type sensor equipped with a light emitting diode (LED) and a phototransistor to optically measure shape variations that occur in the external auditory meatus when the tongue is pressed against the roof of the mouth. To evaluate the operation of the earable TEMPO, experiments were performed on five subjects (men and women aged 22–58) while resting, chewing gum (representing mastication), and walking. The average accuracy was 100% while resting and chewing and 99% while walking. The precision was 100% under all conditions. The average recall value of the five subjects was 92%, 90%, and 48% while resting, masticating, and walking, respectively. All subjects were reliably able to perform the action of pressing the tongue against the roof of the mouth. The measured shape variations in the ear canal were highly reproducible, indicating that this method is suitable for various applications such as controlling a portable audio player.",41,https://www.mdpi.com/1424-8220/18/3/733,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[8],Amesaka et al.,2019,"Face, Head Gestures and Pointing, Mouth",In,"Facial Expression, Head","Facial Gesture (Mouth), Roll","Microphone, Speaker",Yes,5,Semantic,Yes,Yes,No,No,Medium,High,Medium (N=11),N/A,No,No,No,No,No,Yes (N=11),No,Sitting,Lab,Earbud,Research Prototype,No,No,"propose a new input method for mobile and wearable computing using facial expressions.; An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes.; Considering wearable computing environments, an advanced handsfree input method is required because our hands may be occupied in some situations such as cooking, walking with luggage, among other things. Speech recognition systems can be used as a handsfree input method; however, speaking in quiet public places is difficult, and audible sounds can be easily contaminated by environmental noise.",,"Device Control, Device Input, Music Player","facial expression recognition, ultrasound, ear canal transfer function, hearables","In this study, we propose a new input method for mobile and wearable computing using facial expressions. Facial muscle movements induce physical deformation in the ear canal. Our system utilizes such characteristics and estimates facial expressions using the ear canal transfer function (ECTF). Herein, a user puts on earphones with an equipped microphone that can record an internal sound of the ear canal. The system transmits ultrasonic band-limited swept sine signals and acquires the ECTF by analyzing the response. An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes. We investigated the performance of our proposed method for 21 facial expressions with 11 participants. Moreover, we proposed a signal correction method that reduces positional errors caused by attaching/detaching the device. The evaluation results confirmed that the f-score was 40.2% for the uncorrected signal method and 62.5% for the corrected signal method. We also investigated the practical performance of six facial expressions and confirmed that the f-score was 74.4% for the uncorrected signal method and 90.0% for the corrected signal method. We found the ECTF can be used for recognizing facial expressions with high accuracy equivalent to other related work.",42,https://dl.acm.org/doi/10.1145/3341163.3347747,DONE,,,"Selection, Orientation, Path",2,X,relative,20,Yes,No,X,Present (N=11),Not Indicated,Medium (Derived),Not Indicated,Present,31%
[80],Ferlini et al.,2019,Head Gestures and Pointing,Enabling,Head,Yaw,"Accelerometer, Gyroscope, Magnetometer",Yes,1,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=10),"Eating, Speaking, Standing",Lab,Earbud,Commercial,Yes,No,"Earables, like the eSense platform we evaluate in this paper, have an enormous and mostly unexploited potential. For instance, if adopted as hearing-aids, earables could be used both as sensors and actuators.; In this work, we focus on the evaluation of the eSense platform [7, 12] in tracking the head movements of a user concentrating on a specific spatial point.; By tracking instantaneous head movements as a proxy to track visual attention, our study shows how a system, that relies only on accelerometer and gyroscope, can still provide useful insights on where a person is facing.; his paper lays the foundations of a line of work aiming to sense and characterize human attention through earables, wearables that are neither socially-awkward, nor cumbersome, unusable or unrealistic (e.g. combining an hearing-aid with a pair of eye-tracking glasses). Lastly, it sheds light on how a magnetometer would behave if placed in an earable.",,"Accessibility, Motion Tracking","Earables, Head Motion Tracking, Visual Attention","Head tracking is a fundamental component in visual attention detection, which, in turn, can improve the state of the art of hearing aid devices. A multitude of wearable devices for the ear (so called earables) exist. Current devices lack a magnetometer which, as we will show, represents a big challenge when one tries to use them for accurate head tracking. In this work we evaluate the performance of eSense, a representative earable device, to track head rotations. By leveraging two different streams (one per earbud) of inertial data (from the accelerometer and the gyroscope), we achieve an accuracy up to a few degrees. We further investigate the interference generated by a magnetometer in an earable to understand the barriers to its use in these types of devices.",43,https://dl.acm.org/doi/10.1145/3345615.3361131,DONE,,,,,,,,,,,,,,,,
[77],Hoelzemann et al.,2019,Ear and Earable,In,Hand,Press (Earable),Button,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,Yes (N=7),No,No,No,No,No,"Sitting, Walking, Walking Stairs, Sports","Lab, Sports Site",Earbud,Commercial,Yes,No,"Wearable activity recognition research needs benchmark data, which rely heavily on synchronizing and annotating the inertial sensor data, in order to validate the activity classifiers. Such validation studies become challenging when recording outside the lab, over longer stretches of time.; It would allow the users to annotate their data without much effort in a socially comfortable way, which also enables ’in the wild’ experiments as study volunteers annotate activities in their daily lives.",,Data Annotation,N/A,"Wearable activity recognition research needs benchmark data, which rely heavily on synchronizing and annotating the inertial sensor data, in order to validate the activity classifiers. Such validation studies become challenging when recording outside the lab, over longer stretches of time. This paper presents a method that uses an inconspicuous, earworn device that allows the wearer to annotate his or her activities as the recording takes place. Since the ear-worn device has integrated inertial sensors, we use cross-correlation over all wearable inertial signals to propagate the annotations over all sensor streams. In a feasibility study with 7 participants performing 6 different physical activities, we show that our algorithm is able to synchronize signals between sensors worn on the body using cross-correlation, typically within a second. A comfort rating scale study has shown that attachment is critical. Button presses can thus define markers in synchronized activity data, resulting in a fast, comfortable, and reliable annotation method.",44,https://dl.acm.org/doi/10.1145/3345615.3361136,DONE,,,Position,X,X,X,1,Yes,No,X,Not Present,Medium (N=7),High (Derived),Not Indicated,Not Indicated,31%
[106],Lee et al.,2019,Actuation,Enabling,N/A,Vibration (Actuation),N/A,N/A,40,"Coarse, Semantic",Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,Yes (N=14),No,No,No,No,Walking,Lab,Custom Device,Research Prototype,N/A,N/A,"With three studies and a total of 38 participants, we suggest the design of ActivEarring, a ear-worn device capable of imparting information by stimulating six different locations on both ears.; Our goal with the research presented in this paper is to understand the human perception of tactile stimuli presented on the ears, and based on these findings develop and test the prototype of a ear-worn haptic display, named ActivEarring.",,"Navigation, Accessibility, AR/VR, Music Player, Gaming","Ear, haptics, wearable, digital jewelry, spatiotemporal patterns, smart earring","The symmetric configuration and the sensitivity of ears in addition to the long tradition of earrings as adornment open up the possibility for smart ear-worn devices. Taking advantage of these attributes, past research mostly focused on creating novel unobtrusive sensing input devices and auditory displays placed on the ear. Meanwhile, the tactile sensitivity of the ear has long been overshadowed by its auditory capacity, presenting the opportunity to investigate how ears can be exploited for unobtrusive tactile information transfer. With three studies and a total of 38 participants, we suggest the design of ActivEarring, a ear-worn device capable of imparting information by stimulating six different locations on both ears. We evaluated the performance of ActivEarring in a semi-realistic mobile condition and its practical use for information transfer with spatiotemporal patterns. Finally, we demonstrate that ActivEarring can be incorporated in common jewelry design, and present three applications that illustrate promising usage scenarios.",45,https://ieeexplore.ieee.org/document/8750808,DONE,,,,,,,,,,,,,,,,
[14],Lee et al.,2019,Face,In,Facial Expression,"Smile, Facial Gesture (Other)","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,Yes,No,Low,Medium,Medium (N=9),Medium (N=9),No,No,No,No,No,Yes (N=9),No,"Sitting, Walking, Standing",Lab,Earbud,Commercial,No,No,"In this paper, we introduce inertial signals obtained from an earable placed in the ear canal as a new compelling sensing modality for recognising two key facial expressions: smile and frown.",,"Music Player, Phone Calls","smile recognition, frown recognition, earable, FACS, kinetic modeling","In this paper, we introduce inertial signals obtained from an earable placed in the ear canal as a new compelling sensing modality for recognising two key facial expressions: smile and frown. Borrowing principles from Facial Action Coding Systems, we first demonstrate that an inertial measurement unit of an earable can capture facial muscle deformation activated by a set of temporal microexpressions. Building on these observations, we then present three different learning schemes - shallow models with statistical features, hidden Markov model, and deep neural networks to automatically recognise smile and frown expressions from inertial signals. The experimental results show that in controlled non-conversational settings, we can identify smile and frown with high accuracy (F1 score: 0.85).",46,https://doi.org/10.1145/3311823.3311869,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[2],Nasser et al.,2019,Actuation,Enabling,N/A,Thermal (Actuaction),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,N/A,This paper introduces a novel concept of an earable device that can provide thermal (hot and cold) haptic cues at multiple areas around the ear.; We conducted a pilot study using a proof-of-concept prototype and confirmed the feasibility of the concept.,,"Accessibility, AR/VR","Earable interfaces, thermal earable, thermal wearable, earable display","Earables/hearables (ear-worn devices) are on the rise as nextgeneration wearables which are capable of streaming audio, modifying soundscapes, and collecting body vitals. This paper introduces a novel concept of an earable device that can provide thermal (hot and cold) haptic cues at multiple areas around the ear. The concept with discrete thermal encoding can help the hearing and visually impaired individuals to obtain directional and other notifications from mobile and IoT devices. Embedding thermal haptic feedback into an earable form factor have a better edge in terms of privacy of notifications when compared to audio, visual, and vibrohaptic modalities. We conducted a pilot study using a proof-of-concept prototype and confirmed the feasibility of the concept.",47,https://doi.org/10.1145/3308561.3354636,DONE,,,,,,,,,,,,,,,,
[78],Odoemelem et al.,2019,Head Gestures and Pointing,In,Head,"Roll, Pitch","Accelerometer, Gyroscope",Yes,2,Fine,Yes,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,No,N/A,N/A,Earbud,Commercial,Yes,No,"We present an especially minimal and low-cost solution that uses the eSense [1] ear-worn prototype as a small head-worn controller, enabling direct control of an inexpensive robot arm in the environment.; In contrast, we present here a human-robot interface that aims at being minimal in terms of size and costs, and intend to investigate the trade-offs that are caused by this minimalism in terms of accuracy and speed.",,"Device Control, Accessibility, Motion Tracking",N/A,"Head motion-based interfaces for controlling robot arms in real time have been presented in both medical-oriented research as well as human-robot interaction. We present an especially minimal and low-cost solution that uses the eSense [1] ear-worn prototype as a small head-worn controller, enabling direct control of an inexpensive robot arm in the environment. We report on the hardware and software setup, as well as the experiment design and early results.",48,https://dl.acm.org/doi/10.1145/3345615.3361138,DONE,,,"Path, Orientation, Selection",2,fine,absolute,2,No,Yes,Coupled,Not Present,Not Indicated,High (Derived),Not Indicated,Present,38%
[15],Shirota et al.,2019,Actuation,Enabling,N/A,Deformation (Actuation),N/A,N/A,1,Coarse,Yes,Yes,No,N/A,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,N/A,N/A,"This study demonstrates that by opening and closing the human pinna, we can change the direction of sound perceived by humans.; Through our research, we contribute to the search for new possibilities for humans through the opening and closing of the ears.",,"AR/VR, Navigation","Actuating human body, wearable pinna device, ear, sound illusion,sound direction","This study demonstrates that by opening and closing the human pinna, we can change the direction of sound perceived by humans. Each ear was independently transformed into a 100% open, 50% open, and 100% closed state, and all 9 combinations of these ear transformations were tested to evaluate the perceived direction of the sound output from 7 speakers placed 180 degrees around the subject. We demonstrate that by deforming the pinna, we could change the perception of the direction of sound, or make it illusory. We also found that except for 1 out of 7 speakers (or directions of sound), closing 100% of the ear on the side of the speaker where the sound is coming from and 50% of the ear on the other side of the speaker tends produce the most alteration to the perceived direction of sound.",49,https://doi.org/10.1145/3341163.3347725,DONE,,,,,,,,,,,,,,,,
[620],Vega Gálvez et al.,2019,Mouth,In,Teeth,Click,"Accelerometer, Gyroscope",Yes,4,Semantic,Yes,Yes,Yes,No,High,High,Medium,N/A,No,No,No,No,No,Yes,No,Sitting,Lab,Custom Device,Research Prototype,No,No,Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teethclicks for everyday human-computer interaction.,,Device Control,"Discreet Interfaces, Teeth Gestures, Hands-free Interaction, Microgestures","Byte.it is an exploration of the feasibility of using miniaturized, discreet hardware for teeth-clicking as hands-free input for wearable computing. Prior work has been able to identify teeth-clicking of different teeth groups. Byte.it expands on this work by exploring the use of a smaller and more discreetly positioned sensor suite (accelerometer and gyroscope) for detecting four different teethclicks for everyday human-computer interaction. Initial results show that an unobtrusive position on the lower mastoid and mandibular condyle can be used to classify teeth-clicking of four different teeth groups with an accuracy of 89%.",50,https://doi.org/10.1145/3290607.3312925,DONE,,,"Selection, Position",X,X,absolute,,Yes,No,,,,,,,
[27],Yan et al.,2019,Hand Gestures and Location,In,Hand,Cover (Face),Microphone,Yes,1,Semantic,No,Yes,No,No,Medium,Medium,High (N=12),Medium (N=12),No,Yes (N=12),Yes (N=12),Yes (N=17),Yes (N=17),Yes (N=12),No,Sitting,"Lab, Office",Earbud,Commercial,Yes,No,"introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking.;  However, there are two major challenges with voice input [32]. First, users worry about the privacy risks of disclosing their personal information while speaking. Second, they suffer the inconvenience of repeatedly speaking the wake-up word or pressing a button during multiple rounds of voice input. With PrivateTalk we can address these two issues simultane ously. With PrivateTalk, a user can perform voice input by covering the mouth on one side with a hand while speaking (the Hand-On-Mouth gesture) as shown in Figure 1.",,"AR/VR, Privacy, Device Input","Voice input, hand gesture","We introduce PrivateTalk, an on-body interaction technique that allows users to activate voice input by performing the Hand-On-Mouth gesture during speaking. The gesture is per formed as a hand partially covering the mouth from one side. PrivateTalk provides two benefits simultaneously. First, it enhances privacy by reducing the spread of voice while also concealing the lip movements from the view of other people in the environment. Second, the simple gesture removes the need for speaking wake-up words and is more accessible than a physical/software button especially when the device is not in the user’s hands. To recognize the Hand-On-Mouth gesture, we propose a novel sensing technique that leverages the differ ence of signals received by two Bluetooth earphones worn on the left and right ear. Our evaluation shows that the gesture can be accurately detected and users consistently like PrivateTalk and consider it intuitive and effective.",51,https://dl.acm.org/doi/10.1145/3332165.3347950,DONE,,,Position,X,X,X,1,Yes,No,Additional,Present,High (N=12),High (Derived),High (N=12),Not Indicated,65%
[223],Cao et al.,2020,Ear and Earable,Enabling,Wearable State,Wear (Earable),"Microphone, Speaker",Partly,1,Fine,Yes,Yes,Yes,No,High,High,N/A,N/A,No,No,No,No,No,No,Yes (N=5),"Distance, Obstacles, Walking",Lab,Earbud,Commercial,Yes,No,new modality of acoustic motion tracking using earphones; We believe this is an important step towards “earable” sensing. include earphones into the ecosystem of acoustic motion tracking.,,"Motion Tracking, AR/VR, Device Control, Device Input","Earable sensing, Earphone-based acoustic sensing, Motion tracking","Acoustic motion tracking is an exciting new research area with promising progress in the last few years. Due to the inherent low propagation speed in the air, acoustic signals have the unique advantage of fine sensing granularity compared to RF signals. Speakers and microphones nowadays are pervasively available in devices surrounding us, such as smartphones and voice-controlled smart speakers. Though promising, one fundamental issue hindering the adoption of acoustic-based motion tracking is that the positions of microphones and speakers inside a device are fixed, which greatly limits the flexibility of acoustic motion tracking. In this work, we propose a new modality of acoustic motion tracking using earphones. Earphone-based tracking mitigates the constraints associated with traditional smartphone-based tracking. With novel designs and comprehensive experiments, we show earphone-based motion tracking can achieve a great flexibility and a high accuracy at the same time. We believe this is an important step towards “earable” sensing.",52,https://dl.acm.org/doi/10.1145/3384419.3430730,DONE,,,,,,,,,,,,,,,,
[234],Chen et al.,2020,"Ear and Earable, Hand Gestures and Location",Elicitation,N/A,N/A,N/A,N/A,0,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,Yes (N=27),Yes (N=27),No,No,Yes (N=27),No,No,Sitting,Lab,N/A,N/A,N/A,N/A,"elicitation study, This paper aims to bring more understandings of gesture design.; To better understand how users’ physical- and social-comfort is impacted when interacting with smart devices via ear-based input, we conducted a user elicitation study from which results we compile a set of user-defined gestures.",,"Device Control, Device Input, Music Player, Phone Calls","Gestures, Ear-based Input, User-defined, Guessability, Think-aloud","The human ear is highly sensitive and accessible, making it especially suitable for being used as an interface for interacting with smart earpieces or augmented glasses. However, previous works on ear-based input mainly address gesture sensing technology and researcher-designed gestures. This paper aims to bring more understandings of gesture design. Thus, for a user elicitation study, we recruited 28 participants, each of whom designed gestures for 31 smart device-related tasks. This resulted in a total of 868 gestures generated. Upon the basis of these gestures, we compiled a taxonomy and concluded the considerations underlying the participants’ designs that also offer insights into their design rationales and preferences. Thereafter, based on these study results, we propose a set of user-defined gestures and share interesting findings. We hope this work can shed some light on not only sensing technologies of ear-based input, but also the interface design of future wearable interfaces.",53,https://dl.acm.org/doi/10.1145/3427314,DONE,,,,,,,,,,,,,,,,
[212],Chen et al.,2020,Face,In,Facial Expression,"Smile, Facial Gesture (Eye), Facial Gesture (Mouth), Facial Gesture (Other)",Camera,Yes,30,Semantic,Yes,Visual Attention,No,Yes,Low,Low,Medium (N=9),N/A,No,No,No,No,No,Yes (N=9),Yes (N=9),"Wearing Styles, Sitting, Mask, Glasses",Lab,"Earbud, Headphone",Research Prototype,No,No,"To overcome the above challenges, researchers have developed various wearable devices for facial expression recognition using sensing techniques such as acoustic interference, pressure sensing, electrical impedance tomography (EIT) and electromyography (EMG) [42, 43, 10]. Compared to the previously mentioned front-facing camera method, wearable devices are always mounted on the user.",,"Silent Speech, Emotion Recognition, Communication, Feedback System, Customer Analytics","Wearable computing, Deep Learning, Computer Vision, Facial Expression Reconstruction and Tracking, Ear Sensing, Emoji Recognition, Silent Speech","C-Face (Contour-Face) is an ear-mounted wearable sensing technology that uses two miniature cameras to continuously reconstruct facial expressions by deep learning contours of the face. When facial muscles move, the contours of the face change from the point of view of the ear-mounted cameras. These subtle changes are fed into a deep learning model which continuously outputs 42 facial feature points representing the shapes and positions of the mouth, eyes and eyebrows. To evaluate C-Face, we embedded our technology into headphones and earphones. We conducted a user study with nine participants. In this study, we compared the output of our system to the feature points outputted by a state of the art computer vision library (Dlib1) from a font facing camera. We found that the mean error of all 42 feature points was 0.77 mm for earphones and 0.74 mm for headphones. The mean error for 20 major feature points capturing the most active areas of the face was 1.43 mm for earphones and 1.39 mm for headphones. The ability to continuously reconstruct facial expressions introduces new opportunities in a variety of applications. As a demonstration, we implemented and evaluated C-Face for two applications: facial expression detection (outputting emojis) and silent speech recognition. We further discuss the opportunities and challenges of deploying C-Face in real-world applications.",54,http://dx.doi.org/10.1145/3379337.3415879,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[149],Kaveh et al.,2020,"Eye-Tracking, Brain",In,Eye,Blink,EEG,Yes,1,Semantic,Yes,No,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=3),Sitting,Lab,Custom Device,Research Prototype,No,No,"This is the first wireless in-ear EEG to our knowledge to incorporate a dry multielectrode, user-generic design. The user-generic ear EEG recorded a mean alpha modulation of 2.17, outperforming the state-of-the-art in dry electrode in-ear EEG systems.; To make in-ear EEG widely usable, this paper presents a wireless in-ear EEG recording platform (user-generic ear EEG). To fit a large number and wide range of users, the design is based on dry electrodes in a user-generic rather than individualized design.; This work presents a discreet ear EEG recording system with a user-generic earpiece and wireless neural recording module.",,"Device Input, BCI-Application","BCI, dry electrodes, ear EEG, EEG, user-generic, wireless neural recording","In the past few years it has been demonstrated that electroencephalography (EEG) can be recorded from inside the ear (in-ear EEG). To open the door to low-profile earpieces as wearable brain-computer interfaces (BCIs), this work presents a practical in-ear EEG device based on multiple dry electrodes, a user-generic design, and a lightweight wireless interface for streaming data and device programming. The earpiece is designed for improved ear canal contact across a wide population of users and is fabricated in a low-cost and scalable manufacturing process based on standard techniques such as vacuum forming, plasma-treatment, and spray coating. A 2.5 × 2.5 cm2 wireless recording module is designed to record and stream data wirelessly to a host computer. Performance was evaluated on three human subjects over three months and compared with clinical-grade wet scalp EEG recordings. Recordings of spontaneous and evoked physiological signals, eye-blinks, alpha rhythm, and the auditory steady-state response (ASSR), are presented. This is the first wireless in-ear EEG to our knowledge to incorporate a dry multielectrode, user-generic design. The user-generic ear EEG recorded a mean alpha modulation of 2.17, outperforming the state-of-the-art in dry electrode in-ear EEG systems.",55,https://ieeexplore.ieee.org/abstract/document/9115876,DONE,,,Selection,,,,,,,,,,,,,
[16],Prakash et al.,2020,Mouth,In,Teeth,"Click, Slide",Speaker,Yes,9,Semantic,Yes,Yes,No,No,High,High,Medium (N=18),Medium (N=18),No,No,No,No,No,Yes (N=18),No,"Walking, Head Movement, Cycling, Cooking, Device, Speaking, Sitting",Lab,Earbud,Commercial,Yes,No,"In fact, by analyzing the signals at the two earphones, we show the feasibility of also localizing teeth gestures, resulting in a human-to-machine interface.; This paper explores the possibility of sensing teeth-and-jaw motion using commercial off-the-shelf earphones.; This enables a new form of contact-less user interface where a user can scroll, click, type, pause, etc. simply using his/her teeth. Unlike other contact-less user interfaces like hand or body gestures, eye trackers, or voice activated interfaces, EarSense is non-invasive, secure, and maintains the privacy of the user, i.e., the command cannot be heard or seen by anyone.",,"Device Input, Accessibility, Authentification, Device Control, Health, Privacy","Earable, Teeth gestures, User Interface, Vibroacoustics, Headphones, Earphones","This paper finds that actions of the teeth, namely tapping and sliding, produce vibrations in the jaw and skull. These vibrations are strong enough to propagate to the edge of the face and produce vibratory signals at an earphone. By re-tasking the earphone speaker as an input transducer – a software modification in the sound card – we are able to sense teeth-related gestures across various models of ear/headphones. In fact, by analyzing the signals at the two earphones, we show the feasibility of also localizing teeth gestures, resulting in a human-to-machine interface. Challenges range from coping with weak signals, distortions due to different teeth compositions, lack of timing resolution, spectral dispersion, etc. We address these problems with a sequence of sensing techniques, resulting in the ability to detect 6 distinct gestures in real-time. Results from 18 volunteers exhibit robustness, even though our system – EarSense does not depend on per-user training. Importantly, EarSense also remains robust in the presence of concurrent user activities, like walking, nodding, cooking and cycling. Our ongoing work is focused on detecting teeth gestures even while music is being played in the earphone; once that problem is solved, we believe EarSense could be even more compelling.",56,https://doi.org/10.1145/3372224.3419197,DONE,,,Selection,2,coarse (2x3),absolute,,Yes,No,,,,,,,
[26],Xu et al.,2020,"Ear and Earable, Hand Gestures and Location",In,Hand,"Tap (Face), Tap (Ear), Slide (Face), Slide (Ear)",Microphone,Yes,8,Semantic,No,Yes,Yes,Yes,Medium,High,High (N=18),Medium (N=18),Yes (N=16),Yes (N=12),Yes (N=16),No,Yes (N=16),Yes (N=18),No,"Noise, Sitting",Lab,Earbud,Commercial,Yes,No,"This observation gives rise to EarBuddy, a novel eyes-free input system that detects gestures performed along users’ faces using wireless earbuds.; We propose EarBuddy, a novel eyes-free input technique supported by wireless earbuds without the need for hardware modification; gesture set for EarBuddy that is optimized for user preference and microphone detectability.",,"Music Player, Phone Calls, Communication, Device Input, AR/VR","Wireless earbuds, face and ear interaction, gesture recognition","Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy’s usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability.",57,https://dl.acm.org/doi/10.1145/3313831.3376836,DONE,,,"Selection, Position, Path",1,X,relative,27,Yes,No,X,Present (N=16),High (N=12),High (Derived),High (N=16),Present,83%
[611],Yang et al.,2020,Head Gestures and Pointing,In,Head,"Roll, Pitch, Yaw","Accelerometer, Gyroscope",No,1,Fine,Yes,Visual Attention,No,No,Medium,High,Medium (N=7),N/A,No,No,No,No,No,Yes (N=7),Yes (N=7),"Standing, Walking",University Building,Headphone,Commercial,Yes,No,This paper aims to use modern earphones as a platform for acoustic augmented reality (AAR).,,"AR/VR, Activity Recognition","Augmented Reality, Acoustics, Smart Earphones, Wearable Computing, Indoor Localization, Inertial Measurement Unit (IMU), Dead Reckoning, Motion Tracking, Sensor Fusion, Head Related Transfer Function (HRTF), Spatial Audio","This paper aims to use modern earphones as a platform for acoustic augmented reality (AAR). We intend to play 3D audio-annotations in the user’s ears as she moves and looks at AAR objects in the environment. While companies like Bose and Microsoft are beginning to release such capabilities, they are intended for outdoor environments. Our system aims to explore the challenges indoors, without requiring any infrastructure deployment. Our core idea is two-fold. (1) We jointly use the inertial sensors (IMUs) in earphones and smartphones to estimate a user’s indoor location and gazing orientation. (2) We play 3D sounds in the earphones and exploit the human’s responses to (re)calibrate errors in location and orientation. We believe this fusion of IMU and acoustics is novel, and could be an important step towards indoor AAR. Our system, Ear-AR, is tested on 7 volunteers invited to an AAR exhibition – like a museum – that we set up in our building’s lobby and lab. Across 60 different test sessions, the volunteers browsed different subsets of 24 annotated objects as they walked around. Results show that Ear-AR plays the correct audio-annotations with good accuracy. The user-feedback is encouraging and points to further areas of research and applications.",58,https://dl.acm.org/doi/10.1145/3372224.3419213,DONE,,,"Selection, Orientation",3,fine,absolute,1,No,Yes,Coupled,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Not Indicated,25%
[229],Fan et al.,2021,Ear and Earable,In,"Hand, Wearable State","Slide (Earable), Tap (Earable)",Pressure Sensor,Yes,2,Semantic,No,Yes,No,No,Medium,High,High (N=1),Medium (N=1),No,No,No,No,No,Yes (N=1),No,"Music, Sitting",Lab,Custom Device,Research Prototype,No,No,bringing intelligence to headphones.; We envision HeadFi can serve as a vital supplementary solution to existing smart headphone design by directly transforming large amounts of existing “dumb” headphones into intelligent ones.,,"Music Player, Device Control","Earable Computing, Wearable Devices, User Identification, Heartrate Monitoring, Touch Gesture Control, Voice Communication","Headphones continue to become more intelligent as new functions (e.g., touch-based gesture control) appear. These functions usually rely on auxiliary sensors (e.g., accelerometer and gyroscope) that are available in smart headphones. However, for those headphones that do not have such sensors, supporting these functions becomes a daunting task. This paper presents HeadFi, a new design paradigm for bringing intelligence to headphones. Instead of adding auxiliary sensors into headphones, HeadFi turns the pair of drivers that are readily available inside all headphones into a versatile sensor to enable new applications spanning across mobile health, user-interface, and context-awareness. HeadFi works as a plug-in peripheral connecting the headphones and the pairing device (e.g., a smartphone). The simplicity (can be as simple as only two resistors) and small form factor of this design lend itself to be embedded into the pairing device as an integrated circuit. We envision HeadFi can serve as a vital supplementary solution to existing smart headphone design by directly transforming large amounts of existing “dumb” headphones into intelligent ones. We prototype HeadFi on PCB and conduct extensive experiments with 53 volunteers using 54 pairs of non-smart headphones under the institutional review board (IRB) protocols. The results show that HeadFi can achieve 97.2%–99.5% accuracy on user identification, 96.8%–99.2% accuracy on heart rate monitoring, and 97.7%–99.3% accuracy on gesture recognition.",59,https://dl.acm.org/doi/10.1145/3447993.3448624,DONE,,,"Selection, Path",1,X,relative,2,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,38%
[247],Ferlini et al.,2021,Head Gestures and Pointing,Enabling,Head,Yaw,Magnetometer,No,1,Fine,Yes,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes,"Music, Head Movement, Walking, Standing","Lab, Outdoors",Earbud,Research Prototype,No,No,"We explore the feasibility of adding a built-in magnetometer in an earbud, presenting the first comprehensive study of the magnetic interference impacting the magnetometer when placed in an earable: both that caused by the speaker and by RF (music streaming and voice calls) are considered.; We explore how magnetometer signals are affected by magnetic disturbances expected in an earable, highlighting the need for good calibration. (ii) We propose a novel magnetometer calibration technique that leverages the user’s phone sensors (typically well calibrated). The algorithm can run in the background, without user intervention, providing a semi-continuous calibration. (iii) We evaluate the performance of the calibration framework, both in terms of accuracy and system performance. Further, we present a proof of concept study with a navigation application. (iv) We theoretically and practically assess the computational and energy efficiency of our approach, showing our approach is accurate yet computationally inexpensive, allowing its regular execution on a con",,"Navigation, Authentification",N/A,"Earables (in-ear wearables) are a new frontier in wearables. Acting both as leisure devices, providing personal audio, as well as sensing platforms, earables could collect sensor data for the upper part of the body, subject to fewer vibrations and random movement variations than the lower parts of the body, due to inherent damping in the musculoskeletal system. These data may enable application domains such as augmented/virtual reality, medical rehabilitation, and health condition screening. Unfortunately, earables have inherent size, shape, and weight constraints limiting the type and position of the sensors on such platforms. For instance, lacking a magnetometer in all earables reference platforms, earables lack reference points. Thus, it becomes harder to work with absolute orientations. Embedding magnetometers in earables is challenging, as these rely heavily on radio (mostly Bluetooth) communication (RF) and contain magnets for magnetic-driven speakers and docking. We explore the feasibility of adding a built-in magnetometer in an earbud, presenting the first comprehensive study of the magnetic interference impacting the magnetometer when placed in an earable: both that caused by the speaker and by RF (music streaming and voice calls) are considered. We find that appropriate calibration of the magnetometer removes the offsets induced by the magnets, the speaker, and the variable interference due to BT. Further, we present an automatic, user-transparent adaptive calibration that obviates the need for alternative, expensive, and error-prone manual, or robotics, calibration procedures. Our evaluation shows how our calibration approach performs under different conditions, achieving convincing results with errors below 3° for the majority of the experiments.",60,https://ieeexplore.ieee.org/abstract/document/9439112?casa_token=YXCX9klMfrsAAAAA:6sZ70_-2Tioadkkv_2a21sktU8J4HUREXtdAgsuFsizYlG2MPe9t_9-6XwPlhLbQiati-jmUtA,DONE,,,,,,,,,,,,,,,,
[258],Gashi et al.,2021,"Head Gestures and Pointing, Face",In,"Head, Facial Expression","Pitch, Yaw, Smile, Facial Gesture (Mouth)","Accelerometer, Gyroscope",Yes,4,Semantic,Yes,Yes,Yes,No,Low,Medium,Low (N=21),N/A,No,No,No,No,No,Yes (N=21),No,Sitting,Lab,Earbud,Commercial,No,No,"Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions.",,"Social Interaction, Feedback System, Activity Recognition, Video Conference","Earable Computing, Transfer learning, Head Gestures Detection, Facial Expressions Recognition, Hierarchical Classification","Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions. Several researchers have thus tackled the problem of automatically recognizing head gestures and facial expressions, mainly leveraging video data. In this paper, we instead consider inertial signals collected from unobtrusive, earmounted devices. We focus on typical activities performed during social interactions – head shaking, nodding, smiling, talking and yawning – and propose a hierarchical classification approach to discriminate them from each other. Further, we investigate whether the transfer of knowledge learned from publicly available datasets leads to further performance improvements. Our results show that the combined use of our hierarchical approach and transfer learning allows the classifier to discriminate head and mouth activities with an F1 score of 84.79, smile, talk and yawn with an F1 score of 45.42, and nodding and head shaking with an F1 score of 88.24, outperforming shallow classifiers by 2-9 percentage points.",61,https://dl.acm.org/doi/10.1145/3462244.3479921,DONE,,,"Path, Orientation, Selection",2,X,relative,4,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,23%
[217],Hashem et al.,2021,Head Gestures and Pointing,In,Head,Yaw,Bluetooth,No,1,Coarse,Yes,Visual Attention,No,No,Medium,High,High (N=1),Low (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Standing",Lab,Earbud,Commercial,Yes,No,"By harnessing this information, it is possible to lock on that device in a multi-device environment and allow only it to respond to user gestures without ambiguity.; Head orientation detection in a multi-device environment can help indicate which device a user is interacting with, and thus provide a seamless way of mapping user gestures to the intended device without riddling other devices with unintended gestures.; In this paper, we present Look&Lock: a ubiquitous and accurate head orientation tracking system for multi-device environments. Look&Lock leverages Bluetooth Low Energy (BLE) capabilities of earables and their interaction with other BLE-enabled devices to track head orientation and map commands to the intended device.; With the rate of technological advancements, we anticipate that earables will be more compact and present future opportunities to be leveraged beyond audio use.",,Device Control,"IoT, BLE, smart earphones, wearables, earables, earable computing, head orientation","We present Look&Lock: a novel ubiquitous system that utilizes BLE communication between smart appliances in the environment and earables worn by the user to track head orientation and determine which device she is looking at. By harnessing this information, it is possible to lock on that device in a multi-device environment and allow only it to respond to user gestures without ambiguity. Our system leverages commercial off-the-shelf earables to provide accurate training-free head orientation tracking that is robust in different room settings. We implement a prototype using Android phones and eSense, a commercially available multi-sensory personal earable device. Our evaluation shows that Look&Lock can correctly identify devices inside a user’s field of view with accuracy up to 100% and is robust to different configurations and room settings.",62,https://dl.acm.org/doi/10.1145/3446382.3448653,DONE,,,"Selection, Orientation",1,coarse (4),absolute,1,No,Yes,Coupled,Partly Present,Not Indicated,High (Derived),Not Indicated,Not Indicated,31%
[622],Islam et al.,2021,Head Gestures and Pointing,In,Head,"Pitch, Yaw","Accelerometer, Gyroscope",No,2,Semantic,Yes,Yes,N/A,No,Medium,Medium,High (N=6),N/A,No,No,No,No,No,Yes (N=6),No,Sitting,Lab,Earbud,Commercial,No,No,"Detecting head- and mouth-related human activities of elderly people are very important for nurse care centers. They need to track different types of activities of elderly people like swallowing, eating, etc., to measure the health status of elderly people. In this regard, earable devices open up interesting possibilities for monitoring personal-scale behavioral activities.; The objectives of this paper are as follows: collecting accelerometer and gyroscope data from eSense device through Bluetooth and mobile application; detecting headand mouth-related human activities alongside some other regular activities; using traditional machine learning and deep learning classifiers to detect activities and compare performances",,"Accessibility, Health, Activity Recognition","Health care, Earables, Wearable; Activity recognition, eSense","Detecting head- and mouth-related human activities of elderly people are very important for nurse care centers. They need to track different types of activities of elderly people like swallowing, eating, etc., to measure the health status of elderly people. In this regard, earable devices open up interesting possibilities for monitoring personal-scale behavioral activities. Here, we introduce activity recognition based on an earable device called ‘eSense’. It has multiple sensors that can be used for human activity recognition. ‘eSense’ has a 6-axis inertial measurement unit with a microphone and Bluetooth. In this paper, we propose an activity recognition framework using eSense device. We collect accelerometer and gyroscope sensor data from eSense device to detect head- and mouth-related activities along with other normal human activities. We evaluated the classification performance of the classifier using both accelerometer and gyroscope data. For this work, we develop a smartphone application for data collection from the eSense. Several statistical features are exploited to recognize head- and mouth-related activities (e.g., head nodding, head shaking, eating, and speaking), and regular activities (e.g., stay, walk, and speaking while walking). We explored different types of machine learning approaches like Convolutional Neural Network (CNN), Random Forest (RnF), K-Nearest Neighbor (KNN), Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), etc., for classifying activities. We have achieved satisfactory results. Our results show that using both accelerometer and gyroscope sensors can improve performance. We achieve accuracy of 80.45% by LDA, 93.34% by SVM, 91.92% by RnF, 91.64% by KNN, and 93.76% by CNN while we exploit both accelerometer and gyroscope sensor data together. The results demonstrate the prospect of eSense device for detecting human activities in various healthcare monitoring system.",63,https://link.springer.com/chapter/10.1007/978-981-15-8944-7_11,DONE,,,"Selection, Path, Orientation",2,X,relative,2,Yes,No,X,Partly Present,Not Indicated,High (Derived),Not Indicated,Present,26%
[226],Jin et al.,2021,Hand Gestures and Location,In,Hand,"Sign Language (Words), Sign Language (Sentences), Close (Mid-Air), Tap (Mid-Air)","Microphone, Speaker",Yes,74,Semantic,No,Yes,No,No,Low,High,Medium (N=8),High (N=2),No,No,No,No,No,Yes (N=8),No,"Standing, Distance, Orientation","Office, University Building, Outdoors",Headphone,Research Prototype,Yes,No,"We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures.; Many Deaf people live successful and productive lives using sign languages. However, it is still quite challenging for the Deaf group to communicate with the hearing population conveniently.",,Accessibility,"Acoustic sensing, sign language gesture recognition, earphones","We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians.",64,https://dl.acm.org/doi/10.1145/3463519,DONE,,,"Selection, Path, Text, Orientation",6,X,relative,74,Yes,No,Coupled,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Present,47%
[618],Kakaraparthi et al.,2021,Hand Gestures and Location,Enabling,Hand,Touch (Face),"EMG, Thermal Camera, Impedence Sensor",Yes,1,Coarse,No,Yes,Yes (Performance Loss),Yes,Medium,Medium,Medium (N=14),Medium (N=1),No,Yes (N=14),No,No,No,Yes (N=14),No,"Speaking, Drinking, Eating, Walking, Running",Office,Custom Device,Research Prototype,Yes,No,"Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention; We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas.; To facilitate changing face touching behavior, it is critical to monitor face touches accurately and continuously.; The monitoring results can also be integrated into awareness enhancement devices (e.g., phones, watches, headphones) to alert users when face touches occur via various in-situ feedback such as vibrations and sounds [7, 37].; However, existing systems only capture approaching hands rather than detecting actual touches.; FaceSense is the first system capable of detecting actual hand-to-face contact and identifying the facial zone of the touch.",,Health,"face touch detection, thermo-physiological sensing, multimodal deep learning","Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSense integrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch. Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model.",65,https://doi.org/10.1145/3478129,DONE,,,,,,,,,,,,,,,,
[213],Khanna et al.,2021,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),Accelerometer,Yes,9,Semantic,Yes,Yes,No,No,High,High,Medium (N=6),Medium (N=6),No,No,No,No,No,Yes (N=6),No,"Head Movement, Yawn, Music, Speaking, Sitting",Lab,Headphone,Research Prototype,Yes,No,"This paper explores a new wearable system, called JawSense, that enables a novel form of human-computer interaction based on unvoiced jaw movement tracking. JawSense allows its user to interact with computing machine just by moving their jaw.; Speech and touch-typing are the two most common input modalities for smart devices [10]. However, they pose challenges in many scenarios where touch-typing restricts hands-free operation and speech commands are not desirable.",,"Privacy, Device Control, Accessibility, Health","Unvoiced sound recognition, Wearable devices, Accelerometer sensing","This paper explores a new wearable system, called JawSense, that enables a novel form of human-computer interaction based on unvoiced jaw movement tracking. JawSense allows its user to interact with computing machine just by moving their jaw. We study the neurological and anatomical structure of the human cheek and jaw to design JawSense so that jaw movement can be reliably captured under the strong impact of noises from human artifacts. In particular, JawSense senses the muscle deformation and vibration caused by unvoiced speaking to decode the unvoiced phonemes spoken by the user. We model the relationship between jaw movements and phonemes to develop a classification algorithm to recognize nine phonemes. Through a prototyping implementation and evaluation with six subjects, we show that JawSense can be used as a form of hands-free and privacy-preserving human-computer interaction with 92% phoneme classification rate.",66,https://doi.org/10.1145/ 3446382.3448363,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[224],Laporte et al.,2021,Head Gestures and Pointing,In,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Yes,2,Semantic,Yes,Yes,Yes,No,Medium,Medium,Low (N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"Verbal and non-verbal activities convey insightful information about people’s affect, empathy, and engagement during social interactions. In this paper, we investigate the usage of inertial sensors to recognize verbal (e.g., speaking), non-verbal (e.g., head nodding, shaking) and other activities (e.g., eating, no movement).; Our motivation for detecting verbal and non-verbal activities is rooted in our work on building human memory augmentation systems. Using earable computing, we attempt to recognize different types of human activities, in particular head gestures, with the purpose of detecting when a social interaction is taking place. This is because the presence of others and our interactions with them play important roles in our memories, both during the formation of memory [32] and at retrieval time [6]: moments of social interactions might be easier to remember (formation time), and remembering a particular interaction might also help to remind us of particular details (retrieval time)",,"Social Interaction, Data Annotation","Datasets, Earable Computing, Head Gestures Recognition, Memory Recall","Verbal and non-verbal activities convey insightful information about people’s affect, empathy, and engagement during social interactions. In this paper, we investigate the usage of inertial sensors to recognize verbal (e.g., speaking), non-verbal (e.g., head nodding, shaking) and other activities (e.g., eating, no movement). We implement an end-to-end deep neural network to distinguish among these activities. We then explore the generalizability of the approach in three scenarios: (1) using new data to detect a known activity from a known user, (2) detecting a novel activity of a known user and (3) detecting the activity of an unknown user. Results show that using accelerometer and gyroscope sensors, the model achieves a balanced accuracy of 55% when tested on data from a new user, 41% on a new activity of an existing user, and 80% on new data of a known activity from an existing user. The results are between 7-47 percentage points higher than baseline classifiers.",67,https://dl.acm.org/doi/10.1145/3460418.3479322,DONE,,,"Path, Selection, Orientation",2,X,relative,2,Yes,No,X,Partly Present,Not Indicated,High (Derived),Not Indicated,Present,26%
[626],Ma et al.,2021,Hand Gestures and Location,In,Hand,Tap (Face),Microphone,Yes,5,Semantic,No,Yes,No,Yes,Medium,Medium,High (N=29),High (N=4),No,No,No,No,No,Yes (N=29),No,"Sitting, Music","Lab, Outdoors",Earbud,Research Prototype,Yes,No,"However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions.; Thus, in the interest of form factor and cost, we explore other alternatives for human motion sensing on earables. To achieve reliable detection of both intense and light human motions, we present OESense, a novel acoustic-based in-ear human motion sensing system.; With this work, we aim at developing a general earable sensing system for human motion detection.",,"Health, Activity Recognition, Motion Tracking",N/A,"Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3% step counting recall, 98.3% recognition recall for 5 activities, and 97.0% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds’ fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.",68,https://dl.acm.org/doi/10.1145/3458864.3467680,DONE,,,"Selection, Position",X,X,X,12,Yes,No,X,Partly Present (N=29),Not Indicated,Low (Derived),Not Indicated,Present,52%
[259],Nasser et al.,2021,Actuation,Enabling,N/A,Thermal (Actuaction),N/A,N/A,26,"Semantic, Coarse",Yes,Yes,Yes (Performance Loss),N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,N/A,"This paper investigates the design and the evaluation of thermal haptic feedback on an earable form factor with multiple thermoelectric (i.e. Peltier) modules. We propose ThermEarhook, a wearable device that can provide hot and cold stimuli at multiple points on the auricular skin area.; In this paper, we focus on integrating thermal haptic feedback in an earhook form factor for designing the wearable device. More specifically, we designed ThermEarhook as shown in Fig. 1, a wearable device that can provide hot and cold stimuli at multiple points on the auricular skin area.",,"Gaming, Navigation, Communication, Health, Accessibility","Thermotactile, earhook, spatial thermal pattern, earable","Haptic feedbacks are widely adopted in mobile and wearable devices to convey various types of notifications to the users. This paper investigates the design and the evaluation of thermal haptic feedback on an earable form factor with multiple thermoelectric (i.e. Peltier) modules. We propose ThermEarhook, a wearable device that can provide hot and cold stimuli at multiple points on the auricular skin area. To investigate users’ thermal perception on the auricular area, we develop a series of ThermEarhook prototypes with 3, 4, and 5 Peltier modules. While most existing research utilized the constant level of haptic signal for different users, our pilot study with ThermEarhook shows that the auricular thermohaptic threshold varies across the feedback locations and the users. With the user-customized thermohaptic signals around the ear, our first study with 12 participants reports on the selection of the auricular configuration with four TEC modules on each side, considering the users’ identification accuracy (averagely 99.3%) and preference. We then conduct three follow-up studies and a total of 36 participants to further evaluate users’ perception of spatial thermal patterns with ThermEarhook, and finalize a set of multi-points auricular thermal patterns that can be reliably perceived by the users with the average accuracy of 85.3%. Lastly, we discuss the user-proposed potential applications of the thermal haptic feedback with ThermEarhook.",69,https://doi.org/10.1145/3462244.3479922,DONE,,,,,,,,,,,,,,,,
[628],Peng,2021,Actuation,Enabling,N/A,Deformation (Actuation),N/A,N/A,1,Semantic,Yes,Yes,Yes,N/A,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Daily Life,Custom Device,Research Prototype,N/A,N/A,"We build on this and explore its intersection with the felds of biodata and playful expression by presenting “Wigglears”, a wearable system that will wiggle the wearer’s ears based on skin conductance, aiming to explore playful solutions towards integrated biodata-based selfexpression.; However, there is lack of research within the overlap of these areas, being integration in the form of biodatabased body augmentation, especially if aiming to support social interactions.; The question we wish to address is: How can computer systems be integrated for playful expression, and what are the social implications? We provide a proof of concept towards using body augmentation within social situations, and hope to further the research within the Human-Computer Integration paradigm with this knowledge.",,Social Interaction,"Biodata interfacing, playful expression","Human-Computer Integration is an extension of the HumanComputer Interaction paradigm that explores systems in which the boundary between user and computer is blurred. We build on this and explore its intersection with the felds of biodata and playful expression by presenting “Wigglears”, a wearable system that will wiggle the wearer’s ears based on skin conductance, aiming to explore playful solutions towards integrated biodata-based selfexpression. Through an autobiographical study, we demonstrate the system’s ability to fuel social dialogue, amplify positive emotions, and triggering refocus. We intend for our system to be a novel solution to expressing emotions within social interactions, and hope to ofer insights towards the social implications of biodatabased integration as a social cue, to help further the research within Human-Computer Integration.",70,https://doi.org/10.1145/3411763.3451846,DONE,,,,,,,,,,,,,,,,
[227],Röddiger et al.,2021,Ear and Earable,In,Tensor Tympani,Contract,Pressure Sensor,Yes,3,Semantic,Yes,Yes,Yes,No,High,High,High (N=16),Medium (N=16),No,Yes (N=8),Yes (N=16),No,No,Yes (N=16),No,"Sitting, Music",Lab,Earbud,Research Prototype,Yes,No,Input techniques using “subtle” or “motionless” input gestures are desirable in mobile contexts because they take into consideration the social context of mobile device usage; Input techniques with little to no movement avoid the inconvenience of techniques requiring large physical efort and are more socially acceptable to spectators,,"Music Player, Phone Calls","tensor tympani muscle, discreet interaction, subtle gestures, earables, hearables, in-ear barometry","We explore how discreet input can be provided using the tensor tympani - a small muscle in the middle ear that some people can voluntarily contract to induce a dull rumbling sound. We investigate the prevalence and ability to control the muscle through an online questionnaire (N=192) in which 43.2% of respondents reported the ability to “ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can be used to detect voluntary tensor tympani contraction in the sealed ear canal. This data was used to train a classifer based on three simple ear rumble “gestures” which achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction, grounded in three manual, dual-task application scenarios (N=8). This highlights the applicability of EarRumble as a low-efort and discreet eyes- and hands-free interaction technique that users found “magical” and “almost telepathic”.",71,https://dl.acm.org/doi/10.1145/3411764.3445205,DONE,,,Selection,X,X,X,4,Yes,No,X,Present (N=83),High (N=8),Low (Derived),High (N=99),Present,73%
[221],Sun et al.,2021,Mouth,In,Teeth,Click,"Gyroscope, Microphone",Yes,13,Semantic,Yes,Yes,Yes (Performance Loss),No,High,High,Medium (N=11),N/A,No,No,No,No,No,Yes (N=11),No,Sitting,Lab,Custom Device,Research Prototype,Yes,No,"Teeth gestures become an alternative input modality for different situations and accessibility purposes. In this paper, we present TeethTap, a novel eyes-free and hands-free input technique, which can recognize up to 13 discrete teeth tapping gestures.; The vast-majority of input techniques for mobile devices demands the use of hands as an input source, which may constraints user experiences. For example, it would be inconvenient for a user to interact with a smartwatch to reject a phone call while both hands are occupied (e.g. carrying objects [25]). Therefore, providing handsfree interactions may improve the wearable interactive experiences under different situational uses and provide additional input opportunities for accessibility purposes (e.g., people with motor impairments).",,"Music Player, Accessibility, Device Control, Phone Calls, AR/VR","Teeth Gestures, Eyes-free Input, Hands-free Input, Motion Sensing, Acoustic Sensing, Earpiece","Teeth gestures become an alternative input modality for different situations and accessibility purposes. In this paper, we present TeethTap, a novel eyes-free and hands-free input technique, which can recognize up to 13 discrete teeth tapping gestures. TeethTap adopts a wearable 3D printed earpiece with an IMU sensor and a contact microphone behind both ears, which works in tandem to detect jaw movement and sound data, respectively. TeethTap uses a support vector machine to classify gestures from noise by fusing acoustic and motion data, and implements K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW) distance measurement using motion data for gesture classification. A user study with 11 participants demonstrated that TeethTap could recognize 13 gestures with a real-time classification accuracy of 90.9% in a laboratory environment. We further uncovered the accuracy differences on different teeth gestures when having sensors on single vs. both sides. Moreover, we explored the activation gesture under real-world environments, including eating, speaking, walking and jumping. Based on our findings, we further discussed potential applications and practical challenges of integrating TeethTap into future devices.",72,https://doi.org/10.1145/3397481.3450645,DONE,,,Selection,3,coarse (13),absolute,,Yes,No,,,,,,,
[231],Verma et al.,2021,Face,In,Facial Expression,"Facial Gesture (Eye), Facial Gesture (Nose), Facial Gesture (Mouth), Facial Gesture (Other)","Accelerometer, Gyroscope",Yes,32,Semantic,Yes,Visual Attention,Yes (Performance Loss),Yes,Low,Low,Medium (N=12),High (N=3),No,No,No,No,No,Yes (N=12),No,"Noise, Sitting, Walking, Eating, Speaking","Train, Controlled Room",Earbud,Commercial,No,No,"Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements.; However, most of these modalities either fail to capture fine-grained facial movements or can’t be realised in the form of a practical system due to their uncomfortable form factor. This posits a clear need for a practical wearable sensing technology, preferably in the form-factor of a familiar ubiquitous device, that can continuously and unobtrusively monitor fine-grained facial expressions while preserving the privacy of the user.",,"AR/VR, Feedback System, Emotion Recognition, Video Conference, Device Control, Device Input, Accessibility","facial expressions, FACS, earable computing, IMU sensing","Continuous and unobtrusive monitoring of facial expressions holds tremendous potential to enable compelling applications in a multitude of domains ranging from healthcare and education to interactive systems. Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our approach and found that ExpressEar can detect and distinguish between 32 Facial AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for any given user. We further quantify the performance across different mobile scenarios in presence of additional face-related activities. Our results demonstrate ExpressEar’s applicability in the real world and open up research opportunities to advance its practical adoption.",73,https://doi.org/10.1145/3478085,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[616],Wu et al.,2021,"Face, Eye-Tracking, Mouth",Enabling,"Facial Expression, Speech Apparatus, Eye","Ekman 7, Silent Speech (Words), Horizontal Gaze, Vertical Gaze","EMG, EOG",Yes,21,Semantic,Yes,No,Yes,No,Low,Low,N/A,N/A,No,Yes (N=16),Yes (N=16),No,No,No,Yes (N=24),"Sitting, Mask, Head Movement, Time",Lab,Custom Device,Research Prototype,No,No,"In this paper, we propose the first single-earpiece lightweight biosensing system, BioFace3D, that can unobtrusively, continuously, and reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations.; System Objective and Challenges. To circumvent all the limitations of existing approaches, this paper aims to provide a wearable biosensing system that can unobtrusively, continuously, and reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations through fitting a 3D head model to the 2D facial landmarks. Although existing studies (e.g., [41, 58]) have shown the success of using biosensors, such as EMG and electrooculography (EOG), to detect facial muscle activities and eye movements, realizing such a system is still very challengin",,"Emotion Recognition, Accessibility, AR/VR, Feedback System, Driving, Silent Speech, Device Control","mobile computing, wearable sensing, 3D facial reconstruction, singleear biosensing","Over the last decade, facial landmark tracking and 3D reconstruction have gained considerable attention due to their numerous applications such as human-computer interactions, facial expression analysis, and emotion recognition, etc. Traditional approaches require users to be confined to a particular location and face a camera under constrained recording conditions (e.g., without occlusions and under good lighting conditions). This highly restricted setting prevents them from being deployed in many application scenarios involving human motions. In this paper, we propose the first single-earpiece lightweight biosensing system, BioFace3D, that can unobtrusively, continuously, and reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations. Our single-earpiece biosensing system takes advantage of the cross-modal transfer learning model to transfer the knowledge embodied in a high-grade visual facial landmark detection model to the low-grade biosignal domain. After training, our BioFace-3D can directly perform continuous 3D facial reconstruction from the biosignals, without any visual input. Without requiring a camera positioned in front of the user, this paradigm shift from visual sensing to biosensing would introduce new opportunities in many emerging mobile and IoT applications. Extensive experiments involving 16 participants under various settings demonstrate that BioFace-3D can accurately track 53 major facial landmarks with only 1.85 mm average error and 3.38% normalized mean error, which is comparable with most state-of-the-art camerabased solutions. The rendered 3D facial animations, which are in consistency with the real human facial movements, also validate the system’s capability in continuous 3D facial reconstruction.",74,https://doi.org/10.1145/3447993.3483252,DONE,,,,,,,,,,,,,,,,
[608],Alkiek et al.,2022,Hand Gestures and Location,In,Hand,"Approach (Mid-Air), Slide (Mid-Air), Recede (Mid-Air)",Bluetooth,Yes,8,Semantic,No,Yes,Yes,No,Low,Low,High (N=1),High (N=1),No,No,No,No,No,Yes (N=1),No,"Distance, Orientation, Sitting, Standing","Office, Living Room",Earbud,Research Prototype,Yes,No,"However, the limited interface of these devices, usually being a single button or force-sensor, inhibits their potential. Earables can provide a much richer experience by extending the ways in which users can interact with them. In this paper, we present EarGest, a novel earable-based hand gesture recognition system that does not require calibration or training, and works with commercially available BLE-enabled earphones and devices.; Our proposed system is unique in its ability to leverage Bluetooth to detect hand motion near the ear to recognize gestures. By harnessing information from BLE connections between the wireless earphones and a host device, we accurately detect and classify different hand gestures performed by users, while also determining discrete levels of hand speed.",,"Device Control, Music Player, Phone Calls","Earables, gesture recognition, HCI; sensing","Earables have been increasingly gaining attention from consumers and manufacturers alike due to their small footprint, ease of use, and the added accessibility they bring. However, the limited interface of these devices, usually being a single button or force-sensor, inhibits their potential. Earables can provide a much richer experience by extending the ways in which users can interact with them. In this paper, we present EarGest, a novel earable-based hand gesture recognition system that does not require calibration or training, and works with commercially available BLE-enabled earphones and devices. Our proposed system is unique in its ability to leverage Bluetooth to detect hand motion near the ear to recognize gestures. By harnessing information from BLE connections between the wireless earphones and a host device, we accurately detect and classify different hand gestures performed by users, while also determining discrete levels of hand speed. EarGest operates without interfering with the regular functionality of the earphones and introduces minimal energy overhead on the host device. We implement a prototype of the system using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and settings. Results show that our system can detect and classify seven near-ear hand gestures with an accuracy up to 98.5%, as well as identify hand motion speed with 96% accuracy.",75,https://ieeexplore.ieee.org/abstract/document/9918622,DONE,,,"Selection, Path",3,X,relative,8,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,48%
[614],Bi & Liu,2022,Head Gestures and Pointing,In,Head,"Pitch, Roll, Yaw",Accelerometer,Yes,12,Semantic,Yes,Yes,No,Yes,Medium,High,Medium (N=30),High,No,No,No,No,No,Yes (N=30),No,"Walking, Slope, Sitting, Ground Material, Speed",Lab,Earbud,Commercial,No,No,"However, head immobility for a long time has led to the emergence of “phubbers” and “office workers,” which sounds an alarm for people’s health, such as neck pain [1]. Immersing yourself in interaction with mobile phones during walking is also an important cause of traffic accidents [2]. Fortunately, the development of the Internet of Healthcare Things (IoHT) has brought new hope",,"Health, Safety","Earphones, head gesture, Internet of Healthcare Things (IoHT), metalearning","With the popularity of personal computing devices, people often keep long-term head immobility in front of screens, resulting in the emergence of “phubbers” and “office workers.” The early warning solutions in the Internet of Healthcare Things (IoHT) have brought hope to protect users’ health and safety. However, most existing works cannot recognize the different head gestures during walking, which is also a common cause of text neck and traffic accidents. In addition, they also need a large amount of data to update the model to adapt to the new environment, which reduces the practicality of the model. To solve these problems, we propose a system, CSEar, based on builtin accelerometers of off-the-shelf wireless earphones, which can recognize 12 kinds of head gestures both in resting and walking states. First, an innovative algorithm is designed to detect head gesture signals, especially for the signals mixed with gait. Then, we propose the MetaSensing, a head gesture recognition model that can improve the recognition ability with few samples compared with the existing metalearning algorithms. Finally, the experimental results prove the effectiveness and robustness of the CSEar.",76,https://ieeexplore.ieee.org/document/9815053,DONE,,,"Selection, Path, Orientation",3,X,relative,12,Yes,No,X,Partly Present,Not Indicated,High (Derived),Not Indicated,Present,43%
[452],Bi et al.,2022,Ear and Earable,In,Hand,Tap (Earable),Accelerometer,Yes,1,Semantic,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),"Sitting, Standing","Lab, Train",Earbud,Commercial,No,No,"However, some of the earphone-based authentication solutions need to customize special earphones, which are not universal.; To solve this problem, a new authentication solution based on the existing commercial earphones is proposed to authenticate a user by tapping on the earphone rhythmically.; Therefore, the security and privacy problem in ICWSN is becoming increasingly serious",,Authentification,"Accelerometer, earphone, tap gesture, user authentication","The rapid development of the information-centric wireless sensor network (ICWSN) has solved the challenges of information transmission and processing caused by the accelerated growth of wearable devices and the wide deployment of the Internet of Things (IoT) recently. The privacy security is also a growing problem. The existing works use earphones, covert, and user-friendly wearable devices, for user authentication. However, some of the earphone-based authentication solutions need to customize special earphones, which are not universal. Other solutions use microphones and speakers of earphones for authentication, which are susceptible to changes in the auricle’s internal environment, resulting in a decline in performance. To solve this problem, a new authentication solution based on the existing commercial earphones is proposed to authenticate a user by tapping on the earphone rhythmically. This rhythmic tap behavior causes a change of the signal waveform of the built-in accelerometer in the earphone. Based on this, we design a pipeline to authenticate the user’s identity. We first design an event detection algorithm to segment the tap signal accurately. Then, we use the global features calculated based on the event detection algorithm and local features extracted from the convolutional neural network (CNN) for building an authentication model using the Naive Bayes (NB) classifier. Finally, 20 users are recruited to evaluate the experiment and the recognition accuracy reaches 98%. Moreover, we extend the experiment to prove that it has a good performance against the different attacks and is robust in different scenarios.",77,https://ieeexplore.ieee.org/document/9367286,DONE,,,X,X,X,X,1,Yes,No,X,Present (N=3),Not Indicated,High (Derived),Not Indicated,Not Indicated,25%
[345],Choi et al.,2022,Face,In,Facial Expression,Ekman 7,"Accelerometer, PPG",Yes,7,Semantic,Yes,Visual Attention,Yes,No,Low,Low,Medium (N=20),Medium (N=5),No,Yes (N=11),No,No,No,Yes (N=20),No,"Head Movement, Sitting, Standing, Hand Motion, Walking, Mask, Wearing Styles",Lab,Custom Device,Research Prototype,No,No,"To this end, we propose PPGface, a ubiquitous, easy-to-use, user-friendly facial expression recognition platform that leverages earable devices with built-in PPG sensor. PPGface understands the facial expressions through the dynamic PPG patterns resulting from facial muscle movements. With the aid of the accelerometer sensor, PPGface can detect and recognize the user’s seven universal facial expressions and relevant body posture unobtrusively.",,"Customer Analytics, Accessibility, Device Control","Photoplethysmogram, PPG, Facial Expression, Ear Canal, Blood Vessel Deformation","Recognition of facial expressions has been widely explored to represent people’s emotional states. Existing facial expression recognition systems primarily rely on external cameras which make it less accessible and efficient in many real-life scenarios to monitor an individual’s facial expression in a convenient and unobtrusive manner. To this end, we propose PPGface, a ubiquitous, easy-to-use, user-friendly facial expression recognition platform that leverages earable devices with built-in PPG sensor. PPGface understands the facial expressions through the dynamic PPG patterns resulting from facial muscle movements. With the aid of the accelerometer sensor, PPGface can detect and recognize the user’s seven universal facial expressions and relevant body posture unobtrusively. We conducted an user study (N=20) using multimodal ResNet to evaluate the performance of PPGface, and showed that PPGface can detect different facial expressions with 93.5 accuracy and 0.93 f1-score. In addition, to explore the robustness and usability of our proposed platform, we conducted several comprehensive experiments under real-world settings. Overall results of this work validate a great potential to be employed in future commodity earable devices.",78,https://doi.org/10.1145/3534597,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[627],Futami et al,2022,Face,In,Facial Expression,"Facial Gesture (Mouth), Facial Gesture (Eye)",Proximity Sensor,Yes,8,Semantic,Yes,Visual Attention,Yes,No,Low,Low,High (N=10),N/A,No,No,No,No,No,Yes (N=10),No,Sitting,Lab,Custom Device,Research Prototype,No,No,"Although many previous studies use canal-type earphones, few studies focused on the following two points: (1) A method applicable to ear accessories other than canal-type earphones. (2) A method enabling various ear accessories with different styles to have the same hands-free input function. To realize these two points, this study proposes a method to recognize the user’s facial gesture using an infrared distance sensor attached to the ear accessory. The proposed method detects skin movement around the ear and face, which differs for each facial expression gesture.; Although there have been many studies on hands-free input methods using canal-type earphones, few have focused on the following two points. (1) Hands-free input methods that can be applied to ear accessories other than canal-type earphones for presenting sound information and (2) Hands-free input methods that can apply similar input methods to different ear accessories.",,"Device Input, Music Player","ear accessories, infrared distance sensor, photo reflector, skin movement, facial gesture recognition, hands-free input interface, earable","Simple hands-free input methods using ear accessories have been proposed to broaden the range of scenarios in which information devices can be operated without hands. Although many previous studies use canal-type earphones, few studies focused on the following two points: (1) A method applicable to ear accessories other than canal-type earphones. (2) A method enabling various ear accessories with different styles to have the same hands-free input function. To realize these two points, this study proposes a method to recognize the user’s facial gesture using an infrared distance sensor attached to the ear accessory. The proposed method detects skin movement around the ear and face, which differs for each facial expression gesture. We created a prototype system for three ear accessories for the root of the ear, earlobe, and tragus. The evaluation results for nine gestures and 10 subjects showed that the F-value of each device was 0.95 or more, and the F-value of the pattern combining multiple devices was 0.99 or more, which showed the feasibility of the proposed method. Although many ear accessories could not interact with information devices, our findings enable various ear accessories with different styles to have eye-free and hands-free input ability based on facial gestures.",79,https://doi.org/10.3390/electronics11091480,DONE,,,Selection,X,X,relative,,Yes,No,,,,,,,
[315],Jin et al.,2022,"Mouth, Head Gestures and Pointing",In,"Speech Apparatus, Head","Silent Speech (Words), Silent Speech (Sentences), Yaw, Pitch","Microphone, Speaker, Accelerometer",Yes,59,Semantic,Yes,Yes,No,Yes,High,High,N/A,N/A,No,No,No,No,No,No,Yes (N=12),"Sitting, Noise, Standing, Head Movement, Walking, Mask, Wearing Styles",Lab,Earbud,Research Prototype,Yes,No,"To overcome all these challenges in voice/speech interaction, we propose a new earphone-based silent speech interaction method, named “EarCommand.” EarCommand utilizes the inward-facing speaker to emit near-ultrasound signals to propagate within the ear canal, and then the inward-facing microphone receives reflected echos. By analyzing the channel response feature patterns which caused by the changing ear canal deformations during the (silent) speech, EarCommand can recognize and interpret users’ speech as shown in Fig. 1.",,"Device Control, Silent Speech, Privacy","Acoustic sensing, Earphone, Silent Speech, Ear Canal Deformation","Intelligent speech interfaces have been developing vastly to support the growing demands for convenient control and interaction with wearable/earable and portable devices. To avoid privacy leakage during speech interactions and strengthen the resistance to ambient noise, silent speech interfaces have been widely explored to enable people’s interaction with mobile/wearable devices without audible sounds. However, most existing silent speech solutions require either restricted background illuminations or hand involvement to hold device or perform gestures. In this study, we propose a novel earphonebased, hand-free silent speech interaction approach, named EarCommand. Our technique discovers the relationship between the deformation of the ear canal and the movements of the articulator and takes advantage of this link to recognize different silent speech commands. Our system can achieve a WER (word error rate) of 10.02% for word-level recognition and 12.33% for sentence-level recognition, when tested in human subjects with 32 word-level commands and 25 sentence-level commands, which indicates the effectiveness of inferring silent speech commands. Moreover, EarCommand shows high reliability and robustness in a variety of configuration settings and environmental conditions. It is anticipated that EarCommand can serve as an efficient, intelligent speech interface for hand-free operation, which could significantly improve the quality and convenience of interactions.",80,https://doi.org/10.1145/3534613,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[278],Li et al.,2022,Face,In,Facial Expression,"Facial Gesture (Mouth), Facial Gesture (Eye), Smile","Microphone, Speaker",Yes,9,Semantic,Yes,Visual Attention,No,Yes,Low,Medium,N/A,N/A,No,Yes (N=16),No,No,No,No,Yes (N=12),"Sitting, Walking, Wearing Styles, Noise, Speaking","Lab, Outdoors",Custom Device,Research Prototype,No,No,"We present EarIO, the first very low-power and minimally-obtrusive active-acoustic sensing technology, that uses the machine learning method to continuously infer detailed full facial movements from the subtle skin deformations. Our technology only requires one speaker and one microphone on each side of the earable, which are widely available on many modern earphones (e.g., Apple PowerBeats Pro1). The speaker on each earable emits encoded acoustic signals (Frequency-Modulated Continuous-Wave, FMCW) towards the user’s face. These acoustic signals are reflected differently to a microphone based on the skin deformation associated with different facial movements. The received acoustic signals are first decoded and processed to extract the echo profiles around the face, which are then learned by a customized deep learning model to estimate the full facial expressions.",,Data Annotation,"Facial expression reconstruction, Acoustic sensing, Low-power, Deep learning","This paper presents EarIO, an AI-powered acoustic sensing technology that allows an earable (e.g., earphone) to continuously track facial expressions using two pairs of microphone and speaker (one on each side), which are widely available in commodity earphones. It emits acoustic signals from a speaker on an earable towards the face. Depending on facial expressions, the muscles, tissues, and skin around the ear would deform differently, resulting in unique echo profiles in the reflected signals captured by an on-device microphone. These received acoustic signals are processed and learned by a customized deep learning pipeline to continuously infer the full facial expressions represented by 52 parameters captured using a TruthDepth camera. Compared to similar technologies, it has significantly lower power consumption, as it can sample at 86 Hz with a power signature of 154 mW. A user study with 16 participants under three different scenarios, showed that EarIO can reliably estimate the detailed facial movements when the participants were sitting, walking or after remounting the device. Based on the encouraging results, we further discuss the potential opportunities and challenges on applying EarIO on future ear-mounted wearables.",81,https://doi.org/10.1145/3534621,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[612],Grübler & Suzuki,2010,Face,Out,,"Smile, Facial Gesture (Other)",EMG,,2,,,,,,,,,,,,,,,,,,,,,,,"In this paper we present a quantitative analysis of electrode positions on the side of the face for facial expression recognition using facial bioelectrical signals. We show that distal electrode locations on areas of low facial mobility have a strong amplitude and are correlated to signals captured in the traditional positions on top of the facial muscles. We report on electrode position choice as well successful facial expression identification using computational methods.; Then we show the classification rates on the proposed location using computational methods and finally we show the designed unobtrusive wearable interface device, the Emotion Reader, which can recognize the subject’s facial expressions while being unobtrusive to the user because of its location away from the front of the face and record them for facial expression evaluation. The Emotion Reader is able to work in any environment regardless of changing lighting conditions and position of the subject.",,Emotion Recognition,N/A,In this paper we present a quantitative analysis of electrode positions on the side of the face for facial expression recognition using facial bioelectrical signals. We show that distal electrode locations on areas of low facial mobility have a strong amplitude and are correlated to signals captured in the traditional positions on top of the facial muscles. We report on electrode position choice as well successful facial expression identification using computational methods. We also propose a wearable interface device that can detect facial bioelectrical signals distally in a continuous manner while being unobtrusive to the user. The proposed device can be worn on the side of the face and capture signals that are considered to be a mixture of facial electromyographic signals and other bioelectrical signals. Finally we show the design of an interface that can be comfortably worn by the user and makes facial expression recognition possible.,9,https://ieeexplore.ieee.org/abstract/document/5626504,,,,,,,,,,,,,,,,,
[512],Song et al.,2022,Face,In,Facial Expression,Ekman 7,"Microphone, Speaker",Yes,6,Semantic,Yes,Visual Attention,No,No,Low,Low,Low (N=5),High (N=5),No,No,No,No,No,Yes (N=5),Yes (N=5),"Sitting, Device, Music, Movie, Walking, Speaking, Skin Condition","Lab, Outdoors","Headphone, Earbud",Commercial,No,No,"In this paper, we present FaceListener, a new sensing system that recognizes human facial expressions by only using commodity headphones. The basic idea of FaceListener is to transform the commodity headphone into an acoustic sensing device, which captures the face skin deformations caused by facial muscle movements with different facial expressions.; In this paper, we aim to address the aforementioned limitations and instead propose FaceListener, a new sensing system that achieves precise and reliable recognition of human facial expressions by only using commodity headphones.",,"AR/VR, Device Control, Customer Analytics","Human Facial Expressions, Acoustic Sensing, Headphone, Face Skin Deformation, Knowledge Distillation","Facial expressions are important indicators of user needs that can be used in many interactive computing applications to adapt the system behaviors and settings. Current computing approaches to recognizing human facial expressions, however, either rely on continuous camera recordings that are energy consuming, or require custom sensing hardware that are expensive and difficult to use on commodity systems. In this paper, we present FaceListener, a new sensing system that recognizes human facial expressions by only using commodity headphones. The basic idea of FaceListener is to transform the commodity headphone into an acoustic sensing device, which captures the face skin deformations caused by facial muscle movements with different facial expressions. To ensure the recognition accuracy, FaceListener leverages the knowledge distillation technique to learn the subtle correlation between face skin deformation and the acoustic signal changes. Experiment results over multiple human beings demonstrate that FaceListener can accurately recognize more than 80% of different facial expressions. FaceListener is highly energy efficient, and can well adapt to different headphone models, host systems and user activities.",82,https://ieeexplore.ieee.org/document/9825944/,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[296],Rateau et al.,2022,"Ear and Earable, Hand Gestures and Location",Elicitation,N/A,N/A,N/A,N/A,0,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,Yes (N=20),No,No,No,Yes (N=50),No,No,Sitting,Remote Setting,Earbud,Commercial,N/A,N/A,"The specific example we explore in this paper combines smart earbuds with a proximal smartwatch to support input specifically geared toward smartwatch and smartwatch-earbud interactions.; it becomes necessary to increase the set of gestures to include control of multiple wearable devices.; interactions with multiple smart devices may result in different gestures and the social acceptability of these gestures may vary.; when users are aware of the fact that multiple personal devices can sense input, it may be advisable to adapt gesture design.",,"Phone Calls, Music Player, Device Control, Device Input, Communication","elicitation study, smartwatch, earbuds, wearables","Due to the proliferation of smart wearables, it is now the case that designers can explore novel ways that devices can be used in combination by end-users. In this paper, we explore the gestural input enabled by the combination of smart earbuds coupled with a proximal smartwatch. We identify a consensus set of gestures and a taxonomy of the types of gestures participants create through an elicitation study. In a follow-on study conducted on Amazon’s Mechanical Turk, we explore the social acceptability of gestures enabled by watch+earbud gesture capture. While elicited gestures continue to be simple, discrete, in-context actions, we find that elicited input is frequently abstract, varies in size and duration, and is split almost equally between on-body, proximal, and more distant actions. Together, our results provide guidelines for on-body, near-ear, and in-air input using earbuds and a smartwatch to support gesture capture.",83,https://dl.acm.org/doi/10.1145/3567710,DONE,,,,,,,,,,,,,,,,
[307],Srivastava et al.,2022,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),"Accelerometer, Gyroscope",Yes,100,Semantic,Yes,Yes,Yes,No,High,High,High (N=20),High (N=20),No,Yes (N=20),No,No,No,Yes (N=20),No,"Head Movement, Walking, Music, Noise","Lab, Bus",Custom Device,Research Prototype,No,No,"In this paper, we present MuteIt, an earable system that recognizes unvoiced commands by tracking jaw motion. MuteIt is capable of recognizing entire words and is robust to body/head movements. In contrast to prior works [42, 73, 96] , MuteIt reconstructs a word from its components instead of training a word classifier, enabling the system to easily scale to any word.",,"Silent Speech, AR/VR, Privacy, Device Control","Unvoiced Speech, IMU Sensing, Signal Processing","In this paper, we present MuteIt, an ear-worn system for recognizing unvoiced human commands. MuteIt presents an intuitive alternative to voice-based interactions that can be unreliable in noisy environments, disruptive to those around us, and compromise our privacy. We propose a twin-IMU set up to track the user’s jaw motion and cancel motion artifacts caused by head and body movements. MuteIt processes jaw motion during word articulation to break each word signal into its constituent syllables, and further each syllable into phonemes (vowels, visemes, and plosives). Recognizing unvoiced commands by only tracking jaw motion is challenging. As a secondary articulator, jaw motion is not distinctive enough for unvoiced speech recognition. MuteIt combines IMU data with the anatomy of jaw movement as well as principles from linguistics, to model the task of word recognition as an estimation problem. Rather than employing machine learning to train a word classifier, we reconstruct each word as a sequence of phonemes using a bi-directional particle filter, enabling the system to be easily scaled to a large set of words. We validate MuteIt for 20 subjects with diverse speech accents to recognize 100 common command words. MuteIt achieves a mean word recognition accuracy of 94.8% in noise-free conditions. When compared with common voice assistants, MuteIt outperforms them in noisy acoustic environments, achieving higher than 90% recognition accuracy. Even in the presence of motion artifacts, such as head movement, walking, and riding in a moving vehicle, MuteIt achieves mean word recognition accuracy of 91% over all scenarios.",84,https://doi.org/10.1145/3550281,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[309],Wang et al.,2022,Head Gestures and Pointing,In,Head,"Pitch, Yaw",Microphone,No,2,Fine,Yes,Visual Attention,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=12),Sitting,Lab,Headphone,Research Prototype,Yes,No,"Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones.; allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.; Enabling a device to detect this intention precisely and naturally can simplify user interface fow, enable hands-free interaction, and adapt interfaces to the context of use. For example, smartphones can detect users’ proximity and face orientation toward the device to turn on the screen and allow them to read their notifcations in a hand-free manner. Additionally, a system that can provide an accurate classifcation of whether a user’s head is oriented toward a specifc device. This device-specifc binary attention detector can be used to drive context-aware experiences.",,"Device Input, Activity Recognition, Device Control, Motion Tracking","Acoustic ranging, head orientation, earphone, head pose estimation","Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7◦ in yaw, and 5.8◦ in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.",85,https://dl.acm.org/doi/10.1145/3491102.3517698,DONE,,,"Selection, Orientation, Path",2,fine,absolute,2,No,Yes,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Present,28%
[306],Wang et al.,2022,Mouth,In,Teeth,"Click, Slide",Microphone,Yes,10,Semantic,Yes,Yes,No,No,High,High,N/A,N/A,No,Yes (N=25),No,No,No,No,Yes (N=25),"Sitting, Walking, Standing","Living Room, Lab, Car, Outdoors, Supermarket",Earbud,Research Prototype,No,No,"In this work, we propose ToothSonic, a secure authentication system that leverages the toothprint-induced sonic effect produced by a user performing teeth gestures for earable authentication. In particular, when teeth slide or strike against each other, part of their mechanical energy is transformed into a sonic wave. The harmonics of the friction- and collision- excited sonic waves are dependent on the teeth composition, the dental geometry, and the surface characteristics of each tooth [3]. The key insight is that the sonic waves produced from a teeth gesture (either slide or tap) carry the information of the toothprint. As every individual has a unique toothprint just like our fingerprint, two users perform the same teeth gesture will result in distinct toothprint-induced sonic waves, which could be sensed by the earables for user authentication, as shown in Fig. 1.;",,"Authentification, AR/VR","Earable Authentication, Tooth, Wearable, Biometrics, Ear Canal, Acoustic Sensing","Earables (ear wearables) are rapidly emerging as a new platform encompassing a diverse range of personal applications. The traditional authentication methods hence become less applicable and inconvenient for earables due to their limited input interface. Nevertheless, earables often feature rich around-the-head sensing capability that can be leveraged to capture new types of biometrics. In this work, we propose ToothSonic that leverages the toothprint-induced sonic effect produced by a user performing teeth gestures for earable authentication. In particular, we design representative teeth gestures that can produce effective sonic waves carrying the information of the toothprint. To reliably capture the acoustic toothprint, it leverages the occlusion effect of the ear canal and the inward-facing microphone of the earables. It then extracts multi-level acoustic features to reflect the intrinsic toothprint information for authentication. The key advantages of ToothSonic are that it is suitable for earables and is resistant to various spoofing attacks as the acoustic toothprint is captured via the user’s private teeth-ear channel that modulates and encrypts the sonic waves. Our experiment studies with 25 participants show that ToothSonic achieves up to 95% accuracy with only one of the users’ tooth gestures.",86,https://doi.org/10.1145/3534606,DONE,,,"Selection, Position, Path",X,X,absolute,,Yes,No,,,,,,,
[388],Yang et al.,2022,Actuation,Enabling,N/A,Vibration (Actuation),N/A,N/A,70,Semantic,Yes,Yes,Yes,N/A,High,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,N/A,"This study is based on our previous work in [12], which introduced the design of a haptic-signal based hearing aid targeting patients suffering from inner ear malfunction. Compared to previous work, this study focused more on vibrotactile display.",,Accessibility,"hearing aid, artificial ear, vibrotactile","Hearing aid devices have been around for decades, while most of them focus on sound amplification and SNR improvement. This paper proposes an artificial ear based on the vibrotactile feedback. The speech signal is converted into the vibrotactile devices placed around the subject’s ear through the speech recognition algorithm and pattern coding method. Preliminary experiments on the prototype consisting of six motors which has shown that the recognition accuracy of letters and daily sentences reached 90%. The learning time of interpreting the vibrotactile signals could be less than four times that in realtime conversation, proving the feasibility of the proposed device for real-life application.",87,https://ieeexplore.ieee.org/abstract/document/9928488,DONE,,,,,,,,,,,,,,,,
[284],Alkiek et al.,2023,Ear and Earable,In,Hand,"Tap (Ear), Slide (Ear), Pull (Ear), Press (Ear), Pinch (Ear)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,Medium,High (N=4),N/A,No,No,No,No,No,Yes (N=4),No,Sitting,Lab,Earbud,Commercial,Yes,No,"However, modern-day earables usually have a limited interface, inhibiting their potential as an accessible medium of input.; Augmenting earables with a hand-to-ear gesture interface would allow them to overcome this setback and provide a more convenient and richer experience for users.","Expanding Interaction Methods, Enhancing Functionalities of Current Devices, Reducing Interaction Effort, Improving Usability and Ergonomics","Music Player, Phone Calls, Device Control, Device Input","Earables, gesture recognition, inertial sensors, natural interface, on-body interaction","Earables have been gaining popularity over the past few years for their ease of use and convenience over wired earphones. However, modern-day earables usually have a limited interface, inhibiting their potential as an accessible medium of input. To this end, we present EarBender: an ear-based real-time system that bridges the gap between earables and on-body interaction, providing a more diverse and natural form of interaction with devices. EarBender enables touch-based hand-to-ear gestures on mobile devices by leveraging inertial sensors in commercially available earable devices. Our proposed system detects the slight deformation in a user’s ear resulting from different ear-based actions including swiping and tapping and classifies the action performed. EarBender is designed to be energy-efficient, easy to deploy and robust to different users, requiring little to no calibration. We implement a prototype of EarBender using eSense, a multi-sensory earable platform, and evaluate it in different scenarios and parameter settings. Results show that the system can detect the occurrence of gestures with a 96.8% accuracy and classify seven different hand-to-ear gestures with an accuracy up to 97.4% maintained across four subjects.",88,https://dl.acm.org/doi/10.1145/3594739.3610671,DONE,,,"Selection, Path, Position",1,X,relative,7,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,42%
[344],Chugh et al.,2023,"Head Gestures and Pointing, Face",Enabling,"Head, Facial Expression","Yaw, Pitch, Facial Gesture (Eye), Facial Gesture (Other)","Accelerometer, Gyroscope",Yes,5,Semantic,Yes,Visual Attention,No,No,Low,Medium,Low (N=17),N/A,No,No,No,No,No,Yes (N=17),No,Sitting,Lab,Earbud,Commercial,Yes,No,"Understanding the level of participation for a remote attendee in an online meeting setup could significantly improve the quality of experience for virtual interaction; Hence, this demands the development of a mechanism that provides real-time feedback to the speaker regarding listeners’ involvement in a visually intuitive fashion, thus making online-based content delivery more fruitful to both the speaker and the listeners. In this context, our paper explores using IMU (Inertial Measurement Unit) data obtained from earables to detect different cues generated by the human head and body manifestations. In recent times, earables have drawn considerable interest amongst the research community for analyzing user behavior and detecting visual cues such as human gaze, hand gestures, and other verbal or nonverbal cues.; We identify features from gestures, such as head movements, facial expressions, and body postures, that mimic verbal and non-verbal cues and demonstrate the efficacy of earables. (2) Our preliminary evaluation reveals that enVolve identifies overlapping features with reasonable accuracy (around 80%), typically a complex problem for hand-worn wearables [16, 17].",,Video Conference,"online meeting, earables, inertial sensing","Understanding the level of participation for a remote attendee in an online meeting setup could significantly improve the quality of experience for virtual interaction. However, gauging audience involvement over an online meeting becomes particularly challenging when the attendees prefer to turn off the cameras. IMU data have shown promising results in the past to pervasively monitor users’ body language, including the determination of various bodily gestures, postures, and facial expressions. This paper demonstrates how earables could help address the stated problem. We provide a motivational study to assess earables for detecting body language corresponding to involved listeners. We further compare it with other sensing modalities like smartwatches and smartphones and accordingly develop a platform called enVolve. A lab-scale study with 17 participants demonstrates the efficacy of the proposed system with an average F1 score of more than 80%.",89,https://doi.org/10.1145/3544793.3563419,DONE,,,,,,,,,,,,,,,,
[342],Jin et al.,2023,Mouth,In,"Head, Facial Expression",Silent Speech (Morphemes),"Accelerometer, Gyroscope",No,6,Semantic,Yes,Yes,Yes (Performance Loss),No,High,High,Medium (N=10),N/A,No,Yes (N=5),No,No,No,Yes (N=10),Yes (N=10),Sitting,Lab,Earbud,Research Prototype,Yes,No,"However, stateof-the-art wearable-based techniques mainly concentrate on recognizing manual markers (e.g., hand gestures), while frequently overlooking non-manual markers, such as negative head shaking, question markers, and mouthing. This oversight results in the loss of substantial grammatical and semantic information in sign language. To address this limitation, we introduce SmartASL, a novel proof-of-concept system that can 1) recognize both manual and non-manual markers simultaneously using a combination of earbuds and a wrist-worn IMU, and 2) translate the recognized American Sign Language (ASL) glosses into spoken language.; Motivated by the pros and cons of existing ASL recognition methods, in this study, we propose and develop SmartASL, an affordable, off-the-shelf, user-friendly proof-of-concept system to achieve end-to-end ASL recognition which shows great potential to facilitate communications between ASL signers and hearing people.",,Accessibility,"Comprehensive ASL Recognition, Smartwatch, Earbuds, Manual Markers, Non-manual Markers","Sign language builds up an important bridge between the d/Deaf and hard-of-hearing (DHH) and hearing people. Regrettably, most hearing people face challenges in comprehending sign language, necessitating sign language translation. However, stateof-the-art wearable-based techniques mainly concentrate on recognizing manual markers (e.g., hand gestures), while frequently overlooking non-manual markers, such as negative head shaking, question markers, and mouthing. This oversight results in the loss of substantial grammatical and semantic information in sign language. To address this limitation, we introduce SmartASL, a novel proof-of-concept system that can 1) recognize both manual and non-manual markers simultaneously using a combination of earbuds and a wrist-worn IMU, and 2) translate the recognized American Sign Language (ASL) glosses into spoken language. Our experiments demonstrate the SmartASL system’s significant potential to accurately recognize the manual and non-manual markers in ASL, effectively bridging the communication gaps between ASL signers and hearing people using commercially available devices.",90,https://doi.org/10.1145/3596255,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[376],Li et al.,2023,"Ear and Earable, Hand Gestures and Location",In,Hand,"Pinch (Ear), Thinking Gesture, Hold (Face), Cover (Face), Calling Gesture, Cover (Ear), Support (Face)",Microphone,No,8,Semantic,No,Yes,Yes (Performance Loss),No,Low,Medium,High (N=10),N/A,Yes (N=10),No,Yes (N=25),No,Yes (N=25),Yes (N=10),No,Sitting,Lab,Earbud,Commercial,No,No,"We targeted on hand-to-face gestures because such gestures relate closely to speech and yield signifcant acoustic features (e.g., impeding voice propagation).; Thus, researchers have been seeking supplementary input methods as parallel input channels to assist voice interaction; In this paper, we investigated the feasibility of using voiceaccompanying hand-to-face (VAHF) gestures as parallel channels to improve the traditional voice interaction fow.; Specifcally, we aim at designing VAHF gestures and recognizing them with an acoustic-based cross-device sensing method. We targeted hand-toface gestures as the instantiation of voice-accompanying gestures because they have been proven to be natural, expressive (e.g., various landmarks on the face to yield a large gesture space), and more related to the speech by existing researches; Moreover, hand-to-face gestures yielded signifcant features in voice propagation, which is benefcial for acoustic sensing; outline new opportunities for VAHF gestures to beneft voice interaction.; Our quantitative analysis sheds light on the recognition capability of the diferent sensor combinations over VAHF gestures with diferent characteristics. We conducted a comprehensive study to elicit the gesture space of VAHF gestures and proposed a gesture set with better usability, better social acceptance, less fatigue, and less ambiguity.",,"Device Control, Device Input","hand gestures, acoustic sensing, sensor fusion","Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted on hand-to-face gestures because such gestures relate closely to speech and yield signifcant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we frst gathered candidate gestures and then applied a structural analysis to them in diferent dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3% for recognizing 3 gestures and 91.5% for recognizing 8 gestures (excluding the ""empty"" gesture), proving the high applicability. Quantitative analysis also shed light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios.",91,https://dl.acm.org/doi/10.1145/3544548.3581008,DONE,,,"Selection, Position",X,X,X,15,Yes,No,X,Present (N=10),High (N=25),Low (Derived),Not Indicated,Present,44%
[308],Panda et al.,2023,"Ear and Earable, Head Gestures and Pointing, Hand Gestures and Location",In,"Hand, Head","Lift (Earable), Cup (Mid-Air), Cover (Face), Pitch, Roll, Press (Earable), Yaw","Accelerometer, Gyroscope, Magnetometer, LiDAR Sensor, Button",Yes,11,"Semantic, Fine, Coarse",Partly,Visual Attention,No,No,Medium,High,N/A,N/A,Yes,No,No,No,No,No,No,Sitting,Lab,Headphone,Research Prototype,Yes,Yes,"the presence of microphones and even inertial motion sensors on some units (typically for spatial audio support [5, 10]) hints at richer possibilities for headphone-situated input, interaction, and wearable sensing. In this way sensor-enhanced headphones ofer designers an opportunity to (re-)consider notions of the user’s context, activity, and proximal hand gestures–enabling rich interactions beyond the status-quo, button-pushing type of interactions with headphones.; Further, by augmenting existing user actions, such an approach reduces the number of explicit gestures that the user has to learn, control, enact, and remember.",,Video Conference,"headphones, wearables, sensing, research through design, design space","Via Research through Design (RtD), we explore the potential of headphones as a general-purpose input device for both foreground motion-gestures as well as background sensing of user activity. As a familiar wearable device, headphones ofer a compelling site for head-situated interaction and sensing. Using emerging sensing modalities such as inertial motion, capacitive touch sensing, and depth cameras, our implemented prototypes explore sensing and interaction techniques that ofer a range of compelling capabilities. User scenarios include context-aware privacy, gestural audiovisual control, and co-opting natural body language as context to drive animated avatars for ""camera-of"" scenarios in remote workor to co-opt (oft-subconscious) head movements such as dodging attacks in video games to enhance the gameplay experience. Drawing from literature and other frameworks, we situate our prototypes and related techniques in a design space across the dual dimensions of (1) type of input (touch, mid-air, or head orientation); and (2) the context of user action (application, body, or environment). In particular, interactions that combine multiple inputs and contexts at the same time ofer a rich design space of headphonesituated wearable interactions and sensing techniques.",92,https://dl.acm.org/doi/10.1145/3563657.3596022,DONE,,,"Selection, Position, Orientation",4,fine,"absolute, relative",11,Yes,Yes,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,35%
[443],Paul et al.,2023,"Brain, Eye-Tracking",Enabling,Eye,"Blink, Horizontal Gaze, Vertical Gaze","EEG, EOG",Yes,3,"Semantic, Fine",Yes,No,No,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,No,No,This work presents a complete versatile wireless electrophysiology data acquisition system (weDAQ) that is demonstrated for in-ear electroencephalography (EEG) and other on-body electrophysiology with user-generic dry-contact electrodes made from standard printed circuit boards (PCBs).,,"Health, Device Control, BCI-Application","BCI, body sensor network, dry electrodes, EMG, EOG, health sensing, in-ear EEG, PCB electrodes, WiFi","To enable continuous, mobile health monitoring, body-worn sensors need to offer comparable performance to clinical devices in a lightweight, unobtrusive package. This work presents a complete versatile wireless electrophysiology data acquisition system (weDAQ) that is demonstrated for in-ear electroencephalography (EEG) and other on-body electrophysiology with user-generic dry-contact electrodes made from standard printed circuit boards (PCBs). Each weDAQ device provides 16 recording channels, driven right leg (DRL), a 3-axis accelerometer, local data storage, and adaptable data transmission modes. The weDAQ wireless interface supports deployment of a body area network (BAN) capable of aggregating various biosignal streams over multiple worn devices simultaneously, on the 802.11n WiFi protocol. Each channel resolves biopotentials ranging over 5 orders of magnitude with a noise level of 0.52 μVrms over a 1000-Hz bandwidth, and a peak SNDR of 119 dB and CMRR of 111 dB at 2 ksps. The device leverages in-band impedance scanning and an input multiplexer to dynamically select good skin contacting electrodes for reference and sensing channels. In-ear and forehead EEG measurements taken from subjects captured modulation of alpha brain activity, electrooculogram (EOG) characteristic eye movements, and electromyogram (EMG) from jaw muscles. Simultaneous ECG and EMG measurements were demonstrated on multiple, freelymoving subjects in their natural office environment during periods of rest and exercise. The small footprint, performance, and configurability of the open-source weDAQ platform and scalable PCB electrodes presented, aim to provide the biosensing community greater experimental flexibility and lower the barrier to entry for new health monitoring research.",93,https://ieeexplore.ieee.org/document/10115033,DONE,,,,,,,,,,,,,,,,
[277],Stanke et al.,2023,Actuation,Enabling,N/A,"Vibration (Actuation), Thermal (Actuaction), Electrotactile (Actuation), Poke (Actuation), Sound (Actuation), Display (Actuation), Light (Actuation)",N/A,N/A,8,Semantic,Yes,Yes,Yes,N/A,Medium,High,N/A,N/A,No,Yes (N=18),No,No,No,No,No,"Sitting, Sports","Fitness Center, Lab",Custom Device,Research Prototype,N/A,N/A,"This work elaborates the pros and cons of different notification channels for the earlobe.; In this paper, we contribute an evaluation of different private and public notification channels on the earlobe",,Communication,"Notification, Wearable, Ear Clip, Earlobe, Electrotactile, Vibration, Sound, Light, Thermal, Poke, Earring, Ear-Worn","The earlobe is a well-known location for wearing jewelry, but might also be promising for electronic output, such as presenting notifications. This work elaborates the pros and cons of different notification channels for the earlobe. Notifications on the earlobe can be private (only noticeable by the wearer) as well as public (noticeable in the immediate vicinity in a given social situation). A user study with 18 participants showed that the reaction times for the private channels (Poke, Vibration, Private Sound, Electrotactile) were on average less than 1 s with an error rate (missed notifications) of less than 1 %. Thermal Warm and Cold took significantly longer and Cold was least reliable (26 % error rate). The participants preferred Electrotactile and Vibration. Among the public channels the recognition time did not differ significantly between Sound (738 ms) and LED (828 ms), but Display took much longer (3175 ms). At 22 % the error rate of Display was highest. The participants generally felt comfortable wearing notification devices on their earlobe. The results show that the earlobe indeed is a suitable location for wearable technology, if properly miniaturized, which is possible for Electrotactile and LED. We present application scenarios and discuss design considerations. A small field study in a fitness center demonstrates the suitability of the earlobe notification concept in a sports context.",94,https://doi.org/10.1145/3610925,DONE,,,,,,,,,,,,,,,,
[249],Pfreundtner et al.,2020,Head Gestures and Pointing,Out,Head,"Yaw, Pitch",Microphone,No,2,Fine,Yes,No,,,,,N/A,N/A,,,,,,No,Yes,,,Custom Device,Research Prototype,,,"We envision future entertainment scenarios of 3D experiences with virtual sounds that interact with the real environment, also called audio augmented reality [1, 2].; In particular, acoustic mapping and head tracking might be applicable when visual tracking is not feasible,; In this work, we aim for boundary mapping with hearables and ultrasonic signals with simple hardware. We propose an attachment for normal headphones with pairs of microphones, which could be miniaturized and easily integrated into earpods too.",,"Device Control, AR/VR","Wearable computing, hearables, echolocation, acoustic environment capture, audio augmented reality","We present a microphone array structure for spherical sound incidence angle tracking that can be attached to headphones or directly integrated into earphones. We show that this microphone array together with an ultrasonic sound source, e.g., a home assistant speaker in the room, allows to estimate the direction and distance of sound reflections on wall surfaces in the room. With our presented method, we achieved sound incidence angle estimation errors of around 14◦ in the horizontal plane and around 5◦ in the vertical plane, and reflection position estimation errors of around 5 cm within 2 m. Having the reflection points in 3D allows us to sparsely capture the closest room geometry at interactive rate, which is a necessary step towards indoor audio augmented reality applications.",59,https://ieeexplore.ieee.org/abstract/document/9414356?casa_token=ZGxDzFU04UwAAAAA:szM1Aw4kVBK09w5RJT3bBbhPmx9QtdXY_0cFTjnTRVHDEXKml7JyN8geMKhu3OLOpX_7NBhGMQ,,,,,,,,,,,,,,,,,
[461],Yi et al.,2023,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),EMG,Yes,10,Semantic,Yes,Yes,No,No,High,High,Medium (N=45),Medium (N=45),Yes (N=10),No,No,No,No,Yes (N=45),No,"Sitting, Command Deformation, Head Movement",Lab,Custom Device,Research Prototype,No,No,"The prevalence of smart devices encourages increasing requirements of wearable human–computer interactions. To improve user acceptance, such interactions require easy-tomanipulate and unobtrusive characteristics. In this article, we, for the first time, propose to recognize silent commands through a lightweight and around-ear biosensing system Mordo that can be easily integrated with earphones, manipulate smart devices, and minimize social awkwardness.; In this study, we take an initial step to fill in this gap by proposing Mordo, a wearable around-ear biosensing system that senses sEMG signals around ears to interact with other devices by recognizing silent and semantic commands with unobtrusiveness, minimal social awkwardness and reliability.; The study sought to propose and demonstrate a proof-ofconcept design of an around-ear biosensor-enabled silent command recognition system.",,"Silent Speech, Phone Calls, Music Player","EMG signals, micro-interaction, silent command recognition","The prevalence of smart devices encourages increasing requirements of wearable human–computer interactions. To improve user acceptance, such interactions require easy-tomanipulate and unobtrusive characteristics. In this article, we, for the first time, propose to recognize silent commands through a lightweight and around-ear biosensing system Mordo that can be easily integrated with earphones, manipulate smart devices, and minimize social awkwardness. In particular, we first determine the empirical principles of constructing commands and experimentally screen the commands based on the around-ear configuration. Second, we select the optimal around-ear sensor configuration according to the single-channel signal-to-noise ratios (SNRs) and classification accuracies. Third, we propose a multistream CNN-LSTM network to learn the spatiotemporal mapping between the around-ear signals and commands. Finally, extensive experiments have been conducted to evaluate the feasibility and stability. The results indicate an averaged accuracy of 89.66% that outperforms other algorithms of similar tasks. The stability tests show that our system presents sufficient stability under command deformations and head motions. We demonstrate the necessity of collecting such scale of data by gradually reducing training data size. We also validate the generalization ability of our method toward other sensing parameters by reducing the spatial and temporal resolutions. The proof-of-concept design will aim the further development of the commercial products for silent command recognition.",95,https://ieeexplore.ieee.org/document/9878163,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[619],Zhang et al.,2023,Ear and Earable,In,Hand,Touch (Ear),"Speaker, Microphone",Yes,1,Coarse,No,Yes,Yes,No,Medium,High,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Custom Device,Research Prototype,N/A,No,This paper presents an approach using a speaker/microphone pair to perform active acoustic sensing in the inaudible frequency range to achieve continuous finger positioning on the ear.,,"Music Player, Phone Calls","acoustic sensing, bone conduction, positioning, human interface","The advancement of semiconductor and battery technologies popularized tiny acoustic wearable devices such as bone conduction wireless headsets. However, this small form factor poses inconvenience when controlling these devices, as they cannot equip large footprint intuitive interfaces such as volume sliders and touch screens. This paper presents a technique using acoustic responses measured by a bone conduction speaker and a microphone to utilize the ear as a touch input interface. We discovered that a finger placed on different parts of the ear affects the acoustic radiation characteristic of the ear, modulating the leaked sound, and by leveraging this effect, the touch position can be estimated. Experimental results show that five distinct frequency responses with five different finger positions can be obtained, which indicates that our method could allow bone conduction headsets to capture continuous finger positions without additional hardware.",96,https://dl.acm.org/doi/10.1145/3560905.3568075,DONE,,,"Selection, Position",X,coarse (5),X,1,No,Yes,X,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Not Indicated,20%
[292],Zhang et al.,2023,Face,In,Facial Expression,Ekman 7,Microphone,Yes,7,Semantic,Yes,Visual Attention,Yes (Performance Loss),Yes,Low,Low,N/A,N/A,No,Yes (N=20),No,No,No,No,Yes (N=20),"Wearing Styles, Time, Sitting, Head Movement, Walking",Lab,Earbud,Research Prototype,No,No,"This article presents EARFace, a system that shows the feasibility of tracking facial landmarks for 3D facial reconstruction using in-ear acoustic sensors embedded within smart earphones.",,"AR/VR, Accessibility, Emotion Recognition, Device Input, Health, Driving","Wearable sensing, mobile computing, earable computing, facial reconstruction, IoT","This article presents EARFace, a system that shows the feasibility of tracking facial landmarks for 3D facial reconstruction using in-ear acoustic sensors embedded within smart earphones. This enables a number of applications in the areas of facial expression tracking, user interfaces, AR/VR applications, affective computing, and accessibility, among others. Although conventional vision-based solutions break down under poor lighting and occlusions, and also suffer from privacy concerns, earphone platforms are robust to ambient conditions while being privacy-preserving. In contrast to prior work on earable platforms that perform outer-ear sensing for facial motion tracking, EARFace shows the feasibility of completely in-ear sensing with a natural earphone form factor, thus enhancing the comfort levels of wearing. The core intuition exploited by EARFace is that the shape of the ear canal changes due to the movement of facial muscles during facial motion. EARFace tracks the changes in shape of the ear canal by measuring ultrasonic channel frequency response of the inner ear, ultimately resulting in tracking of the facial motion. A transformer-based machine learning model is designed to exploit spectral and temporal relationships in the ultrasonic channel frequency response data to predict the facial landmarks of the user with an accuracy of 1.83 mm. Using these predicted landmarks, a 3D graphical model of the face that replicates the precise facial motion of the user is then reconstructed. Domain adaptation is further performed by adapting the weights of layers using a group-wise and differential learning rate. This decreases the training overhead in EARFace. The transformer-based machine learning model runs on smart phone devices with a processing latency of 13 ms and an overall low power consumption profile. Finally, usability studies indicate higher levels of comforts of wearing EARFace’s earphone platform in comparison with alternative form factors.",97,https://doi.org/10.1145/3614438,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[713],Zhang et al.,2023,Mouth,In,Speech Apparatus,Silent Speech (Words),"Speaker, Microphone",Yes,8,Semantic,Yes,Yes,Yes (Performance Loss),Yes,High,High,Medium (N=18),Medium (N=18),No,Yes (N=18),No,No,No,Yes (N=18),No,"Device, Music",Lab,Headphone,Research Prototype,No,No,"In this paper, we present the design and implementation of HPSpeech, a silent speech interface for commodity headphones that can recognize 8 silent speech phrases to control music player. HPSpeech utilizes the existing speakers of the headphones to emit inaudible acoustic signals. The movements of the temporomandibular joint (TMJ) during speech alter the reflections of the signal before it is captured by a microphone positioned inside the headphone.",,"Silent Speech, Music Player","Silent Speech, Acoustic Sensing, Headphones, Commodity-off-theshelf","We present HPSpeech, a silent speech interface for commodity headphones. HPSpeech utilizes the existing speakers of the headphones to emit inaudible acoustic signals. The movements of the temporomandibular joint (TMJ) during speech modify the reflection pattern of these signals, which are captured by a microphone positioned inside the headphones. To evaluate the performance of HPSpeech, we tested it on two headphones with a total of 18 participants. The results demonstrated that HPSpeech successfully recognized 8 popular silent speech commands for controlling the music player with an accuracy over 90%. While our tests use modified commodity hardware (both with and without active noise cancellation), our results show that sensing the movement of the TMJ could be as simple as a firmware update for ANC headsets which already include a microphone inside the hear cup. This leaves us to believe that this technique has great potential for rapid deployment in the near future. We further discuss the challenges that need to be addressed before deploying HPSpeech at scale.",98,https://doi.org/10.1145/3594738.3611365,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[516],Zhu et al.,2023,Head Gestures and Pointing,In,Head,"Roll, Pitch, Yaw","Accelerometer, Gyroscope",Yes,12,Semantic,Yes,Yes,Yes,Yes,Medium,High,High (N=15),N/A,No,No,No,No,No,Yes (N=15),No,"Walking Stairs, Standing, Walking, Sports",Lab,Earbud,Research Prototype,Yes,No,"The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations.; In spite of good recognition performance, these works share a common shortcoming, that is, only considering singular activities and outputting a single activity type.",,"Device Control, Music Player, Phone Calls, Activity Recognition","Composite activity recognition, Earable device, Multi-task learning","The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations. We implement a real-time prototype and conduct extensive experiments to evaluate it. The results show that CHAR can recognize 60 kinds of composite activities (12 head gestures and 5 body movements) with high accuracies of 97.0% and 89.7% in user- dependent and independent cases, respectively.",99,https://ieeexplore.ieee.org/abstract/document/10916516,DONE,,,"Selection, Path, Orientation",3,X,relative,12,Yes,No,X,Present,Not Indicated,Medium (Derived),Not Indicated,Present,52%
[346],Dong et al.,2024,Mouth,In,Speech Apparatus,Silent Speech (Words),Microphone,Yes,100,Semantic,Yes,Yes,No,Yes,High,High,Medium (N=4),Medium (N=9),No,No,No,No,No,Yes (N=4),No,"Noise, Head Movement, Walking, Wearing Styles, Sitting, Standing",Lab,Earbud,Research Prototype,No,No,"Although speech-based interactions can serve as a strong alternative to other interaction modalities, it is not always appropriate or possible to use speech in public settings.; In this work, we explore the feasibility of using ultrasonic refections recorded by commercially available, active noise-canceling (ANC) earbuds to enable SSI.; We propose a system called ReHEarSSE that can recognize words that are within a known lexicon but do not have a recorded example in the training dataset (i.e., unseen words).",,"Silent Speech, Device Control, Accessibility","silent speech interface, text entry, autoregressive model, earable computing, acoustic sensing","Silent speech interaction (SSI) allows users to discreetly input text without using their hands. Existing wearable SSI systems typically require custom devices and are limited to a small lexicon, limiting their utility to a small set of command words. This work proposes ReHEarSSE, an earbud-based ultrasonic SSI system capable of generalizing to words that do not appear in its training dataset, providing support for nearly an entire dictionary’s worth of words. As a user silently spells words, ReHEarSSE uses autoregressive features to identify subtle changes in ear canal shape. ReHEarSSE infers words using a deep learning model trained to optimize connectionist temporal classifcation (CTC) loss with an intermediate embedding that accounts for diferent letters and transitions between them. We fnd that ReHEarSSE recognizes 100 unseen words with an accuracy of 89.3%.",100,https://doi.org/10.1145/3613904.3642095,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[21],Choi & Kim,2020,Actuation,Out,N/A,Colored Light (Actuation),N/A,N/A,1,Fine,Yes,Yes,Yes,N/A,Medium,Medium,N/A,N/A,,,,,,,,,,,,N/A,N/A,"In this paper, we present an interactive music sharing service called Toning, that makes users express their musical taste and share music with people in the same space through the most common wearable device, bluetooth earbuds.; Thus, to satisfy the desire of young users, this study suggests an interaction service called Toning, that makes users express their musical taste and share music with people in the same space through the most common wearable device, bluetooth earbuds.",,"Music Player, Social Interaction","Interactive Earphone, Tangible Music Profile, Sharing Music Experience, User Preference, Public Space","The users in their 20s are curious about the feedback after sharing the music that they like, as much as the act of sharing itself. Moreover, they wish to share their musical taste with various people not only with their acquaintance, in everyday spaces and when on the move. In this paper, we present an interactive music sharing service called Toning, that makes users express their musical taste and share music with people in the same space through the most common wearable device, bluetooth earbuds. Our design process started with user survey and interview, aiming to understand their opinion about proper way of expressing music preference, and define design guidelines. Based on this, early concept design was conducted.",57,https://doi.org/10.1145/3374920.3374983,,,,,,,,,,,,,,,,,
[498],Ge et al.,2024,Head Gestures and Pointing,In,Head,Yaw,Microphone,No,1,Fine,Yes,Visual Attention,No,No,Medium,High,High (N=6),High (N=6),No,No,No,No,No,Yes (N=6),Yes,"Standing, Distance, Noise","University Building, Outdoors",Earbud,Research Prototype,No,No,"Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakers generates a periodically fluctuating sound field, which the user’s two earphones detect.; Consequently, head tracking plays a crucial role in human–computer interaction (HCI) applications, such as tracking users’ attention during webpage browsing for content customization or tracking user attention for exhibit introduction in museums. An accurate, user-friendly, and widely applicable head tracking solution compatible with commercial off-the-shelf (COTS) devices is desired for daily use.",,"Customer Analytics, Activity Recognition","Acoustic signal processing, human computer interaction, signal processing, systems, user interfaces","Head tracking is a technique that allows for the measurement and analysis of human focus and attention, thus enhancing the experience of human–computer interaction (HCI). Nevertheless, current solutions relying on vision and motion sensors exhibit limitations in accuracy, user-friendliness, and compatibility with the majority of commercial off-the-shelf (COTS) devices. To overcome these limitations, we present EHTrack, an earphone-based system that achieves head tracking exclusively through acoustic signals. EHTrack employs acoustic sensing to measure the movement of a pair of earphones, subsequently enabling precise head tracking. In particular, a pair of speakers generates a periodically fluctuating sound field, which the user’s two earphones detect. By assessing the distance and angle alterations between the earphones and speakers, we propose a model to determine the user’s head movement and orientation. Our evaluation results indicate a high degree of accuracy in both head movement tracking, with an average tracking error of 2.98 cm,  and head orientation tracking, with an average error of 1.83◦. Furthermore, in a deployed exhibition scenario, we attained an accuracy of 89.2% in estimating the user’s focus direction.",101,https://ieeexplore.ieee.org/document/10192901,DONE,,,"Selection, Orientation",1,fine,absolute,1,No,Yes,X,Partly Present,Not Indicated,Medium (Derived),Not Indicated,Not Indicated,30%
[482],Hu et al.,2024,Head Gestures and Pointing,Enabling,Head,"Yaw, Pitch","Accelerometer, Microphone",No,2,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=50),"Distance, Sitting, Noise, Position, Standing, Running, Walking, Device","Lab, Living Room, Office, Car, Outdoors","Earbud, Headphone",Research Prototype,Yes,No,"Head motion tracking is a promising research field with vast applications in ubiquitous human-computer interaction (HCI) scenarios. Unfortunately, solutions based on vision and wireless sensing have shortcomings in user privacy and tracking range, respectively. To address these issues, we propose IA-Track, a novel head motion tracking system that combines inertial measurement units (IMU) and acoustic sensing.",,"AR/VR, Driving, Motion Tracking","Acoustic signal, head motion tracking, humanmachine interface","Head motion tracking is a promising research field with vast applications in ubiquitous human-computer interaction (HCI) scenarios. Unfortunately, solutions based on vision and wireless sensing have shortcomings in user privacy and tracking range, respectively. To address these issues, we propose IA-Track, a novel head motion tracking system that combines inertial measurement units (IMU) and acoustic sensing. Our wireless earphone-based method balances flexibility, computational complexity, and tracking accuracy, requiring only an earphone with an IMU and a smartphone. However, we still face two challenges. First, wireless earphones have limited hardware resources, making acoustic Doppler effect-based method unsuitable for acoustic tracking. Second, traditional Kalman filter-based trajectory restoration methods may introduce significant cumulative errors. To tackle these challenges, we rely on IMU sensor data to recover the trajectory and use smartphones to emit ”inaudible” acoustic signals that the earphone receives to adjust the IMU drift track. We conducted extensive experiments involving 50 volunteers in various potential IA-Track usage scenarios, demonstrating that our well-designed system achieves satisfactory head motion tracking performance.",102,https://ieeexplore.ieee.org/document/10288089,DONE,,,,,,,,,,,,,,,,
[509],Hu et al.,2024,Head Gestures and Pointing,Enabling,Head,"Yaw, Pitch",Microphone,No,2,Fine,Yes,Yes,No,No,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=32),"Device, Distance, Noise, Position","Lab, Café, Outdoors, Library",Earbud,Commercial,Yes,No,"In this paper, to overcome the abovementioned issues, we strive to develop a low-cost head tracking system, namely HeadTrack, to achieve high-precision and real-time head motion tracking via wireless earphones.",,"AR/VR, Device Control, Privacy","Human–computer interaction, acoustic sensing, acoustic ranging, head motion tracking","Accurate head movement tracking is crucial for virtual reality and Metaverse in ubiquitous human-computer interaction (HCI) applications. Existing works for head tracking with wearable VR kits and wireless signals require expensive devices and heavy algorithmic processing. To resolve this problem, we propose HeadTrack, a low-cost, high-precision head motion tracking system that uses commercially available wireless earphones to capture the user’s head motion in real-time. HeadTrack uses smartphones as ‘sound anchors’ and emits inaudible chirps picked up by the user’s wireless earphones. By measuring the time-of-flight of these signals from the smartphone to each microphone on the earphone, we can deduce the user’s face orientation and distance relative to the smartphone, enabling us to accurately track the user’s head movement. To realize HeadTrack, we use the cross-correlation method to optimize the Frequency Modulated Continuous Wave (FMCW) based acoustic ranging method, which solves the problem of insufficient wireless earphone bandwidth. Moreover, we solve the problems of asynchronous startup time between devices and the existence of sampling frequency offset. We conduct excessive experiments in real scenarios, and the results prove that HeadTrack can continuously track the direction of the user’s head, with an average error under 6.3◦ in pitch and 4.9◦ in yaw.",103,https://ieeexplore.ieee.org/document/10373032,DONE,,,,,,,,,,,,,,,,
[625],Lepold et al.,2024,Eye-Tracking,Enabling,Eye,Vertical Gaze,EOG,Yes,1,Fine,Yes,No,No,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,No,No,"However, currently there is no platform available that is targeted at the ears to sense biopotentials. To address this gap, we introduce OpenEarable ExG - an open-source hardware platform designed to measure biopotentials in and around the ears.; The key contributions of this work include: Our results show that OpenEarable ExG is capable of detecting standard EEG phenomena like alpha brain activity, jaw clenching via EMG, and smooth pursuit eye movements via EOG, all using a simple in-ear electrode configuration.",,Health,"open-source, earables, hearables, bio-potential, Electrooculography, Electroencephalography, Electromyography, EOG, EEG, EMG","While traditional earphones primarily offer private audio spaces, so-called “earables” emerged to offer a variety of sensing capabilities. Pioneering platforms like OpenEarable have introduced novel sensing platforms targeted at the ears, incorporating various sensors. The proximity of the ears to the eyes, brain, and facial muscles has also sparked investigation into sensing biopotentials. However, currently there is no platform available that is targeted at the ears to sense biopotentials. To address this gap, we introduce OpenEarable ExG - an open-source hardware platform designed to measure biopotentials in and around the ears. OpenEarable ExG can be freely configured and has up to 7 sensing channels. We initially validate OpenEarable ExG in a study with a left-right in-ear dualelectrode montage setup with 3 participants. Our results demonstrate the successful detection of smooth pursuit eye movements via Electrooculography (EOG), alpha brain activity via Electroencephalography (EEG), and jaw clenching via Electromyography (EMG). OpenEarable ExG is part of the OpenEarable initiative and is fully open-source under MIT license.",104,https://doi.org/10.1145/3675094.3678480,DONE,,,,,,,,,,,,,,,,
[90],Looney et al.,2014,"Brain, Eye-Tracking",Out,Eye,Blink,EEG,Yes,1,Semantic,Yes,No,Yes,No,Medium,Medium,N/A,N/A,No,No,No,No,No,No,No,Sitting,Lab,Earbud,Research Prototype,No,No,"We present a radically new solution for EEG-based brain computer interface (BCI) where electrodes are embedded on a customized earpiece, as typically used in hearing aids (Ear-EEG). This provides a noninvasive, minimally intrusive and user-friendly EEG platform suitable for long-term use (days) in natural environments.",,BCI-Application,"Ear-EEG, brain computer interface (BCI), non-medical BCI, wearable EEG, steady state visual evoked potential (SSVEP)","We present a radically new solution for EEG-based brain computer interface (BCI) where electrodes are embedded on a customized earpiece, as typically used in hearing aids (Ear-EEG). This provides a noninvasive, minimally intrusive and user-friendly EEG platform suitable for long-term use (days) in natural environments. The operation of Ear-EEG is illustrated for alpha-attenuation and responses to auditory stimuli, and its potential in BCI is evaluated on an SSVEP study. We show that Ear-EEG bitrate performances are comparable with those of on-scalp electrodes, thus promising a quantum step forward for wearable BCI.",19,https://link.springer.com/chapter/10.1007/978-3-642-54707-2_5,DONE,,,,,,,,,,,,,,,,
[449],Ronco et al.,2024,Hand Gestures and Location,In,Hand,"Pinch (Mid-Air), Tilt (Mid-Air), Slide (Mid-Air), Push (Mid-Air), Pull (Mid-Air), Rub (Mid-Air), Circle (Mid-Air), Hold (Mid-Air)",mmWaveRadar,Yes,11,Semantic,No,Yes,Yes,No,Low,Medium,High (N=20),N/A,No,No,No,No,No,Yes (N=20),No,Sitting,Lab,Earbud,Research Prototype,Yes,Yes,"Hand Gesture Recognition (HGR) has been proposed as an intuitive Human-Machine Interface, potentially suitable for controlling several classes of devices in the context of the Internet of Things. This paper proposes a low-power in-ear HGR system based on mm-wave radars, efficient spatial and temporal Convolutional Neural Networks and an energy-optimized hardware design.; However, their limitations are becoming more evident when it comes to adapting to the evolving landscape of ubiquitous, and perhaps simple devices such as wearable systems, where the integration of traditional interfaces is not a viable option for physical constraints, strict energy requirements, or purely aesthetic reasons. The need for more intuitive and adaptive solutions has become increasingly evident, prompting extensive research into alternative technologies.",,Device Control,"mm-wave, radar, gesture recognition, low-power, embedded, sensor","Smart Internet of Things (IoT) devices are on the rise in popularity, with innovative use cases and applications emerging every year. Including intelligence in these novel systems presents the challenge of integrating interaction and communication in scenarios where traditional interfaces are not viable. Hand Gesture Recognition (HGR) has been proposed as an intuitive Human-Machine Interface, potentially suitable for controlling several classes of devices in the context of the Internet of Things. This paper proposes a low-power in-ear HGR system based on mm-wave radars, efficient spatial and temporal Convolutional Neural Networks and an energy-optimized hardware design. The design is suitable for battery-operated devices, with stringent size and energy constraints, enabling user interaction with wearable devices, but also suitable for home appliances and industrial applications. The proposed machine learning model is characterized thoroughly for robustness and generalization capabilities, achieving 94.9% (single subject) and 86.1% (LeaveOne-Out Cross-validation) accuracy on a set of 11+1 gestures with a model size of only 36 KiB and inference latency of 32.4 ms on a 64 MHz Cortex-M33 microcontroller, making it compatible with real-time applications. The system is demonstrated in a fully integrated, miniaturized in-ear device with a full-system average power consumption of 18.4 mW, a more than 6x improvement on the current state of the art.",105,https://ieeexplore.ieee.org/document/10562162,DONE,,,"Selection, Path, Orientation",3,X,relative,11,Yes,No,X,Present,Not Indicated,High (Derived),Not Indicated,Present,54%
[635],Sato et al.,2024,Ear and Earable,In,Hand,"Pull (Ear), Swipe (Ear), Pinch (Ear), Fold (Ear)","Accelerometer, Gyroscope, Magnetometer",Yes,15,Semantic,No,Yes,No,No,Medium,Medium,Low (N=10),Low (N=10),Yes (N=19),No,No,No,No,Yes (N=10),No,"Sitting, Walking",Lab,Earbud,Commercial,No,No,"however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures.; Consequently, a compelling demand exists for a device operation method that uses only hearables.; Consequently, there is a need to systematically identify and classify gesture sets that align with various input technologies, particularly by exploring user-defined gestures under defined interaction area constraints.; Consequently, it becomes important to investigate user-defined gestures that are suitable for scenarios where the user is only wearing the hearables.",,"Device Control, Music Player, Phone Calls, Communication, Health, Data Annotation, Device Input","Hearables, Hands Gesture Recognition, User-Defined Gesture, Gesture Elicitation Study, IMU","Hearables are highly functional earphone-type wearables; however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures. In previous research on hearables for hand input, user understanding and gesture recognition systems have been developed. However, in the realm of user understanding, investigation concerning hand input with hearables remains incomplete, and existing recognition systems have not demonstrated proficiency in discerning user-defined gestures. In this study, we conducted a gesture elicitation study (GES) assuming hand input using hearables under six conditions (three interaction areas × two device shapes). Then, we extracted ear-level gestures that the device’s built-in IMU sensor could recognize from the user-defined gestures and investigated the recognition performance. The results of sitting experiments showed that the gesture recognition rate for in-ear devices was 91.0% and that for ear-hook devices was 74.7%.",106,https://dl.acm.org/doi/10.1145/3676503,DONE,,,"Selection, Position, Path",3,X,relative,47,Yes,No,X,Present (N=19),Not Indicated,High (Derived),Not Indicated,Partly Present,23%
[291],Shojaeifard et al.,2024,Head Gestures and Pointing,Enabling,Head,Yaw,"Accelerometer, Gyroscope",Yes,1,Coarse,Yes,Yes,No,No,Medium,High,High (N=6),Medium (N=1),No,No,No,No,No,Yes (N=6),No,Sitting,"Lab, Car",Earbud,Commercial,No,No,"In-vehicular sensing, i.e. sensing the movements of a person driving a moving vehicle, sets its own challenges for the data quality. Different wearable sensors (wristbands, smart rings, glasses etc.) can detect the driver’s condition, such as sleepiness and attention level, and behavioural patterns, such as head movements and the extent of the field of vision [4]. Considering the potentiality and challenges of head movement detection technology, we aim to detect head movement by applying machine learning models for lightweight gyroscope data.; the earables for improving traffic safety. Possible use cases include reminding drivers to look at so-called dead angles and vehicle blind spots [3, 13] when passing lines and teaching such activity to new drivers.",,"Driving, Safety","earables, vehicular sensing, head position tracking","The Internet of Things is enabling innovations in the automotive industry by expanding the capabilities of vehicles by connecting them with the cloud. One important application domain is traffic safety, which can benefit from monitoring the driver’s condition on how safely they are handling the vehicle. By detecting drowsiness, inattentiveness, and distraction of the driver, it is possible to react before accidents happen. This paper uses accelerometer and gyroscope data collected using an ear-worn sensor to classify the orientation of the driver’s head in a moving vehicle. We show that lightweight machine learning algorithms such as Random Forest and K-Nearest Neighbor can be used to reach accurate classifications even without applying any noise reduction to the signal data. Data cleaning and transformation approaches are studied to see how they give deeper insights into the classification problem. This study paves the way for the development of driver monitoring systems capable of reacting to anomalous driving behaviour before traffic accidents can happen.",107,https://doi.org/10.1145/3627050.3627067,DONE,,,,,,,,,,,,,,,,
[640],Srivastava et al.,2024,Mouth,In,Speech Apparatus,"Silent Speech (Words), Silent Speech (Sentences)","Accelerometer, Gyroscope",No,65,Semantic,Yes,Yes,Yes,No,High,High,Medium (N=19),N/A,No,No,No,No,No,Yes (N=19),Yes (N=19),"Sitting, Walking, Noise",Lab,Custom Device,Research Prototype,No,No,"We present Unvoiced, a novel unvoiced user interface that leverages jaw motion to enable users to silently interact with their devices using earables.; It is evident that there is a need for an unvoiced interface that allows users to seamlessly communicate with their devices privately, without the impact of background noise, but also without using obtrusive sensing devices that are not socially acceptable.; Is it possible to harness the capabilities of LLMs to support unvoiced user interactions using jaw motion only? To design this LLM-assisted robust UUI, our key intuition is to translate IMU data to audio spectrograms which can then be converted to text using off-the-shelf LLMs.",,"Silent Speech, Device Input, Privacy, Security, Communication, Authentification, Accessibility, AR/VR, Music Player, Device Control","Accessible Interfaces, Silent Speech, Transformers, Earables, IMU Sensing, GPT, LLM","We present Unvoiced, a novel unvoiced user interface that leverages jaw motion to enable users to silently interact with their devices using earables. The core idea is to translate low-frequency jaw motion signals into high-frequency information-rich mel spectrograms. Our proposed cross-modal translation incorporates phonetic, contextual, and syntactic information, while the specialized loss function optimizes for these linguistic features. This ensures that the generated spectrograms capture nuanced speech characteristics. Evaluated for 19 users across four tasks, Unvoiced demonstrates >94% task completion rate and <9% word error rate for over 90% of phrases. Further, Unvoiced maintains >90% task completion rate in noisy conditions.",108,https://doi.org/10.1145/3666025.3699374,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[275],Shimon et al.,2024,"Ear and Earable, Hand Gestures and Location",Elicitation,N/A,N/A,N/A,N/A,0,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,N/A,Yes (N=18),No,No,No,No,No,No,Sitting,Lab,N/A,N/A,N/A,N/A,"Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration.; However, the small form factor and input area constraint in wireless earbuds restrict physical touch interaction to simple on-device taps and swipes, thus limiting their input space.; Although prior literature explored off-device uni-manual gesture reuse across different segmented regions for outer-ear touches to support gesture reusability, similar exploration is lacking for around-ear on-skin space (i.e., face, head, and neck) and above-ear mid-air spaces for supporting similar gestures.; We address this research gap by studying the reuse of different gesture classes at different mid-air and onskin gesture regions and the effect of increasing the number of gesture regions on quantitative and qualitative gesture performance metrics for mid-air and on-skin interaction spaces.; Understanding of end-user preference shift from one interaction space to another for increasing gesture reuse; Analysis of the effects of different factors (i.e., the number of gesture regions and associated boundaries) on interaction space segmentation for uni-manual, off-device earable gestures.",,Device Control,"Embodied Interaction, Input Techniques, Uni-manual Interaction, Touch Surfaces, Earbased Interaction, Earables","Small form factor limits physical input space in earable (i.e., ear-mounted wearable) devices. Off-device earable inputs in alternate mid-air and on-skin around-ear interaction spaces using uni-manual gestures can address this input space limitation. Segmenting these alternate interaction spaces to create multiple gesture regions for reusing off-device gestures can expand earable input vocabulary by a large margin. Although prior earable interaction research has explored off-device gesture preferences and recognition techniques in such interaction spaces, supporting gesture reuse over multiple gesture regions needs further exploration. We collected and analyzed 7560 uni-manual gesture motion data from 18 participants to explore earable gesture reuse by segmentation of on-skin and mid-air spaces around the ear. Our results show that gesture performance degrades significantly beyond 3 mid-air and 5 on-skin around-ear gesture regions for different uni-manual gesture classes (e.g., swipe, pinch, tap). We also present qualitative findings on most and least preferred regions (and associated boundaries) by end-users for different uni-manual gesture shapes across both interaction spaces for earable devices. Our results complement earlier elicitation studies and interaction technologies for earables to help expand the gestural input vocabulary and potentially drive future commercialization of such devices.",109,https://dl.acm.org/doi/10.1145/3643513,DONE,,,,,,,,,,,,,,,,
[654],Srivastava et al.,2024,Mouth,In,Speech Apparatus,Silent Speech (Sentences),"Accelerometer, EMG, Gyroscope",No,12,Semantic,Yes,Yes,No,Yes,High,High,High (N=9),N/A,No,No,No,No,No,Yes (N=9),No,Sitting,Lab,"Headphone, Earbud",Research Prototype,Yes,No,"In this paper, we present QuietSync, a multimodal system that combines inertial measurement unit (IMU) and contact electrode (ExG) signals to achieve accurate silent speech recognition using of-the-shelf devices.; To the best of our knowledge, QuietSync is the frst system to enable silent speech interaction for multiple form factors.; This paper introduces QuietSync, a novel multi-modal system that addresses these challenges by combining IMUs and novel, foambased, dry electrodes (ExG). QuietSync aims to overcome the limitations of previous works by providing a versatile and user-friendly solution for silent speech interaction. The system’s key contributions lie in its ability to enable SSI across four diferent form factors—headphones, earphones, glasses, and VR/XR systems—making it the frst of its kind.",,"AR/VR, Privacy, Accessibility, Communication, Device Control, Phone Calls, Device Input, Video Conference, Music Player","Silent speech recognition, Accessibility, EXG, IMU sensing","Silent speech recognition has emerged as a promising approach for enabling hands-free and discreet interaction with head-worn devices. In this paper, we present QuietSync, a multimodal system that combines inertial measurement unit (IMU) and contact electrode (ExG) signals to achieve accurate silent speech recognition using of-the-shelf devices. QuietSync utilizes an IMU attached to the lower part of the headphones near the ear and strategically places ExG electrodes on the headphones, glasses (nose and behind the ear), and face (for VR applications) to capture subtle movements and muscle activity associated with silent speech production. We conducted a user study with 9 participants and successfully recognized 12 commands with an accuracy of 94.2%. Our system leverages the complementary nature of IMU and ExG signals to enhance the robustness and reliability of silent speech recognition. The IMU captures subtle movements of the jaw and facial muscles, while the ExG electrodes detect low-amplitude surface muscle activity associated with speech production. We show that our system is not afected by the length and speech mannerisms of the commands, and can be fne-tuned for users of varied native languages with only 5 samples. Our fndings demonstrate the feasibility of using of-the-shelf head-worn devices to enable silent speech recognition, opening up new possibilities for seamless and discreet interaction with devices such as VR/AR headsets and earables. To the best of our knowledge, QuietSync is the frst system to enable silent speech interaction for multiple form factors.",110,https://doi.org/10.1145/3678957.3685720,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[506],Sun et al.,2024,Mouth,In,Speech Apparatus,"Silent Speech (Phonemes), Silent Speech (Words)","Microphone, Speaker",Yes,76,Semantic,Yes,Yes,No,No,High,High,High (N=50),Medium (N=50),No,Yes (N=50),No,No,No,Yes (N=50),No,"Sitting, Noise, Wearing Styles, Mask, Hand Motion, Head Movement, Walking, Music",Lab,Earbud,Research Prototype,No,No,"In this paper, we present EarSSR, an earphone-based silent speech recognition system to enable interaction without a need of vocalization. The key insight is that when people are speaking, their ear canals exhibit unique deformation patterns and the corresponding deformation patterns are related to words/letters even without any vocalization. We utilize the built-in microphone and speaker of an earphone to capture the ear canal deformation.; The above limitations motivate us to propose an alternative speech recognition system. We propose to utilize the most popular wearable devices, i.e., earphones to achieve silent speech recognition.",,Silent Speech,"Acoustic sensing, silent speech recognition, earphone","As the most natural and convenient way to communicate with people, speech is always preferred in HumanComputer Interactions. However, voice-based interaction still has several limitations. It raises privacy concerns in some circumstances and the accuracy severely degrades in noisy environments. To address these limitations, silent speech recognition (SSR) has been proposed, which leverages the inaudible information (e.g., lip movements and throat vibration) to recognize the speech. In this paper, we present EarSSR, an earphone-based silent speech recognition system to enable interaction without a need of vocalization. The key insight is that when people are speaking, their ear canals exhibit unique deformation patterns and the corresponding deformation patterns are related to words/letters even without any vocalization. We utilize the built-in microphone and speaker of an earphone to capture the ear canal deformation. Ultrasound signals are emitted and the reflected signals are analyzed to extract the signal features corresponding to speech-induced ear canal deformation for silent speech recognition. We design a two-channel hierarchical convolutional neural network to achieve fine-grained letter/word recognition. Our extensive experiments show that EarSSR can achieve an accuracy of 82% for single alphabetic letter recognition and an accuracy of 93% for word recognition.",111,https://ieeexplore.ieee.org/document/10411110,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[639],Suzuki et al.,2024,Hand Gestures and Location,In,Hand,"Swipe (Mid-Air), Twist (Mid-Air), Grip (Mid-Air), Squeeze (Mid-Air), Calling Gesture","Speaker, Microphone",Yes,7,Semantic,No,Yes,Yes,No,Low,Medium,Low (N=20),Low (N=20),Yes (N=11),No,Yes (N=11),No,Yes (N=11),Yes (N=20),No,"Sitting, Gloves, Walking, Surplus Person, Music",Lab,Earbud,Research Prototype,No,No,"We introduce EarHover, an innovative system that enables mid-air gesture input for hearables. Mid-air gesture input, which eliminates the need to touch the device and thus helps to keep hands and the device clean.; We propose EarHover (Figure 1), a system that minimizes the cost of additional sensors and enables mid-air gesture recognition.",,Device Input,"Hearables, mid-air gesture recognition, Doppler efect, sound leakage, deep learning, acoustic sensing","We introduce EarHover, an innovative system that enables mid-air gesture input for hearables. Mid-air gesture input, which eliminates the need to touch the device and thus helps to keep hands and the device clean. However, existing mid-air gesture input methods for hearables have been limited to adding cameras or infrared sensors. By focusing on the sound leakage phenomenon unique to hearables, we have realized mid-air gesture recognition using a speaker and an external microphone that are highly compatible with hearables. The signal leaked to the outside of the device due to sound leakage can be measured by an external microphone, which detects the diferences in refection characteristics caused by the hand’s speed and shape during mid-air gestures. Among 27 types of gestures, we determined the seven suitable gestures for EarHover in terms of signal discrimination and user acceptability. We then evaluated the gesture detection and classifcation performance of two prototype devices (in-ear type/open-ear type) for real-world application scenarios.",112,https://dl.acm.org/doi/10.1145/3654777.3676367,DONE,,,"Selection, Path, Orientation",4,X,relative,27,Yes,No,X,Present (N=11),Not Indicated,Low (Derived),High (N=11),Present,37%
[286],Wang et al.,2024,Hand Gestures and Location,In,Hand,Slide (Face),Microphone,Yes,5,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,Yes (N=20),No,No,Yes (N=20),No,Yes (N=26),"Noise, Music, Sitting, Standing, Sports, Walking, Make-Up","Living Room, Office, Outdoors, Car",Earbud,Research Prototype,No,No,"There is an increasing demand for robust earables authentication because of the growing amount of sensitive information and the IoT devices that the earable could access.; In particular, we repurpose the face and earables as a natural scanner to indirectly sense the fingerprint biometrics.",,Authentification,"Biometrics, Fingerprint, Friction, User Authentication, Earable","Ear wearables (earables) are emerging platforms that are broadly adopted in various applications. There is an increasing demand for robust earables authentication because of the growing amount of sensitive information and the IoT devices that the earable could access. Traditional authentication methods become less feasible due to the limited input interface of earables. Nevertheless, the rich head-related sensing capabilities of earables can be exploited to capture human biometrics. In this paper, we propose EarSlide, an earable biometric authentication system utilizing the advanced sensing capacities of earables and the distinctive features of acoustic fingerprints when users slide their fingers on the face. It utilizes the inward-facing microphone of the earables and the face-ear channel of the ear canal to reliably capture the acoustic fingerprint. In particular, we study the theory of friction sound and categorize the characteristics of the acoustic fingerprints into three representative classes, pattern-class, ridge-groove-class, and coupling-class. Different from traditional fingerprint authentication only utilizes 2D patterns, we incorporate the 3D information in acoustic fingerprint and indirectly sense the fingerprint for authentication. We then design representative sliding gestures that carry rich information about the acoustic fingerprint while being easy to perform. It then extracts multi-class acoustic fingerprint features to reflect the inherent acoustic fingerprint characteristic for authentication. We also adopt an adaptable authentication model and a user behavior mitigation strategy to effectively authenticate legit users from adversaries. The key advantages of EarSlide are that it is resistant to spoofing attacks and its wide acceptability. Our evaluation of EarSlide in diverse real-world environments with intervals over one year shows that EarSlide achieves an average balanced accuracy rate of 98.37% with only one sliding gesture.",113,https://dl.acm.org/doi/10.1145/3643515,DONE,,,"Position, Path",2,X,relative,5,Yes,No,X,Partly Present,High (N=20),Medium (Derived),Not Indicated,Not Indicated,38%
[532],Wang et al.,2024,Hand Gestures and Location,In,Hand,"Slide (Face), Tap (Face)","Accelerometer, Gyroscope",Yes,7,Semantic,No,Yes,Yes,Yes,Medium,High,N/A,N/A,No,No,No,No,No,No,Yes (N=20),Sitting,Lab,Earbud,Commercial,No,No,"However, the widespread adoption of wireless earbuds has spurred concerns regarding security and privacy, necessitating robust bespoke security measures. Despite the miniaturization of mobile chips enabling the integration of sophisticated algorithms into smart wearables, the research and industrial communities have yet to accord adequate attention to earbud security.; This article focuses on empowering wireless earbuds to authenticate their legitimate users, tackling the challenges associated with conventional authentication methods. Instead of relying on input interface authentication methods like PIN or lock patterns, this research delves into leveraging inertial measurement unit (IMU) data collected during interactions with devices to extract novel biometric features, presenting an alternative approach that nonetheless confronts challenges related to signal capture and interference.; Especially as the demand for interactive functions on smart earbuds grows, necessitating increased user permissions and information access, highlighting the importance of privacy and security measures.",,Authentification,"Adversarial learning, implicit authentication, wearable computing","The surge in popularity of wireless headphones, particularly wireless earbuds, as smart wearables, has been notable in recent years. These devices, empowered by artificial intelligence (AI), are broadening their utility in areas such as speech recognition, augmented reality, pose recognition, and health care monitoring, thereby enriching user experiences through novel interactive interfaces driven by embedded sensors. However, the widespread adoption of wireless earbuds has spurred concerns regarding security and privacy, necessitating robust bespoke security measures. Despite the miniaturization of mobile chips enabling the integration of sophisticated algorithms into smart wearables, the research and industrial communities have yet to accord adequate attention to earbud security. This article focuses on empowering wireless earbuds to authenticate their legitimate users, tackling the challenges associated with conventional authentication methods. Instead of relying on input interface authentication methods like PIN or lock patterns, this research delves into leveraging inertial measurement unit (IMU) data collected during interactions with devices to extract novel biometric features, presenting an alternative approach that nonetheless confronts challenges related to signal capture and interference. Consequently, we propose and design BudsAuth, an implicit user authentication framework that harnesses built-in IMU sensors in smart earbuds to capture vibration signals induced by onface touching interactions with the earbuds. These vibrations are utilized to deliver continuous and implicit user authentication with high precision and compatibility across various earbud models. Extensive evaluation demonstrates BudsAuth’s capability to achieve an equal error rate (EER) of 0.0003, representing an approximate 99.97% accuracy with seven consecutive samples of interactive gestures for implicit authentication.",114,https://ieeexplore.ieee.org/abstract/document/10478100,DONE,,,"Selection, Position, Path",2,X,relative,7,Yes,No,X,Not Present,Not Indicated,Not Indicated,Not Indicated,Present,25%
[70],Bleichner & Debener,2017,Eye-Tracking,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We also describe the cEEGrid flex-printed sensor array, which enables unobtrusive multi-channel EEG acquisition from around the ear. In a number of validation studies we found that the cEEGrid enables the recording of meaningful continuous EEG, event-related potentials and neural oscillations.",,,"mobile EEG, ear-centered EEG, ear EEG, transparent EEG, wearable EEG","Electroencephalography (EEG) is an important clinical tool and frequently used to study the brain-behavior relationship in humans noninvasively. Traditionally, EEG signals are recorded by positioning electrodes on the scalp and keeping them in place with glue, rubber bands, or elastic caps. This setup provides good coverage of the head, but is impractical for EEG acquisition in natural daily-life situations. Here, we propose the transparent EEG concept. Transparent EEG aims for motion tolerant, highly portable, unobtrusive, and near invisible data acquisition with minimum disturbance of a user’s daily activities. In recent years several ear-centered EEG solutions that are compatible with the transparent EEG concept have been presented. We discuss work showing that miniature electrodes placed in and around the human ear are a feasible solution, as they are sensitive enough to pick up electrical signals stemming from various brain and non-brain sources. We also describe the cEEGrid flex-printed sensor array, which enables unobtrusive multi-channel EEG acquisition from around the ear. In a number of validation studies we found that the cEEGrid enables the recording of meaningful continuous EEG, event-related potentials and neural oscillations. Here, we explain the rationale underlying the cEEGrid ear-EEG solution, present possible use cases and identify open issues that need to be solved on the way toward transparent EEG.",31,https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2017.00163/full,,,,,,,,,,,,,,,,,
[676],Wang et al.,2024,Hand Gestures and Location,In,Hand,Write (Face),Microphone,Yes,36,Semantic,No,Yes,Yes,Yes,Low,Medium,High (N=10),High,No,Yes (N=20),No,No,No,Yes (N=10),No,"Sitting, Standing, Head Movement, Walking","Office, Living Room, Car, Outdoors",Earbud,Research Prototype,No,No,"As earables gain popularity, there emerges a need for intuitive user interfaces that adapt to diverse daily scenarios. Traditional methods like touchscreens and voice control often fall short in environments like movie theatres, where silence and darkness are required, or on busy streets where visual distraction introduces extra risk. We propose an innovative earable-based system utilizing unique acoustic friction generated by fingers for alphanumeric input.; This paper introduces FaceTyping, an interaction system that enables alphanumeric handwriting on the face using passive acoustic sensing in earables.",,"Device Input, Privacy","Earable, Face and Ear Interaction, Gestures Recognition, Acoustic Sensing","As earables gain popularity, there emerges a need for intuitive user interfaces that adapt to diverse daily scenarios. Traditional methods like touchscreens and voice control often fall short in environments like movie theatres, where silence and darkness are required, or on busy streets where visual distraction introduces extra risk. We propose an innovative earable-based system utilizing unique acoustic friction generated by fingers for alphanumeric input. Our approach digs into the acoustic friction theory, applying this knowledge to better understand the transformation from 2D handwriting into a 1D acoustic time series. This theoretical foundation guides our system design and feature extraction. Specifically, we have redesigned certain characters to enhance their acoustic distinctiveness without compromising the natural handwriting style of users, ensuring the system userfriendly. Our system combines DenseNet and GRU architectures in a multimodal model, refined through transfer learning to adapt to diverse user behaviors. Tested in real-world scenarios with 10 participants, our system achieves a 95% accuracy in recognizing both letters and numbers.",115,https://ieeexplore.ieee.org/document/10637602,DONE,,,"Selection, Path, Quantification, Text",2,X,relative,36,Yes,No,X,Present,High (N=20),High (Derived),Not Indicated,Present,62%
[675],Xie et al.,2024,Head Gestures and Pointing,Enabling,Head,General Head Movement,"Accelerometer, Gyroscope",No,1,Semantic,Yes,Yes,Yes,No,Low,Medium,N/A,N/A,No,No,No,No,No,No,Yes (N=30),"Sitting, Music, Noise, Standing, Walking, Cycling, Wearing Styles","Lab, Outdoors",Earbud,Commercial,Yes,No,"In this paper, we propose another earphone-based authentication system, namely EarPass, that leverages users’ unique head motion patterns in response to a very short period of music segment.; We present a practical earphone-based authentication system named EarPass that captures natural head motions through wireless earphones from users while they listen to music segments. This user-centric approach obviates the need for additional authentication steps.",,Authentification,"ubiquitous computing, user authentication, earphones, head motion","With the growing reliance on digital systems in today’s mobile Internet era, robust authentication methods are crucial for safeguarding personal data and controlling access to resources. Conventional methods, such as knowledge-based and biometric-based authentication, are widely used but still have some usage limitations and potential security concerns, like wearing protective suits/masks or being imitated by attackers with ulterior motives. In this paper, we propose another earphone-based authentication system, namely EarPass, that leverages users’ unique head motion patterns in response to a very short period of music segment. Here, we employ a Convolutional Neural Network (CNN)-based feature extractor to capture and map distinct head motions into a well-separated latent space, achieving high-dimensional data extraction. We demonstrate the consistency, uniqueness, and robustness of head motion patterns through extensive experiments and reach a 98.2% F1-score, indicating superior performance compared to conventional authentication methods. Additionally, EarPass is user-friendly, secure, and adaptable to various environments, including noisy and movement-oriented scenarios. By integrating the authentication system into Android devices, we showcase its real-world applicability and low energy consumption with minimal latency. The source code of EarPass will be open-source to further research and collaboration within the community.",116,https://ieeexplore.ieee.org/document/10631065,DONE,,,,,,,,,,,,,,,,
[354],Yang et al.,2024,"Ear and Earable, Hand Gestures and Location",In,Hand,"Press (Face), Pinch (Face), Pinch (Ear), Cover (Face), Scratch (Face), Close (Mid-Air), Open (Mid-Air), Slide (Mid-Air), Click (Mid-Air), Approach (Mid-Air)","Speaker, Microphone",Yes,10,Semantic,No,Yes,No,No,Low,Medium,High (N=22),Medium (N=1),No,Yes (N=22),Yes (N=22),No,Yes (N=22),Yes (N=22),No,"Sitting, Noise, Distance, Hydration, Standing, Walking, Sports, Eating, Speaking, Music",Lab,Earbud,Research Prototype,No,No,"However, these systems face limitations in capturing contactless gestures (i.e., those gestures performed over the face) because such gestures do not generate signals that are detectable by the aforementioned sensors.; can we design a system that is able to detect hand-to-face gestures, whether in contact with the face or above it, using widely accessible mobile devices?; Firstly, its wearable nature guarantees that users can move freely without any inconvenience or hindrance, always interacting with the device seamlessly.; Secondly, MAF does not depend on specialized sensors or require any modifications to standard bone conduction earphones.; We then craft a plethora of user studies to i) examine the impact of various human factors on the mobile acoustic field; and ii) assess the social acceptance of this mobile acoustic field-based gesture interaction by interviewing 22 participants.",,"Music Player, AR/VR, Health, Device Control","Wearable Computing, Gesture Detection, Acoustic Sensing","We present MAF, a novel acoustic sensing approach that leverages the commodity hardware in bone conduction earphones for handto-face gesture interactions. Briefly, by shining audio signals with bone conduction earphones, we observe that these signals not only propagate along the surface of the human face but also dissipate into the air, creating an acoustic field that envelops the individual’s head. We conduct benchmark studies to understand how various handto-face gestures and human factors influence this acoustic field. Building on the insights gained from these initial studies, we then propose a deep neural network combined with signal preprocessing techniques. This combination empowers MAF to effectively detect, segment, and subsequently recognize a variety of hand-to-face gestures, whether in close contact with the face or above it. Our comprehensive evaluation based on 22 participants demonstrates that MAF achieves an average gesture recognition accuracy of 92% across ten different gestures tailored to users’ preferences.",117,https://dl.acm.org/doi/10.1145/3613904.3642437,DONE,,,Selection,"1, X",X,"absolute, relative",12,Yes,No,X,Partly Present,Medium (N=22),Low (Derived),Medium (N=22),Present,43%
[86],Bedri et al.,2015,Mouth,Out,Jaw,General Jaw Movement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19,,,,,,,,,,,,,,,,,,
[527],Yi et al.,2024,Mouth,In,Speech Apparatus,Silent Speech (Phonemes),EMG,Yes,10,Semantic,Yes,Yes,No,No,High,High,Medium (N=33),Medium (N=3),No,No,No,No,No,Yes (N=33),No,"Sitting, Time",Lab,Custom Device,Research Prototype,No,No,"Our previous work demonstrated the feasibility of recognizing silent commands through around-ear biosensors with the limitation of user adaptation. In this work, we ease the limitation by a personalization framework that integrates spectral factorization of signals, temporal confidence rejection and commonly used transfer learning algorithms.",,Silent Speech,"EMG signals, silent command recognition, user adaptation","Wearable human-computer interactions in daily life are increasingly encouraged by the prevalence of intelligent wearables. It poses a demanding requirement of micro-interaction and minimizing social awkwardness. Our previous work demonstrated the feasibility of recognizing silent commands through around-ear biosensors with the limitation of user adaptation. In this work, we ease the limitation by a personalization framework that integrates spectral factorization of signals, temporal confidence rejection and commonly used transfer learning algorithms. Specifically, we first empirically formulate the user adaptation issue by presenting the accuracies of applying transfer learning algorithms to our previous method. Second, we improve the signal-to-noise ratio by proposing the supervised spectral factorization method that learns the amplitude and phase mappings between around-ear signals and the signals of articulated facial muscles. Third, we leverage the time continuity of commands and introduce the time decay into confidence rejection. Finally, extensive experiments have been conducted to evaluate the feasibility and improvements. The results indicate an average accuracy of 92.38% which is significantly larger than solely using transfer learning algorithms. And a comparable accuracy can be achieved with significantly reduced data of new users. The overall performance shows the framework can significantly improve the accuracy of user adaptations. The work would aid a further step toward commercial products for silent command recognition and inspire the solution to the user adaptation challenge of wearable human-computer interactions.",118,https://doi.org/10.1109/TNSRE.2023.3342068,DONE,,,Selection,X,X,X,,Yes,No,,,,,,,
[712],Manabe et al.,2012,Hand Gestures and Location,Duplicate of [23],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[617],Hossain et al.,2019,,Duplicate of [622],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[621],Wang et al.,2021,,Duplicate of [306],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[629],Futami et al,2022,,Duplicate of [627],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[281],Ma et al.,2022,,Duplicate of [626],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[335],Choi et al.,2023,,Duplicate of [345],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[299],Wang et al.,2023,Mouth,Duplicate of [306],Teeth,"Click, Slide",Microphone,,,,Yes,Yes,,,,,,,,,,,,,,,,,,,,,,Authentification,,,-,,,,,"Position, Selection, Path",X,X,absolute,,Yes,No,,,,,,,
[32],Metzger et al.,2004,Hand Gestures and Location,Singled Loc,Hand,"Slide (Mid-Air), Hold (Mid-Air)",Proximity Sensor,Present,6,,No,Yes,Not Present,Not Present,Low (Derived),Low (Derived),High (N=1),Not Indicated,,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Accessibility, Device Control",,,-,,Done,,,"Selection, Path, Quantification",1,coarse,relative,6,Yes,Yes,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,32%
[32],Metzger et al.,2004,Head Gestures and Pointing,Singled Loc,Head,Roll,Accelerometer,Present,1,,No,Yes,Not Present,Not Present,Medium (Derived),High (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Accessibility, Device Control",,,-,,Done,,,"Path, Orientation",1,X,relative,1,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,22%
[17],Ando et al.,2017,Face,Singled Loc,Jaw,Facial Gesture (Mouth),Pressure Sensor,Present,6,,Yes,Yes,Partly Present,Not Present,Medium (Derived),Medium (Derived),High (N=12),Medium (N=6),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, Device Control",,,-,,Done,,,Selection,2,X,relative,6,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,34%
[17],Ando et al.,2017,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Roll, Yaw",Pressure Sensor,Present,5,,Yes,Yes,Partly Present,Not Present,Medium (Derived),High (Derived),Medium (N=12),Medium (N=6),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, Device Control",,,-,,Done,,,"Selection, Orientation, Path",3,X,relative,5,Yes,No,X,Not Present,Not Indicated,Medium (Derived),Not Indicated,Present,31%
[12],Matthies et al.,2017,Face,Singled Loc,Facial Expression,"Smile, Facial Gesture (Mouth), Facial Gesture (Eye)","EMG, Capacitive Sensor, Electrical Field Sensing",Present,11,,Yes,Visual Attention,Not Present,Not Present,Low (Derived),Low (Derived),Low (N=1),Medium (N=3),,,,,,,,,,,,Not Present,Not Present,,,Device Input,,,-,,Done,,,"Selection, Orientation",2,X,relative,11,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,18%
[12],Matthies et al.,2017,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Yaw","EMG, Capacitive Sensor, Electrical Field Sensing",Present,4,,Yes,Yes,Not Present,Not Present,Medium (Derived),High (Derived),Low (N=1),Medium (N=3),,,,,,,,,,,,Not Present,Not Present,,,Device Input,,,-,,Done,,,"Selection, Path, Orientation",2,X,relative,4,Yes,No,X,Not Present,Not Indicated,Low (Derived),Not Indicated,Present,23%
[111],Wang et al.,2017,Eye-Tracking,Singled Loc,"Facial Expression, Gaze",,"EOG, EMG",,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Device Control,,,-,,,,,Selection,2,coarse (4),relative,,Yes,No,,,,,,,
[111],Wang et al.,2017,Face,Singled Loc,"Facial Expression, Gaze",,"EOG, EMG",,,,Yes,No,,,,,,,,,,,,,,,,,,,,,,Device Control,,,-,,,,,Selection,2,coarse (4),relative,,Yes,No,,,,,,,
[8],Amesaka et al.,2019,Face,Singled Loc,Facial Expression,Facial Gesture (Mouth),"Microphone, Speaker",Present,3,,Yes,Yes,Not Present,Not Present,Medium (Derived),Medium (Derived),Medium (N=11),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input, Music Player",,,-,,Done,,,"Selection, Orientation, Path",1,X,relative,12,Yes,No,X,Present (N=11),Not Indicated,Medium (Derived),Not Indicated,Present,29%
[8],Amesaka et al.,2019,Head Gestures and Pointing,Singled Loc,Head,Roll,"Microphone, Speaker",Present,2,,Yes,Yes,Not Present,Not Present,Medium (Derived),High (Derived),Medium (N=11),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input, Music Player",,,-,,Done,,,"Selection, Path, Orientation",1,X,relative,6,Yes,No,X,Present (N=11),Not Indicated,Medium (Derived),Not Indicated,Present,31%
[234],Chen et al.,2020,Ear and Earable,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[234],Chen et al.,2020,Hand Gestures and Location,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[26],Xu et al.,2020,Ear and Earable,Singled Loc,Hand,"Tap (Ear), Slide (Ear)",Microphone,Present,3,,No,Yes,Present,Present,Medium (Derived),High (N=16),Medium (N=18),Medium (N=18),,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Communication, Device Input, AR/VR",,,-,,Done,,,"Selection, Position, Path",1,X,relative,8,Yes,No,X,Present (N=16),High (N=12),High (Derived),Medium (N=16),Present,73%
[26],Xu et al.,2020,Hand Gestures and Location,Singled Loc,Hand,"Tap (Face), Slide (Face)",Microphone,Present,5,,No,Yes,Present,Present,Medium (Derived),High (N=16),High (N=18),Medium (N=18),,,,,,,,,,,,Present,Not Present,,,"Music Player, Phone Calls, Communication, Device Input, AR/VR",,,-,,Done,,,"Selection, Position, Path",1,X,relative,19,Yes,No,X,Present (N=16),High (N=12),High (Derived),High (N=16),Present,83%
[258],Gashi et al.,2021,Face,Singled Loc,Facial Expression,"Smile, Facial Gesture (Mouth)","Accelerometer, Gyroscope",Present,2,,Yes,Yes,Present,Not Present,Low (Derived),Low (Derived),Low (N=21),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Social Interaction, Feedback System",,,-,,Done,,,"Path, Selection",X,X,X,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,21%
[258],Gashi et al.,2021,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Yaw","Accelerometer, Gyroscope",Present,2,,Yes,Yes,Present,Not Present,Medium (Derived),Medium (Derived),Low (N=21),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Social Interaction, Feedback System",,,-,,Done,,,"Path, Orientation, Selection",2,X,relative,2,Yes,No,X,Not Present,Not Indicated,High (Derived),Not Indicated,Present,24%
[296],Rateau et al.,2022,Ear and Earable,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[296],Rateau et al.,2022,Hand Gestures and Location,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[344],Chugh et al.,2023,Head Gestures and Pointing,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[344],Chugh et al.,2023,Face,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[376],Li et al.,2023,Ear and Earable,Singled Loc,Hand,"Pinch (Ear), Cover (Ear)",Microphone,Not Present,2,,No,Yes,Partly Present,Not Present,Medium (Derived),High (N=25),High (N=10),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input",,,-,,Done,,,"Selection, Position",X,X,X,5,Yes,No,X,Present (N=10),High (N=25),Low (Derived),Not Indicated,Present,44%
[376],Li et al.,2023,Hand Gestures and Location,Singled Loc,Hand,"Thinking Gesture, Hold (Face), Cover (Face), Calling Gesture, Support (Face)",Microphone,Not Present,6,,No,Yes,Partly Present,Not Present,Low (Derived),High (N=25),High (N=10),Not Indicated,,,,,,,,,,,,Not Present,Not Present,,,"Device Control, Device Input",,,-,,Done,,,"Selection, Position",X,X,X,10,Yes,No,X,Present (N=10),High (N=25),Low (Derived),Not Indicated,Present,44%
[308],Panda et al.,2023,Ear and Earable,Singled Loc,Hand,"Lift (Earable), Press (Earable)","Accelerometer, Gyroscope, Magnetometer, Button",Present,2,,No,Yes,Not Present,Not Present,Medium (Derived),Medium (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Present,,,Video Conference,,,-,,Done,,,"Selection, Position",1,X,relative,2,Yes,No,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,32%
[308],Panda et al.,2023,Hand Gestures and Location,Singled Loc,Hand,"Cup (Mid-Air), Cover (Face)",LiDAR Sensor,Present,2,,No,Yes,Not Present,Not Present,Low (Derived),Medium (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Present,,,Video Conference,,,-,,Done,,,"Selection, Position",X,X,X,2,Yes,No,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,31%
[308],Panda et al.,2023,Head Gestures and Pointing,Singled Loc,Head,"Pitch, Roll, Yaw","Accelerometer, Gyroscope, Magnetometer",Present,8,,Yes,Visual Attention,Not Present,Not Present,Medium (Derived),High (Derived),Not Indicated,Not Indicated,,,,,,,,,,,,Present,Present,,,Video Conference,,,-,,Done,,,"Selection, Position, Orientation",3,fine,"absolute, relative",2,Yes,Yes,Coupled,Present,Not Indicated,Low (Derived),Not Indicated,Not Indicated,35%
[275],Shimon et al.,2024,Ear and Earable,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[275],Shimon et al.,2024,Hand Gestures and Location,Singled Loc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[354],Yang et al.,2024,Ear and Earable,Singled Loc,Hand,Pinch (Ear),"Speaker, Microphone",Present,1,,No,Yes,Not Present,Not Present,Medium (Derived),High (Derived),High (N=22),Medium (N=1),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, AR/VR, Health, Device Control",,,-,,Done,,,Selection,X,X,X,1,Yes,No,X,Partly Present,Medium (N=22),Low (Derived),Medium (N=22),Present,46%
[354],Yang et al.,2024,Hand Gestures and Location,Singled Loc,Hand,"Press (Face), Cover (Face), Close (Mid-Air), Open (Mid-Air), Slide (Mid-Air), Click (Mid-Air), Approach (Mid-Air)","Speaker, Microphone",Present,9,,No,Yes,Not Present,Not Present,Low (Derived),Medium (Derived),High (N=22),Medium (N=1),,,,,,,,,,,,Not Present,Not Present,,,"Music Player, AR/VR, Health, Device Control",,,-,,Done,,,"Selection, Path",2,X,relative,11,Yes,No,X,Partly Present,Medium (N=22),Low (Derived),Medium (N=22),Present,43%
[9],Pham et al.,2020,Eye-Tracking,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[244],Cao et al.,2021,Mouth,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,
[57],Salzar et al.,2008,Head Gestures and Pointing,Out,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,-,,,,,,,,,,,,,,,,,,